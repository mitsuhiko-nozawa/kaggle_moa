{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex6:\n",
    "cv score : 0.01582363008925492\n",
    "cv score : 0.014583733652430035\n",
    "\n",
    "ex7:\n",
    "prodしたら良さそうだった\n",
    "CVを書き直しましょう\n",
    "かき直した\n",
    "cv score : 0.01696220617131911\n",
    "cv score : 0.015633094022344574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys, tqdm, time\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n",
    "        \n",
    "    \n",
    "def make_scaler(flag, seed):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    elif flag == \"gauss\":\n",
    "        return GaussRankScaler()\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "def special_pearsonr(X, Y):\n",
    "    pearsonr_value = pearsonr(X, Y)\n",
    "    if pearsonr_value[0] < 0:\n",
    "        return abs(pearsonr_value[0])\n",
    "    else:\n",
    "        return 1 - pearsonr_value[0]    \n",
    "\n",
    "seeds = [0, 1, 2, 3, 4, 5, 6]\n",
    "SCALE = \"quantile\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "pub_test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "drug_df = pd.read_csv(\"../../../Data/Raw/train_drug.csv\")#\n",
    "\n",
    "train_df = train_df.merge(drug_df, on=\"sig_id\")\n",
    "\n",
    "y = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "y = y.merge(drug_df, on='sig_id', how='left') #\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\" and col != \"drug_id\"]#\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]\n",
    "pub_test_df[\"time_dose\"] = pub_test_df[\"cp_time\"].astype(str) + \" * \" + pub_test_df[\"cp_dose\"]\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "pub_test_df = pub_test_df[pub_test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "y_nonv = y[mask].reset_index(drop=True)#\n",
    "\n",
    "scored = y_nonv.copy()#\n",
    "y_nonv.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "y.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "\n",
    "TR_NONV_SIZE = train_df.shape[0]\n",
    "TE_NONV_SHAPE = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prod_cols = [    ['g-712',  'g-208',  'g-38',  'g-100',  'g-123',  'g-328',  'g-744',  'g-248',  'g-460',  'g-731',  'g-417',  'g-349',  'g-131'], ['g-228',  'g-75',  'g-67',  'g-760',  'g-37',  'g-406',  'g-50',  'g-672',  'g-63',  'g-72',  'g-195'], ['g-100', 'g-157', 'g-178'],['g-183', 'g-300', 'g-767'],['g-50', 'g-37', 'g-489', 'g-257', 'g-332'],['g-270', 'g-135', 'g-231', 'g-158', 'g-478', 'g-146', 'g-491', 'g-392'],['g-745', 'g-635', 'g-235'], ['g-300', 'g-414', 'g-62', 'g-34'], ['g-91', 'g-392'], ['g-75', 'g-113'], ['g-599', 'g-261', 'g-38', 'g-146', 'g-392', 'g-512', 'g-744'], ['g-50', 'g-332', 'g-37', 'g-58', 'g-705'], ['g-157', 'g-178'],['g-759', 'g-100', 'g-167', 'g-75', 'g-431', 'g-189', 'g-522', 'g-91'],['g-202', 'g-385', 'g-769'],           ]\n",
    "prod_cols = [['g-145', 'g-201', 'g-208'], ['g-370', 'g-508', 'g-37'], ['g-38', 'g-392', 'g-707'], ['g-328', 'g-28', 'g-392'], ['g-441', 'g-157', 'g-392'], ['g-181', 'g-100', 'g-392'], ['g-67', 'g-760', 'g-50'], ['g-731', 'g-100', 'g-707'], ['g-478', 'g-468', 'g-310'], ['g-91', 'g-145', 'g-208'], ['g-106', 'g-744', 'g-91'], ['g-131', 'g-208', 'g-392'], ['g-144', 'g-123', 'g-86'], ['g-228', 'g-72', 'g-67'], ['g-31', 'g-328', 'g-460'], ['g-392', 'g-731', 'g-100'], ['g-732', 'g-744', 'g-707'], ['g-705', 'g-375', 'g-704'], ['g-508', 'g-50', 'g-411'], ['g-234', 'g-58', 'g-520'], ['g-503', 'g-761', 'g-50'], ['g-113', 'g-75', 'g-178'], ['g-50', 'g-508', 'g-113'], ['g-113', 'g-375', 'g-75'], ['g-576', 'g-452', 'g-392'], ['g-50', 'g-37', 'g-36'], ['g-707', 'g-133', 'g-392'], ['g-484', 'g-392', 'g-544'], ['g-508', 'g-67', 'g-370'], ['g-123', 'g-731', 'g-100'], ['g-298', 'g-477', 'g-644'], ['g-72', 'g-370', 'g-50'], ['g-67', 'g-178', 'g-113'], ['g-744', 'g-608', 'g-100'], ['g-91', 'g-100', 'g-707'], ['g-37', 'g-228', 'g-202'], ['g-37', 'g-300', 'g-370'], ['g-234', 'g-508', 'g-595'], ['g-596', 'g-744', 'g-707'], ['g-300', 'g-227', 'g-591'], ['g-135', 'g-392', 'g-512'], ['g-731', 'g-744', 'g-158'], ['g-69', 'g-707', 'g-100'], ['g-276', 'g-653', 'g-291'], ['g-624', 'g-615', 'g-189'], ['g-181', 'g-707', 'g-38'], ['g-72', 'g-75', 'g-508'], ['g-231', 'g-707', 'g-392'], ['g-508', 'g-37', 'g-72'], ['g-725', 'g-712', 'g-640'], ['g-67', 'g-644', 'g-113'], ['g-508', 'g-228', 'g-656'], ['g-185', 'g-37', 'g-672'], ['g-370', 'g-50', 'g-503'], ['g-201', 'g-745', 'g-599'], ['g-332', 'g-50', 'g-571'], ['g-50', 'g-37', 'g-59'], ['g-508', 'g-113', 'g-231'], ['g-707', 'g-158', 'g-100'], ['g-257', 'g-50', 'g-72']]\n",
    "\n",
    "for cols in prod_cols:\n",
    "    name = \"prod-\" + \" * \".join(cols)\n",
    "    #train_df[name] = train_df[cols].apply(lambda x :  x[cols[0]] * x[cols[1]] * x[cols[2]], axis=1)\n",
    "    #pub_test_df[name] = pub_test_df[cols].apply(lambda x :  x[cols[0]] * x[cols[1]] * x[cols[2]], axis=1)\n",
    "    #test_df[name] = test_df[cols].apply(lambda x :  x[cols[0]] * x[cols[1]] * x[cols[2]], axis=1)\n",
    "    \n",
    "    train_df[name] = train_df[cols].mean(axis=1)\n",
    "    test_df[name] = test_df[cols].mean(axis=1)\n",
    "    pub_test_df[name] = pub_test_df[cols].mean(axis=1)\n",
    "\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop cols num : 67\n",
      "agg\n"
     ]
    }
   ],
   "source": [
    "#out fold preprocessing\n",
    "\n",
    "#variance threshold\n",
    "VAR_THRESHOLD = 0.8\n",
    "drop_cols = []\n",
    "temp = pd.concat([train_df, pub_test_df])\n",
    "for col in BIOS+PRODS:\n",
    "    if temp[col].var() <= VAR_THRESHOLD:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "print(\"drop cols num : {}\".format(len(drop_cols)))\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "pub_test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "GENES_ = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS_ = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS_ = GENES_ + CELLS_\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]\n",
    "        \n",
    "del temp\n",
    "\n",
    "# onehot encode of categorical feature and drop\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "pub_test_df = pd.concat([pd.get_dummies(pub_test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), pub_test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "#train_X = pd.concat([pd.get_dummies(train_X[\"time_dose\"], prefix=\"onehot\", drop_first=True), train_X.drop(drop_cols, axis=1) ], axis=1)\n",
    "#valid_X = pd.concat([pd.get_dummies(valid_X[\"time_dose\"], prefix=\"onehot\", drop_first=True), valid_X.drop(drop_cols, axis=1) ], axis=1)\n",
    "#test_X = pd.concat([pd.get_dummies(test_X[\"time_dose\"], prefix=\"onehot\", drop_first=True), test_X.drop(drop_cols, axis=1) ], axis=1)\n",
    "#pub_test_X = pd.concat([pd.get_dummies(pub_test_X[\"time_dose\"], prefix=\"onehot\", drop_first=True), pub_test_X.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "\n",
    "# aggregation feature\n",
    "print(\"agg\")\n",
    "\"\"\"\n",
    "for df in [train_df, pub_test_df, test_df]:\n",
    "    df[\"agg-sum-g\"] = (df[GENES_].sum(axis=1) - df[GENES_].sum(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-mean-g\"] = (df[GENES_].mean(axis=1) - df[GENES_].mean(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-std-g\"] = df[GENES_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-g\"] = (df[GENES_].kurt(axis=1) - df[GENES_].kurt(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-skew-g\"] = (df[GENES_].skew(axis=1) - df[GENES_].skew(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-sum-c\"] = (df[CELLS_].sum(axis=1) - df[CELLS_].sum(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-mean-c\"] = (df[CELLS_].mean(axis=1) - df[CELLS_].mean(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-std-c\"] = df[CELLS_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-c\"] = (df[CELLS_].kurt(axis=1) - df[CELLS_].kurt(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-skew-c\"] = (df[CELLS_].skew(axis=1) - df[CELLS_].skew(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-sum-gc\"] = (df[BIOS_].sum(axis=1) - df[BIOS_].sum(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-mean-gc\"] = (df[BIOS_].mean(axis=1) - df[BIOS_].mean(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-std-gc\"] = df[BIOS_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-gc\"] = (df[BIOS_].kurt(axis=1) - df[BIOS_].kurt(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-skew-gc\"] = (df[BIOS_].skew(axis=1) - df[BIOS_].skew(axis=1).min() + 1).map(np.log)\n",
    "\"\"\"\n",
    "\n",
    "for df in [train_df, pub_test_df, test_df]:\n",
    "    df[\"agg-sum-g\"] = df[GENES_].sum(axis=1)\n",
    "    df[\"agg-mean-g\"] = df[GENES_].mean(axis=1)\n",
    "    df[\"agg-std-g\"] = df[GENES_].std(axis=1)\n",
    "    df[\"agg-kurt-g\"] = df[GENES_].kurt(axis=1)\n",
    "    df[\"agg-skew-g\"] = df[GENES_].skew(axis=1)\n",
    "    df[\"agg-sum-c\"] = df[CELLS_].sum(axis=1)\n",
    "    df[\"agg-mean-c\"] = df[CELLS_].mean(axis=1)\n",
    "    df[\"agg-std-c\"] = df[CELLS_].std(axis=1)\n",
    "    df[\"agg-kurt-c\"] = df[CELLS_].kurt(axis=1)\n",
    "    df[\"agg-skew-c\"] = df[CELLS_].skew(axis=1)\n",
    "    df[\"agg-sum-gc\"] = df[BIOS_].sum(axis=1)\n",
    "    df[\"agg-mean-gc\"] = df[BIOS_].mean(axis=1)\n",
    "    df[\"agg-std-gc\"] = df[BIOS_].std(axis=1)\n",
    "    df[\"agg-kurt-gc\"] = df[BIOS_].kurt(axis=1)\n",
    "    df[\"agg-skew-gc\"] = df[BIOS_].skew(axis=1)\n",
    "\n",
    "AGG = [col for col in train_df.columns if col.startswith(\"agg-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y_nonv.drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dateset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if cycle\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\nclass Model(nn.Module):\\n    def __init__(self, num_features, num_targets):\\n        super(Model, self).__init__()\\n        self.hidden1 = 1500\\n        self.hidden2 =  1500\\n        \\n        self.nn_model = nn.Sequential(\\n            nn.BatchNorm1d(num_features),\\n            nn.utils.weight_norm(nn.Linear(num_features, self.hidden1)),\\n            nn.LeakyReLU(),\\n            \\n            nn.BatchNorm1d(self.hidden1),\\n            nn.Dropout(0.45),\\n            nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2)),\\n            nn.LeakyReLU(),\\n            \\n           #nn.BatchNorm1d(self.hidden2),\\n            #nn.Dropout(0.35),\\n            #nn.utils.weight_norm(nn.Linear(self.hidden2, self.hidden3)),\\n            #nn.LeakyReLU(),\\n            \\n            nn.BatchNorm1d(self.hidden2),\\n            nn.Dropout(0.45),\\n            nn.utils.weight_norm(nn.Linear(self.hidden2, num_targets)),\\n        )\\n\\n    \\n    def forward(self, x):\\n        return self.nn_model(x)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size=1500):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        #self.hidden1 = 1024\n",
    "        #self.hidden2 = 512\n",
    "        self.hidden1 = 2048\n",
    "        self.hidden2 = 512\n",
    "        \n",
    "        self.nn_model = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, self.hidden1)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden2, num_targets)),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn_model(x)\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 =  512\n",
    "        self.hidden3 = 512\n",
    "        \n",
    "        self.nn_model = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, self.hidden1)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden2, self.hidden3)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden3),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden3, num_targets)),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn_model(x)\n",
    "\"\"\"\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden1 = 1500\n",
    "        self.hidden2 =  1500\n",
    "        \n",
    "        self.nn_model = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, self.hidden1)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "           #nn.BatchNorm1d(self.hidden2),\n",
    "            #nn.Dropout(0.35),\n",
    "            #nn.utils.weight_norm(nn.Linear(self.hidden2, self.hidden3)),\n",
    "            #nn.LeakyReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            nn.Dropout(0.45),\n",
    "            nn.utils.weight_norm(nn.Linear(self.hidden2, num_targets)),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn_model(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, trainloader, validloader, epoch_, optimizer, scheduler, loss_fn, loss_tr, early_stopping_steps, verbose, device, fold, seed):\n",
    "    \n",
    "    early_step = 0\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    t = time.time() - start\n",
    "    for epoch in range(epoch_):\n",
    "        train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, device)\n",
    "        valid_loss = valid_fn(model, loss_fn, validloader, device)\n",
    "        # if ReduceLROnPlateau\n",
    "        #scheduler.step(valid_loss)\n",
    "        if epoch % verbose==0 or epoch==epoch_-1:\n",
    "            t = time.time() - start\n",
    "            print(f\"EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}, time: {t}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'dnn_weights/{}_{}.pt'.format(seed, fold))\n",
    "            #torch.save(model, 'dnn_weights/{}_{}.pt'.format(seed, fold))\n",
    "            early_step = 0\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        elif early_stopping_steps != 0:\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                t = time.time() - start\n",
    "                print(f\"early stopping in iteration {epoch},  : best itaration is {best_epoch}, valid loss is {best_loss}, time: {t}\")\n",
    "                return model\n",
    "    t = time.time() - start       \n",
    "    print(f\"training until max epoch {epoch_},  : best itaration is {best_epoch}, valid loss is {best_loss}, time: {t}\")\n",
    "    return model\n",
    "            \n",
    "    \n",
    "def predict(model, testloader, device):\n",
    "    model.to(device)\n",
    "    predictions = inference_fn(model, testloader, device)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5746833705545767, valid_loss: 0.05081737275634493, time: 2.46225905418396\n",
      "EPOCH: 5, train_loss: 0.020976100785090872, valid_loss: 0.01814824590193374, time: 12.245971441268921\n",
      "EPOCH: 10, train_loss: 0.020568705636306084, valid_loss: 0.018397652677127292, time: 23.100956916809082\n",
      "EPOCH: 15, train_loss: 0.020090963360786005, valid_loss: 0.01764068491756916, time: 33.49903464317322\n",
      "EPOCH: 20, train_loss: 0.01906453909865324, valid_loss: 0.01726321059146098, time: 43.918723344802856\n",
      "EPOCH: 24, train_loss: 0.01823269412515388, valid_loss: 0.01726005551006113, time: 52.37661957740784\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.017245766866419997, time: 52.37683033943176\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5723531028876702, valid_loss: 0.051734902183799183, time: 2.213512420654297\n",
      "EPOCH: 5, train_loss: 0.021032641425836777, valid_loss: 0.017558926004259026, time: 12.830238103866577\n",
      "EPOCH: 10, train_loss: 0.02060537333127813, valid_loss: 0.017602733911617714, time: 23.54321026802063\n",
      "EPOCH: 15, train_loss: 0.020182384515478127, valid_loss: 0.0170496849288397, time: 33.883092403411865\n",
      "EPOCH: 20, train_loss: 0.019222182667125828, valid_loss: 0.01682838033336927, time: 43.91254544258118\n",
      "EPOCH: 24, train_loss: 0.018322565302630697, valid_loss: 0.01679084838970619, time: 52.059948444366455\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01679084838970619, time: 52.080506563186646\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5727396589180413, valid_loss: 0.051011274648564205, time: 2.0174801349639893\n",
      "EPOCH: 5, train_loss: 0.02096542945797861, valid_loss: 0.01817149179322379, time: 12.473344326019287\n",
      "EPOCH: 10, train_loss: 0.02050313462306113, valid_loss: 0.018166279260601315, time: 22.87164616584778\n",
      "EPOCH: 15, train_loss: 0.02004144743193675, valid_loss: 0.017853452691010067, time: 33.05502963066101\n",
      "EPOCH: 20, train_loss: 0.01906799145695502, valid_loss: 0.01756490298679897, time: 43.38375663757324\n",
      "EPOCH: 24, train_loss: 0.01819549179398013, valid_loss: 0.017492671151246343, time: 51.73718237876892\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017471252408410822, time: 51.73738884925842\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5724192222823268, valid_loss: 0.048849795226539885, time: 2.048220157623291\n",
      "EPOCH: 5, train_loss: 0.021220018007401108, valid_loss: 0.017567516703690802, time: 12.396637201309204\n",
      "EPOCH: 10, train_loss: 0.02073068478131208, valid_loss: 0.017567359443221773, time: 22.69414210319519\n",
      "EPOCH: 15, train_loss: 0.02033286386479934, valid_loss: 0.01710540215883936, time: 33.30462145805359\n",
      "EPOCH: 20, train_loss: 0.01933566139390071, valid_loss: 0.016807132640055247, time: 43.7933235168457\n",
      "EPOCH: 24, train_loss: 0.018550111426283485, valid_loss: 0.016688245428459984, time: 52.31143593788147\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016688245428459984, time: 52.333871364593506\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5741598082409389, valid_loss: 0.05093771953667913, time: 2.118417501449585\n",
      "EPOCH: 5, train_loss: 0.020792412828060165, valid_loss: 0.018484059641403813, time: 12.461196660995483\n",
      "EPOCH: 10, train_loss: 0.020327219672069168, valid_loss: 0.01827527422990118, time: 23.31418514251709\n",
      "EPOCH: 15, train_loss: 0.02002919095473877, valid_loss: 0.017974036266761168, time: 33.94117832183838\n",
      "EPOCH: 20, train_loss: 0.01890354276891204, valid_loss: 0.017778427207044194, time: 44.60072731971741\n",
      "EPOCH: 24, train_loss: 0.018030595648493887, valid_loss: 0.017733378096350602, time: 53.02647662162781\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.01771288506154503, time: 53.02694034576416\n",
      "seed 0 , cv score : 0.017201294335417405\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5748729028732237, valid_loss: 0.048459074114050184, time: 2.1020007133483887\n",
      "EPOCH: 5, train_loss: 0.02098901735713882, valid_loss: 0.017729335464537144, time: 12.72425651550293\n",
      "EPOCH: 10, train_loss: 0.02055308031067796, valid_loss: 0.01794056080813919, time: 23.44629144668579\n",
      "EPOCH: 15, train_loss: 0.020205878101996262, valid_loss: 0.017397154734602998, time: 33.7143828868866\n",
      "EPOCH: 20, train_loss: 0.01913388929989216, valid_loss: 0.01712507467184748, time: 44.094130754470825\n",
      "EPOCH: 24, train_loss: 0.018303046397266598, valid_loss: 0.016995045303234032, time: 52.34473490715027\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016995045303234032, time: 52.37572455406189\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5699818582422491, valid_loss: 0.0588022350200585, time: 2.177218198776245\n",
      "EPOCH: 5, train_loss: 0.021154635437372803, valid_loss: 0.01825855102922235, time: 12.853091716766357\n",
      "EPOCH: 10, train_loss: 0.02072227709805188, valid_loss: 0.017499364939119133, time: 22.988658905029297\n",
      "EPOCH: 15, train_loss: 0.02027812786400318, valid_loss: 0.017123840004205703, time: 33.315253019332886\n",
      "EPOCH: 20, train_loss: 0.019304283570660198, valid_loss: 0.016918677464127542, time: 43.925227642059326\n",
      "EPOCH: 24, train_loss: 0.018437471929127754, valid_loss: 0.016757632392857755, time: 52.63692235946655\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016757632392857755, time: 52.65954041481018\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5738325701027677, valid_loss: 0.05719796338063829, time: 2.0506322383880615\n",
      "EPOCH: 5, train_loss: 0.020986569829393124, valid_loss: 0.018222691491246223, time: 12.33996868133545\n",
      "EPOCH: 10, train_loss: 0.020533295078338055, valid_loss: 0.01819962575374281, time: 22.731415271759033\n",
      "EPOCH: 15, train_loss: 0.020093451038110947, valid_loss: 0.01780891073319842, time: 33.195072889328\n",
      "EPOCH: 20, train_loss: 0.0190779990044193, valid_loss: 0.017688093076953116, time: 43.70948886871338\n",
      "EPOCH: 24, train_loss: 0.01812580062508367, valid_loss: 0.01764938813250731, time: 52.47004699707031\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01764938813250731, time: 52.48831272125244\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5730106594391923, valid_loss: 0.04772023037075997, time: 2.0962281227111816\n",
      "EPOCH: 5, train_loss: 0.020893246866762638, valid_loss: 0.01823058072477579, time: 12.700540542602539\n",
      "EPOCH: 10, train_loss: 0.020415940052033333, valid_loss: 0.018049772190196173, time: 23.312923908233643\n",
      "EPOCH: 15, train_loss: 0.020121123641729355, valid_loss: 0.017860742339066097, time: 33.72712326049805\n",
      "EPOCH: 20, train_loss: 0.019067288740821507, valid_loss: 0.017454028688371183, time: 44.197755336761475\n",
      "EPOCH: 24, train_loss: 0.018246301692789017, valid_loss: 0.017448695270078522, time: 52.5991268157959\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017422115323798996, time: 52.599334716796875\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5759849042298585, valid_loss: 0.054117321223020556, time: 2.04470157623291\n",
      "EPOCH: 5, train_loss: 0.02108023932924236, valid_loss: 0.017423031026763576, time: 12.757724046707153\n",
      "EPOCH: 10, train_loss: 0.020631206527787403, valid_loss: 0.01735502372362784, time: 23.335881233215332\n",
      "EPOCH: 15, train_loss: 0.02020450684167173, valid_loss: 0.01710207401109593, time: 33.53826713562012\n",
      "EPOCH: 20, train_loss: 0.019200226631912874, valid_loss: 0.0169101978253041, time: 44.022292613983154\n",
      "EPOCH: 24, train_loss: 0.0183506329700242, valid_loss: 0.016851637911583697, time: 52.31667947769165\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016851637911583697, time: 52.34340500831604\n",
      "seed 1 , cv score : 0.017179946648630522\n",
      "======================== fold 1 ========================\n",
      "quantile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5735226949487907, valid_loss: 0.06104336593832289, time: 2.104341983795166\n",
      "EPOCH: 5, train_loss: 0.02094541619653287, valid_loss: 0.01795882995107344, time: 12.416082859039307\n",
      "EPOCH: 10, train_loss: 0.02049615552675897, valid_loss: 0.01784599612333945, time: 22.62675189971924\n",
      "EPOCH: 15, train_loss: 0.02001213903228442, valid_loss: 0.01753577911960227, time: 33.08937168121338\n",
      "EPOCH: 20, train_loss: 0.018995703049543976, valid_loss: 0.017248797469905444, time: 43.743698596954346\n",
      "EPOCH: 24, train_loss: 0.018125216685829386, valid_loss: 0.017134687464152063, time: 52.27531337738037\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017134687464152063, time: 52.29903316497803\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5723295288727336, valid_loss: 0.050482551540647236, time: 1.9609110355377197\n",
      "EPOCH: 5, train_loss: 0.02098315604624973, valid_loss: 0.018112458448324886, time: 12.394015312194824\n",
      "EPOCH: 10, train_loss: 0.0205522775919973, valid_loss: 0.017978895562035697, time: 22.935220956802368\n",
      "EPOCH: 15, train_loss: 0.02013496953346159, valid_loss: 0.017688337048249587, time: 33.69733715057373\n",
      "EPOCH: 20, train_loss: 0.0190698625654846, valid_loss: 0.01739347582416875, time: 44.26014971733093\n",
      "EPOCH: 24, train_loss: 0.018201164786528418, valid_loss: 0.01740618252328464, time: 52.575382232666016\n",
      "training until max epoch 25,  : best itaration is 21, valid loss is 0.01739125597689833, time: 52.575592041015625\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5734757836098257, valid_loss: 0.05540074003594262, time: 1.987722396850586\n",
      "EPOCH: 5, train_loss: 0.020957010082792545, valid_loss: 0.018145831009106977, time: 12.364928007125854\n",
      "EPOCH: 10, train_loss: 0.020577393607168957, valid_loss: 0.017793549011860574, time: 22.93772292137146\n",
      "EPOCH: 15, train_loss: 0.02017854414610327, valid_loss: 0.017593610579414026, time: 33.556278705596924\n",
      "EPOCH: 20, train_loss: 0.01916450614352589, valid_loss: 0.017245763460440294, time: 43.772767782211304\n",
      "EPOCH: 24, train_loss: 0.01831662361982508, valid_loss: 0.017198844866028854, time: 52.09345245361328\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017198844866028854, time: 52.11526679992676\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5752244739364969, valid_loss: 0.05770716911980084, time: 2.141338586807251\n",
      "EPOCH: 5, train_loss: 0.020959333822566227, valid_loss: 0.017954331104244506, time: 12.876917123794556\n",
      "EPOCH: 10, train_loss: 0.020505428164653533, valid_loss: 0.017790111367191587, time: 23.390515565872192\n",
      "EPOCH: 15, train_loss: 0.020027556191504436, valid_loss: 0.017786558025649615, time: 33.79038643836975\n",
      "EPOCH: 20, train_loss: 0.019088776156741337, valid_loss: 0.017232271816049305, time: 44.12101364135742\n",
      "EPOCH: 24, train_loss: 0.018164069534544528, valid_loss: 0.017181719999228205, time: 52.66300940513611\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.01717450405870165, time: 52.6632285118103\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.573423276938822, valid_loss: 0.051082139036485126, time: 1.9347150325775146\n",
      "EPOCH: 5, train_loss: 0.021233656200701775, valid_loss: 0.017829825835568563, time: 12.497202396392822\n",
      "EPOCH: 10, train_loss: 0.020715034674799095, valid_loss: 0.017612839197473865, time: 22.928324222564697\n",
      "EPOCH: 15, train_loss: 0.020206830936713494, valid_loss: 0.017094658155526434, time: 33.26792049407959\n",
      "EPOCH: 20, train_loss: 0.019386866750816505, valid_loss: 0.01681072520358222, time: 44.09846568107605\n",
      "EPOCH: 24, train_loss: 0.018459272800364357, valid_loss: 0.01674552778048175, time: 52.38721299171448\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.016727702133357523, time: 52.38742470741272\n",
      "seed 2 , cv score : 0.01717039973153307\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5734282735789168, valid_loss: 0.05935543488178934, time: 1.9786803722381592\n",
      "EPOCH: 5, train_loss: 0.02106088907390401, valid_loss: 0.017854663250701768, time: 12.699674844741821\n",
      "EPOCH: 10, train_loss: 0.020621877852017464, valid_loss: 0.017519824419702804, time: 22.796409130096436\n",
      "EPOCH: 15, train_loss: 0.020180656264225643, valid_loss: 0.017310054866330964, time: 33.714539766311646\n",
      "EPOCH: 20, train_loss: 0.019153623193826363, valid_loss: 0.016972980595060756, time: 44.498231649398804\n",
      "EPOCH: 24, train_loss: 0.018315924311299688, valid_loss: 0.01691682721887316, time: 52.70513129234314\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01691682721887316, time: 52.73056197166443\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5711397921423549, valid_loss: 0.05298011289799914, time: 1.956679105758667\n",
      "EPOCH: 5, train_loss: 0.020875157734406166, valid_loss: 0.01839281267979566, time: 12.77211332321167\n",
      "EPOCH: 10, train_loss: 0.020438262409922005, valid_loss: 0.01803134461207425, time: 23.085716724395752\n",
      "EPOCH: 15, train_loss: 0.019967396905564743, valid_loss: 0.01797213901163024, time: 33.5786030292511\n",
      "EPOCH: 20, train_loss: 0.018948186769325664, valid_loss: 0.01765954212340362, time: 43.95982789993286\n",
      "EPOCH: 24, train_loss: 0.018172253888316343, valid_loss: 0.017594628802993718, time: 52.28072643280029\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017593265456311843, time: 52.28093409538269\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5761241677903781, valid_loss: 0.057026757832084385, time: 2.116098642349243\n",
      "EPOCH: 5, train_loss: 0.021111724558320357, valid_loss: 0.01759808337582009, time: 12.754912376403809\n",
      "EPOCH: 10, train_loss: 0.020604501770686928, valid_loss: 0.017528686486184598, time: 22.922913312911987\n",
      "EPOCH: 15, train_loss: 0.020159487333828514, valid_loss: 0.017391623663050786, time: 33.08827352523804\n",
      "EPOCH: 20, train_loss: 0.019080835754853965, valid_loss: 0.017104631795414858, time: 43.539427042007446\n",
      "EPOCH: 24, train_loss: 0.01821590819307705, valid_loss: 0.017024738155305384, time: 51.983567237854004\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.01701102110424212, time: 51.98437452316284\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5745927036950623, valid_loss: 0.050360437163284845, time: 1.9766802787780762\n",
      "EPOCH: 5, train_loss: 0.020984957171400098, valid_loss: 0.018254219847066062, time: 12.517722845077515\n",
      "EPOCH: 10, train_loss: 0.02048560067413062, valid_loss: 0.017962842833782945, time: 22.818359851837158\n",
      "EPOCH: 15, train_loss: 0.020109211849252673, valid_loss: 0.01761729440518788, time: 33.23781967163086\n",
      "EPOCH: 20, train_loss: 0.019074797603118157, valid_loss: 0.01735441463866404, time: 44.06923699378967\n",
      "EPOCH: 24, train_loss: 0.018228043231052638, valid_loss: 0.017322316127164023, time: 52.49625825881958\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.01731825274016176, time: 52.49696683883667\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.572501373409793, valid_loss: 0.05043380377548082, time: 2.0385830402374268\n",
      "EPOCH: 5, train_loss: 0.021102921308382698, valid_loss: 0.01780565149549927, time: 12.239642143249512\n",
      "EPOCH: 10, train_loss: 0.02064291435037402, valid_loss: 0.01747261370931353, time: 22.870275259017944\n",
      "EPOCH: 15, train_loss: 0.0202483755900808, valid_loss: 0.017205075334225384, time: 33.3467903137207\n",
      "EPOCH: 20, train_loss: 0.01913715537259544, valid_loss: 0.01685616642768894, time: 44.02179718017578\n",
      "EPOCH: 24, train_loss: 0.018308118991283834, valid_loss: 0.016821247711777686, time: 52.44998574256897\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016821247711777686, time: 52.4745454788208\n",
      "seed 3 , cv score : 0.01718249569341467\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5727190921164077, valid_loss: 0.05371856231774603, time: 2.0942366123199463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, train_loss: 0.02091365616660619, valid_loss: 0.018276916391083173, time: 12.782114028930664\n",
      "EPOCH: 10, train_loss: 0.020482026054051475, valid_loss: 0.01810706973608051, time: 23.37301802635193\n",
      "EPOCH: 15, train_loss: 0.02012413636227881, valid_loss: 0.017706347602818695, time: 33.84536361694336\n",
      "EPOCH: 20, train_loss: 0.01896664745889712, valid_loss: 0.01741545732532229, time: 44.049196004867554\n",
      "EPOCH: 24, train_loss: 0.018122752095856096, valid_loss: 0.017391214732612882, time: 52.28674864768982\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017391214732612882, time: 52.307504415512085\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5760267898984199, valid_loss: 0.05388573440057891, time: 1.9692444801330566\n",
      "EPOCH: 5, train_loss: 0.02086486686428968, valid_loss: 0.018297029339841436, time: 12.730836391448975\n",
      "EPOCH: 10, train_loss: 0.02040962347366514, valid_loss: 0.01808098184743098, time: 23.199552059173584\n",
      "EPOCH: 15, train_loss: 0.02000941094147028, valid_loss: 0.017911995282130583, time: 33.578038930892944\n",
      "EPOCH: 20, train_loss: 0.019043535633135017, valid_loss: 0.017548173053988387, time: 43.95425748825073\n",
      "EPOCH: 24, train_loss: 0.018219808157343063, valid_loss: 0.017508535459637643, time: 52.59899592399597\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017508535459637643, time: 52.6194281578064\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5754818962212058, valid_loss: 0.04995874475155558, time: 1.9228506088256836\n",
      "EPOCH: 5, train_loss: 0.0211044872260612, valid_loss: 0.01803307807339089, time: 12.285332202911377\n",
      "EPOCH: 10, train_loss: 0.020576800071242927, valid_loss: 0.017468195940767015, time: 22.55364727973938\n",
      "EPOCH: 15, train_loss: 0.020209792110583057, valid_loss: 0.01738726980984211, time: 33.399685859680176\n",
      "EPOCH: 20, train_loss: 0.01922535657396783, valid_loss: 0.016940857150724957, time: 44.0468692779541\n",
      "EPOCH: 24, train_loss: 0.018412547509955322, valid_loss: 0.016877768481416363, time: 52.925477504730225\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.016855297024760928, time: 52.92567563056946\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5741849470965183, valid_loss: 0.05094905719161034, time: 1.9644436836242676\n",
      "EPOCH: 5, train_loss: 0.021315438890435398, valid_loss: 0.01764193166579519, time: 12.452358961105347\n",
      "EPOCH: 10, train_loss: 0.020590392791115453, valid_loss: 0.01730990997914757, time: 23.221760272979736\n",
      "EPOCH: 15, train_loss: 0.020154871073735023, valid_loss: 0.017058203741908074, time: 33.68362617492676\n",
      "EPOCH: 20, train_loss: 0.019151649240703478, valid_loss: 0.016848943063191006, time: 44.17931771278381\n",
      "EPOCH: 24, train_loss: 0.018312778622999678, valid_loss: 0.01681804002395698, time: 52.471813440322876\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.01679598077067307, time: 52.47259211540222\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5715006841751544, valid_loss: 0.053715972070183075, time: 2.023390293121338\n",
      "EPOCH: 5, train_loss: 0.021091565233317837, valid_loss: 0.017764297979218618, time: 12.745498418807983\n",
      "EPOCH: 10, train_loss: 0.020705381840251495, valid_loss: 0.017495099348681313, time: 23.75060796737671\n",
      "EPOCH: 15, train_loss: 0.020252593887456947, valid_loss: 0.017327253919626986, time: 34.06599020957947\n",
      "EPOCH: 20, train_loss: 0.019167247714231842, valid_loss: 0.017141200602054596, time: 44.5447793006897\n",
      "EPOCH: 24, train_loss: 0.018393510783874037, valid_loss: 0.016989004878061157, time: 52.66394639015198\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016989004878061157, time: 52.684430837631226\n",
      "seed 4 , cv score : 0.017203975655137402\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5741846273126809, valid_loss: 0.055215497421366826, time: 2.181431770324707\n",
      "EPOCH: 5, train_loss: 0.02117968233221251, valid_loss: 0.017829198097544057, time: 12.600919723510742\n",
      "EPOCH: 10, train_loss: 0.020635867585846478, valid_loss: 0.017694170586764812, time: 23.059101343154907\n",
      "EPOCH: 15, train_loss: 0.02023595806373202, valid_loss: 0.017546896875969, time: 35.100980281829834\n",
      "EPOCH: 20, train_loss: 0.019327753102001938, valid_loss: 0.017215878409998758, time: 48.19348764419556\n",
      "EPOCH: 24, train_loss: 0.018524764186662178, valid_loss: 0.01699615857963051, time: 56.48302912712097\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01699615857963051, time: 56.50244474411011\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5749009750565909, valid_loss: 0.0539223490016801, time: 2.1503028869628906\n",
      "EPOCH: 5, train_loss: 0.020896491111956374, valid_loss: 0.01859232613018581, time: 12.492231607437134\n",
      "EPOCH: 10, train_loss: 0.020441753898550123, valid_loss: 0.018204813530402524, time: 23.34251642227173\n",
      "EPOCH: 15, train_loss: 0.019901735164279486, valid_loss: 0.018001313587384566, time: 33.920469760894775\n",
      "EPOCH: 20, train_loss: 0.018961595670476446, valid_loss: 0.017794162221252918, time: 44.7522759437561\n",
      "EPOCH: 24, train_loss: 0.018054803867355314, valid_loss: 0.017743552742259843, time: 52.97352480888367\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.01773370552275862, time: 52.973897218704224\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5725452966070261, valid_loss: 0.04655281560761588, time: 2.139051914215088\n",
      "EPOCH: 5, train_loss: 0.021145023715992767, valid_loss: 0.01761903150805405, time: 13.10223937034607\n",
      "EPOCH: 10, train_loss: 0.020698368913777496, valid_loss: 0.01747988590172359, time: 23.91483187675476\n",
      "EPOCH: 15, train_loss: 0.020273487711244303, valid_loss: 0.017119143637163298, time: 34.32349181175232\n",
      "EPOCH: 20, train_loss: 0.019349673789912376, valid_loss: 0.01674435974231788, time: 44.5886435508728\n",
      "EPOCH: 24, train_loss: 0.01850227659325237, valid_loss: 0.01670633942953178, time: 52.828144550323486\n",
      "training until max epoch 25,  : best itaration is 21, valid loss is 0.01667061232562576, time: 52.82834219932556\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5753858500308748, valid_loss: 0.05426420984523637, time: 2.232774257659912\n",
      "EPOCH: 5, train_loss: 0.021189889762604584, valid_loss: 0.01773513129779271, time: 13.121175289154053\n",
      "EPOCH: 10, train_loss: 0.02059318383048842, valid_loss: 0.01743146029434034, time: 23.298915147781372\n",
      "EPOCH: 15, train_loss: 0.02018534695810598, valid_loss: 0.017296211873846393, time: 33.84471821784973\n",
      "EPOCH: 20, train_loss: 0.019193180836737156, valid_loss: 0.01699307273541178, time: 44.37582468986511\n",
      "EPOCH: 24, train_loss: 0.018339126350601084, valid_loss: 0.016906685887702874, time: 53.11771249771118\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.016886454075574875, time: 53.11793851852417\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5707045114612666, valid_loss: 0.05147022402712277, time: 2.193079710006714\n",
      "EPOCH: 5, train_loss: 0.021053911159759846, valid_loss: 0.018163050125752176, time: 12.934846878051758\n",
      "EPOCH: 10, train_loss: 0.020509013548439394, valid_loss: 0.017909648322633333, time: 23.363396644592285\n",
      "EPOCH: 15, train_loss: 0.020026221817386322, valid_loss: 0.017719408071466856, time: 33.78219509124756\n",
      "EPOCH: 20, train_loss: 0.019122433173807636, valid_loss: 0.017273329916809285, time: 44.52667999267578\n",
      "EPOCH: 24, train_loss: 0.018314674327932837, valid_loss: 0.017230624279805592, time: 53.08553504943848\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.017218079843691416, time: 53.08574724197388\n",
      "seed 5 , cv score : 0.017180931360129374\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5758765758334721, valid_loss: 0.05155098118952343, time: 2.1333322525024414\n",
      "EPOCH: 5, train_loss: 0.020974992726841113, valid_loss: 0.017848608403333594, time: 12.508678913116455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10, train_loss: 0.020607251973047743, valid_loss: 0.017616835449423107, time: 23.387864589691162\n",
      "EPOCH: 15, train_loss: 0.02014285766512808, valid_loss: 0.017709005091871535, time: 33.53389835357666\n",
      "EPOCH: 20, train_loss: 0.01919688947879485, valid_loss: 0.017132081357496127, time: 44.081469774246216\n",
      "EPOCH: 24, train_loss: 0.01839935953599693, valid_loss: 0.01707207140113626, time: 52.47675037384033\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017043340871376652, time: 52.47697830200195\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5716587150658387, valid_loss: 0.051852867965187344, time: 2.2315988540649414\n",
      "EPOCH: 5, train_loss: 0.021192844160764977, valid_loss: 0.01785019702677216, time: 12.667377948760986\n",
      "EPOCH: 10, train_loss: 0.020761611884918766, valid_loss: 0.017718328322683063, time: 23.26608920097351\n",
      "EPOCH: 15, train_loss: 0.020271691291228584, valid_loss: 0.01746219793068511, time: 33.277989625930786\n",
      "EPOCH: 20, train_loss: 0.019335841112162754, valid_loss: 0.017165249612714564, time: 43.76806020736694\n",
      "EPOCH: 24, train_loss: 0.018536042638015057, valid_loss: 0.01711918559989759, time: 52.18835639953613\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01711918559989759, time: 52.213560342788696\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5728713105824115, valid_loss: 0.05543103956562631, time: 2.1190249919891357\n",
      "EPOCH: 5, train_loss: 0.020997701739163502, valid_loss: 0.01817072703338721, time: 12.64020562171936\n",
      "EPOCH: 10, train_loss: 0.020637788296933624, valid_loss: 0.017652447883258846, time: 22.99478030204773\n",
      "EPOCH: 15, train_loss: 0.02020825865422038, valid_loss: 0.01747663039714098, time: 33.246975898742676\n",
      "EPOCH: 20, train_loss: 0.019170587113046127, valid_loss: 0.01714237776639707, time: 43.90334439277649\n",
      "EPOCH: 24, train_loss: 0.018288458872964417, valid_loss: 0.017120775081874692, time: 52.55661869049072\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017099320724168244, time: 52.55682301521301\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5722639760461407, valid_loss: 0.04913886444909232, time: 2.0196585655212402\n",
      "EPOCH: 5, train_loss: 0.021192932056020134, valid_loss: 0.017499595880508423, time: 12.683117151260376\n",
      "EPOCH: 10, train_loss: 0.02057965803027585, valid_loss: 0.01743311198162181, time: 23.25849962234497\n",
      "EPOCH: 15, train_loss: 0.02013602873067493, valid_loss: 0.017132034019700117, time: 33.73955154418945\n",
      "EPOCH: 20, train_loss: 0.019168068681830082, valid_loss: 0.0167862271490906, time: 44.506455421447754\n",
      "EPOCH: 24, train_loss: 0.018332221498038027, valid_loss: 0.016739320355866637, time: 53.25868272781372\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016739320355866637, time: 53.27913570404053\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "(21948, 886)\n",
      "EPOCH: 0, train_loss: 0.5749568898964973, valid_loss: 0.05340844914317131, time: 2.1090281009674072\n",
      "EPOCH: 5, train_loss: 0.020898766462167685, valid_loss: 0.018199106359056066, time: 12.67767858505249\n",
      "EPOCH: 10, train_loss: 0.020541253560868493, valid_loss: 0.018119198536234244, time: 23.396183490753174\n",
      "EPOCH: 15, train_loss: 0.02004283899101463, valid_loss: 0.017942977829703263, time: 34.25768828392029\n",
      "EPOCH: 20, train_loss: 0.01908140509885593, valid_loss: 0.017594496506665433, time: 44.53788900375366\n",
      "EPOCH: 24, train_loss: 0.018283577867015434, valid_loss: 0.017515317989247187, time: 53.03119421005249\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017515317989247187, time: 53.05308175086975\n",
      "seed 6 , cv score : 0.01719435860197834\n",
      "cv score : 0.01696759116938932\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = ('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 25\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_df.shape[0], y_nonv.shape[1]))\n",
    "imps = []\n",
    "imp_cols = []\n",
    "folds = []\n",
    "test_cv_preds = []\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 5\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    \n",
    "    ###############################################################################################\n",
    "    # LOAD LIBRARIES\n",
    "    targets = SCORED_MOAS.copy()\n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = scored[\"drug_id\"].value_counts()\n",
    "    vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "    vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values} # drug id がどのフォールドに属すか格納\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.loc[scored[\"drug_id\"].isin(vc2)].reset_index(drop=True)\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp[\"sig_id\"][idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN K\n",
    "    scored['fold'] = scored.drug_id.map(dct1)\n",
    "    scored.loc[scored[\"fold\"].isna(),'fold'] = scored.loc[scored[\"fold\"].isna(),'sig_id'].map(dct2)\n",
    "    scored[\"fold\"] = scored[\"fold\"].astype('int8')\n",
    "    ###############################################################################################\n",
    "\n",
    "    #for fold, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):    \n",
    "    for fold in range(K):\n",
    "        train_index = scored[scored[\"fold\"] != fold].index.to_list()\n",
    "        valid_index = scored[scored[\"fold\"] == fold].index.to_list()\n",
    "        print(\"======================== fold {} ========================\".format(fold+1))\n",
    "        folds.append(train_index)\n",
    "                \n",
    "        ### split data ##########\n",
    "        train_X = X.iloc[train_index]\n",
    "        train_y = y_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index]\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        test_X = (test_df.drop(\"sig_id\", axis=1))\n",
    "        pub_test_X = (pub_test_df.drop(\"sig_id\", axis=1))\n",
    "\n",
    "        \n",
    "    \n",
    "        train_X.drop(\"drug_id\", axis=1, inplace=True)\n",
    "        valid_X.drop(\"drug_id\", axis=1, inplace=True)\n",
    "        \n",
    "        ### scaler ##########\n",
    "        print(SCALE)\n",
    "        scale_cols = BIOS_+PRODS+AGG\n",
    "        scaler = make_scaler(SCALE, seed).fit(train_X.append(pub_test_X)[scale_cols])\n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ### PCA ##########\n",
    "        print(\"PCA\")\n",
    "        n_decom_g = 80\n",
    "        n_decom_c = 10\n",
    "        decom_g_cols = [f\"pca_g-{i}\" for i in range(n_decom_g)]\n",
    "        decom_c_cols = [f\"pca_c-{i}\" for i in range(n_decom_c)]\n",
    "        \n",
    "        pca_g = PCA(n_components = n_decom_g, random_state = seed).fit(train_X.append(pub_test_X)[GENES_])\n",
    "        pca_c = PCA(n_components = n_decom_c, random_state = seed).fit(train_X.append(pub_test_X)[CELLS_])\n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[decom_g_cols] = pca_g.transform(df[GENES_])\n",
    "            df[decom_c_cols] = pca_c.transform(df[CELLS_])\n",
    "\n",
    "        \n",
    "\n",
    "        # prepare data for training\n",
    "        train_X = train_X.values\n",
    "        valid_X = valid_X.values\n",
    "        test_X = test_X.values\n",
    "        print(X.shape)\n",
    "        \n",
    "        \n",
    "        # ================================model training===========================\n",
    "        train_dataset = MoADataset(train_X, train_y)\n",
    "        valid_dataset = MoADataset(valid_X, valid_y)\n",
    "        test_dataset = TestDataset(test_X)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = Model(\n",
    "            num_features=train_X.shape[1],\n",
    "            num_targets=train_y.shape[1],\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        #optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,)\n",
    "        optimizer = torch.optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader) )\n",
    "        #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode = \"min\", patience = 3, min_lr = 1e-5, factor = 0.1, eps=1e-5,verbose=True)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_tr = SmoothBCEwLogits(smoothing=1e-3)\n",
    "        \n",
    "        # train\n",
    "        model = run_training(\n",
    "            model=model,\n",
    "            trainloader=trainloader,\n",
    "            validloader=validloader,\n",
    "            epoch_=EPOCHS,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            loss_fn=loss_fn,\n",
    "            loss_tr=loss_tr,\n",
    "            early_stopping_steps=EARLY_STOPPING_STEPS,\n",
    "            device=DEVICE,\n",
    "            verbose=5,\n",
    "            fold=fold,\n",
    "            seed=seed,)\n",
    "        #model = torch.load('dnn_weights/{}_{}.pt'.format(seed, fold))\n",
    "        model.load_state_dict(torch.load('dnn_weights/{}_{}.pt'.format(seed, fold)), DEVICE)\n",
    "        \n",
    "        #valid predict\n",
    "        val_preds = predict(\n",
    "            model=model,\n",
    "            testloader=validloader,\n",
    "            device=DEVICE,)\n",
    "        \n",
    "        #test predict\n",
    "        test_preds = predict(\n",
    "            model=model,\n",
    "            testloader=testloader,\n",
    "            device=DEVICE)\n",
    "        \n",
    "        # ================================model training===========================\n",
    "\n",
    "        train_pred[valid_index] +=  val_preds\n",
    "        \n",
    "        preds += test_preds / (K*len(seeds))\n",
    "\n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE,  y.shape[1]))\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "\n",
    "\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"../../../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "cols = [col for col in train_sub.columns if col != \"sig_id\"]\n",
    "train_sub[cols] = train_preds2\n",
    "train_sub.to_csv(\"train_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip dnn_weights.zip dnn_weights/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### distance feature ##########\n",
    "        print(\"target distance\")\n",
    "        tag_drugs = train_X[\"drug_id\"].value_counts().index[:8]\n",
    "        for drug in tag_drugs:\n",
    "            print(drug)\n",
    "            col = \"mean-{}\".format(drug)\n",
    "            train_X[col] = 0\n",
    "            valid_X[col] = 0\n",
    "            test_X[col] = 0\n",
    "            pub_test_X[col] = 0\n",
    "            for td in train_X[\"time_dose\"].unique():\n",
    "                print(td)\n",
    "                mean_vec = train_X[(train_X[\"time_dose\"] == td) & (train_X[\"drug_id\"] == drug)][GENES_].mean()\n",
    "                \n",
    "                tr_ind = train_X[(train_X[\"time_dose\"] == td)].index\n",
    "                va_ind = valid_X[(valid_X[\"time_dose\"] == td)].index\n",
    "                te_ind = test_X[(test_X[\"time_dose\"] == td)].index\n",
    "                pte_ind = pub_test_X[(pub_test_X[\"time_dose\"] == td)].index\n",
    "                \n",
    "                train_X[col].loc[tr_ind] = train_X.loc[tr_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                valid_X[col].loc[va_ind] = valid_X.loc[va_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                test_X[col].loc[te_ind] = test_X.loc[te_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                pub_test_X[col].loc[pte_ind] = pub_test_X.loc[pte_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### distance feature ##########\n",
    "        print(\"target distance\")\n",
    "        tag_drugs = train_X[\"drug_id\"].value_counts().index[:8]\n",
    "        for drug in tag_drugs:\n",
    "            print(drug)\n",
    "            for td in train_X[\"time_dose\"].unique():\n",
    "                print(td)\n",
    "                col = \"mean-{}-{}\".format(drug, td)\n",
    "                train_X[col] = 0\n",
    "                valid_X[col] = 0\n",
    "                test_X[col] = 0\n",
    "                pub_test_X[col] = 0\n",
    "                mean_vec = train_X[(train_X[\"time_dose\"] == td) & (train_X[\"drug_id\"] == drug)][GENES_].mean()\n",
    "                \n",
    "                tr_ind = train_X[(train_X[\"time_dose\"] == td)].index\n",
    "                va_ind = valid_X[(valid_X[\"time_dose\"] == td)].index\n",
    "                te_ind = test_X[(test_X[\"time_dose\"] == td)].index\n",
    "                pte_ind = pub_test_X[(pub_test_X[\"time_dose\"] == td)].index\n",
    "                \n",
    "                train_X[col].loc[tr_ind] = train_X.loc[tr_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                valid_X[col].loc[va_ind] = valid_X.loc[va_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                test_X[col].loc[te_ind] = test_X.loc[te_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)\n",
    "                pub_test_X[col].loc[pte_ind] = pub_test_X.loc[pte_ind].apply(lambda x : distance.euclidean(x[GENES_], mean_vec), axis=1).map(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### distance feature ##########\n",
    "print(\"target distance\")\n",
    "tag_drugs = train_X[\"drug_id\"].value_counts().index[:7]\n",
    "for drug in tag_drugs:\n",
    "    print(drug)\n",
    "    col = \"mean-{}\".format(drug)\n",
    "    mean_vec = train_X[train_X[\"drug_id\"] == drug][GENES_].mean()\n",
    "\n",
    "    train_X[col] = train_X.apply(lambda x : special_pearsonr(x[GENES_], mean_vec), axis=1)\n",
    "    valid_X[col] = valid_X.apply(lambda x : special_pearsonr(x[GENES_], mean_vec), axis=1)\n",
    "    test_X[col] = test_X.apply(lambda x : special_pearsonr(x[GENES_], mean_vec), axis=1)\n",
    "    pub_test_X[col] = pub_test_X.apply(lambda x : special_pearsonr(x[GENES_], mean_vec), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
