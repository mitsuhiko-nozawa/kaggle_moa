{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝  \n",
    "ex5:  \n",
    "特徴量を新たに加える\n",
    "・onehot\n",
    "・ある遺伝子集合の平均\n",
    "\n",
    "後処理  \n",
    "・近傍\n",
    "\n",
    "とりあえず突っ込んだ\n",
    "cv score : 0.016759827614837304 (control removed)\n",
    "cv score : 0.01544\n",
    "\n",
    "何もなし\n",
    "cv score : 0.016659014838521283\n",
    "cv score : 0.01535365993431876\n",
    "\n",
    "random\n",
    "cv score : 0.016784831050276998 \n",
    "random(:400)\n",
    "cv score : 0.016759801481980134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys, tqdm, time\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n",
    "        \n",
    "    \n",
    "def make_scaler(flag, seed):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    elif flag == \"gauss\":\n",
    "        return GaussRankScaler()\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "\n",
    "seeds = [14, 15, 16, 17, 18, 19, 20]\n",
    "SCALE = \"quantile\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "#pub_test_df = pd.read_csv(\"../input/moapublictest/test_features.csv\")\n",
    "pub_test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "drug_df = pd.read_csv(\"../../../Data/Raw/train_drug.csv\")#\n",
    "\n",
    "y = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "y = y.merge(drug_df, on='sig_id', how='left') #\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\" and col != \"drug_id\"]#\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]\n",
    "pub_test_df[\"time_dose\"] = pub_test_df[\"cp_time\"].astype(str) + \" * \" + pub_test_df[\"cp_dose\"]\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "pub_test_df = pub_test_df[pub_test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "y_nonv = y[mask].reset_index(drop=True)#\n",
    "\n",
    "scored = y_nonv.copy()#\n",
    "y_nonv.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "y.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "\n",
    "TR_NONV_SIZE = train_df.shape[0]\n",
    "TE_NONV_SHAPE = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)\n",
    "    \n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.0, n_cls=2):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing + smoothing / n_cls\n",
    "        self.smoothing = smoothing / n_cls\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        probs = torch.nn.functional.sigmoid(x,)\n",
    "        # ylogy + (1-y)log(1-y)\n",
    "        #with torch.no_grad():\n",
    "        target1 = self.confidence * target + (1-target) * self.smoothing\n",
    "        #print(target1.cpu())\n",
    "        loss = -(torch.log(probs+1e-15) * target1 + (1-target1) * torch.log(1-probs+1e-15))\n",
    "        #print(loss.cpu())\n",
    "        #nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        #nll_loss = nll_loss.squeeze(1)\n",
    "        #smooth_loss = -logprobs.mean(dim=-1)\n",
    "        #loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod\n",
    "# 上位500こ\n",
    "#prod_cols = [['g-145', 'g-201', 'g-208'], ['g-370', 'g-508', 'g-37'], ['g-38', 'g-392', 'g-707'], ['g-328', 'g-28', 'g-392'], ['g-441', 'g-157', 'g-392'], ['g-181', 'g-100', 'g-392'], ['g-67', 'g-760', 'g-50'], ['g-731', 'g-100', 'g-707'], ['g-478', 'g-468', 'g-310'], ['g-91', 'g-145', 'g-208'], ['g-106', 'g-744', 'g-91'], ['g-131', 'g-208', 'g-392'], ['g-144', 'g-123', 'g-86'], ['g-228', 'g-72', 'g-67'], ['g-31', 'g-328', 'g-460'], ['g-392', 'g-731', 'g-100'], ['g-732', 'g-744', 'g-707'], ['g-705', 'g-375', 'g-704'], ['g-508', 'g-50', 'g-411'], ['g-234', 'g-58', 'g-520'], ['g-503', 'g-761', 'g-50'], ['g-113', 'g-75', 'g-178'], ['g-50', 'g-508', 'g-113'], ['g-113', 'g-375', 'g-75'], ['g-576', 'g-452', 'g-392'], ['g-50', 'g-37', 'g-36'], ['g-707', 'g-133', 'g-392'], ['g-484', 'g-392', 'g-544'], ['g-508', 'g-67', 'g-370'], ['g-123', 'g-731', 'g-100'], ['g-298', 'g-477', 'g-644'], ['g-72', 'g-370', 'g-50'], ['g-67', 'g-178', 'g-113'], ['g-744', 'g-608', 'g-100'], ['g-91', 'g-100', 'g-707'], ['g-37', 'g-228', 'g-202'], ['g-37', 'g-300', 'g-370'], ['g-234', 'g-508', 'g-595'], ['g-596', 'g-744', 'g-707'], ['g-300', 'g-227', 'g-591'], ['g-135', 'g-392', 'g-512'], ['g-731', 'g-744', 'g-158'], ['g-69', 'g-707', 'g-100'], ['g-276', 'g-653', 'g-291'], ['g-624', 'g-615', 'g-189'], ['g-181', 'g-707', 'g-38'], ['g-72', 'g-75', 'g-508'], ['g-231', 'g-707', 'g-392'], ['g-508', 'g-37', 'g-72'], ['g-725', 'g-712', 'g-640'], ['g-67', 'g-644', 'g-113'], ['g-508', 'g-228', 'g-656'], ['g-185', 'g-37', 'g-672'], ['g-370', 'g-50', 'g-503'], ['g-201', 'g-745', 'g-599'], ['g-332', 'g-50', 'g-571'], ['g-50', 'g-37', 'g-59'], ['g-508', 'g-113', 'g-231'], ['g-707', 'g-158', 'g-100'], ['g-257', 'g-50', 'g-72']]\n",
    "#prod_cols = [['g-712',  'g-208',  'g-38',  'g-100',  'g-123',  'g-328',  'g-744',  'g-248',  'g-460',  'g-731',  'g-417',  'g-349',  'g-131'], ['g-228',  'g-75',  'g-67',  'g-760',  'g-37',  'g-406',  'g-50',  'g-672',  'g-63',  'g-72',  'g-195'], ['g-100', 'g-157', 'g-178'],['g-183', 'g-300', 'g-767'],['g-50', 'g-37', 'g-489', 'g-257', 'g-332'],['g-270', 'g-135', 'g-231', 'g-158', 'g-478', 'g-146', 'g-491', 'g-392'],['g-745', 'g-635', 'g-235'], ['g-300', 'g-414', 'g-62', 'g-34'], ['g-91', 'g-392'], ['g-75', 'g-113'], ['g-599', 'g-261', 'g-38', 'g-146', 'g-392', 'g-512', 'g-744'], ['g-50', 'g-332', 'g-37', 'g-58', 'g-705'], ['g-157', 'g-178'],['g-759', 'g-100', 'g-167', 'g-75', 'g-431', 'g-189', 'g-522', 'g-91'],['g-202', 'g-385', 'g-769']            ]\n",
    "\n",
    "prod_cols = [\n",
    "    ['g-712',  'g-208',  'g-38',  'g-100',  'g-123',  'g-328',  'g-744',  'g-248',  'g-460',  'g-731',  'g-417',  'g-349',  'g-131'], ['g-228',  'g-75',  'g-67',  'g-760',  'g-37',  'g-406',  'g-50',  'g-672',  'g-63',  'g-72',  'g-195'], ['g-100', 'g-157', 'g-178'],['g-183', 'g-300', 'g-767'],['g-50', 'g-37', 'g-489', 'g-257', 'g-332'],['g-270', 'g-135', 'g-231', 'g-158', 'g-478', 'g-146', 'g-491', 'g-392'],['g-745', 'g-635', 'g-235'], ['g-300', 'g-414', 'g-62', 'g-34'], ['g-91', 'g-392'], ['g-75', 'g-113'], ['g-599', 'g-261', 'g-38', 'g-146', 'g-392', 'g-512', 'g-744'], ['g-50', 'g-332', 'g-37', 'g-58', 'g-705'], ['g-157', 'g-178'],['g-759', 'g-100', 'g-167', 'g-75', 'g-431', 'g-189', 'g-522', 'g-91'],['g-202', 'g-385', 'g-769'],\n",
    "    #['c-99', 'c-37', 'c-58', 'c-22', 'c-74'],\n",
    "    #['c-70',  'c-6',  'c-38',  'c-93',  'c-33',  'c-60',  'c-63',  'c-17',  'c-12',  'c-26'],\n",
    "    #['c-81', 'c-44'],\n",
    "    #['c-59', 'c-47', 'c-9'],\n",
    "    #['c-37', 'c-1'],\n",
    "    #['c-18', 'c-98'],\n",
    "    #['c-88', 'c-1'],\n",
    "    #['c-6', 'c-30'],\n",
    "    #['c-61', 'c-7', 'c-16'],\n",
    "    #['c-48', 'c-18', 'c-81'],\n",
    "    #['c-88', 'c-32', 'c-74', 'c-16', 'c-37'],\n",
    "    #['c-63', 'c-18', 'c-91', 'c-82'],\n",
    "    #['c-49', 'c-51', 'c-76'],\n",
    "    #['c-65', 'c-98'],\n",
    "    #['c-87', 'c-19'],\n",
    "    #['c-65', 'c-70'],\n",
    "            \n",
    "]\n",
    "\n",
    "for cols in prod_cols:\n",
    "    name = \"prod-\" + \" * \".join(cols)\n",
    "    train_df[name] = train_df[cols].mean(axis=1)\n",
    "    test_df[name] = test_df[cols].mean(axis=1)\n",
    "    pub_test_df[name] = pub_test_df[cols].mean(axis=1)\n",
    "\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop cols num : 70\n",
      "agg\n"
     ]
    }
   ],
   "source": [
    "#out fold preprocessing\n",
    "\n",
    "#variance threshold\n",
    "\n",
    "VAR_THRESHOLD = 0.8\n",
    "drop_cols = []\n",
    "temp = pd.concat([train_df, pub_test_df])\n",
    "#for col in BIOS:\n",
    "for col in BIOS+PRODS:\n",
    "    if temp[col].var() <= VAR_THRESHOLD:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "print(\"drop cols num : {}\".format(len(drop_cols)))\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "pub_test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "GENES_ = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS_ = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS_ = GENES_ + CELLS_\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]\n",
    "        \n",
    "del temp\n",
    "\n",
    "# onehot encode of categorical feature and drop\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "pub_test_df = pd.concat([pd.get_dummies(pub_test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), pub_test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "# aggregation feature\n",
    "print(\"agg\")\n",
    "for df in [train_df, pub_test_df, test_df]:\n",
    "    df[\"sum-g\"] = df[GENES_].sum(axis=1)\n",
    "    df[\"mean-g\"] = df[GENES_].mean(axis=1)\n",
    "    df[\"std-g\"] = df[GENES_].std(axis=1)\n",
    "    df[\"kurt-g\"] = df[GENES_].kurt(axis=1)\n",
    "    df[\"skew-g\"] = df[GENES_].skew(axis=1)\n",
    "    df[\"sum-c\"] = df[CELLS_].sum(axis=1)\n",
    "    df[\"mean-c\"] = df[CELLS_].mean(axis=1)\n",
    "    df[\"std-c\"] = df[CELLS_].std(axis=1)\n",
    "    df[\"kurt-c\"] = df[CELLS_].kurt(axis=1)\n",
    "    df[\"skew-c\"] = df[CELLS_].skew(axis=1)\n",
    "    df[\"sum-gc\"] = df[BIOS_].sum(axis=1)\n",
    "    df[\"mean-gc\"] = df[BIOS_].mean(axis=1)\n",
    "    df[\"std-gc\"] = df[BIOS_].std(axis=1)\n",
    "    df[\"kurt-gc\"] = df[BIOS_].kurt(axis=1)\n",
    "    df[\"skew-gc\"] = df[BIOS_].skew(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y_nonv.drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47812 | val_logits_ll: 0.09802 |  0:00:02s\n",
      "epoch 10 | loss: 0.02007 | val_logits_ll: 0.02047 |  0:00:29s\n",
      "epoch 20 | loss: 0.01803 | val_logits_ll: 0.02061 |  0:00:55s\n",
      "epoch 30 | loss: 0.0176  | val_logits_ll: 0.01853 |  0:01:21s\n",
      "epoch 40 | loss: 0.01716 | val_logits_ll: 0.01836 |  0:01:48s\n",
      "epoch 50 | loss: 0.01675 | val_logits_ll: 0.01828 |  0:02:16s\n",
      "epoch 60 | loss: 0.01621 | val_logits_ll: 0.01764 |  0:02:44s\n",
      "epoch 70 | loss: 0.01575 | val_logits_ll: 0.01782 |  0:03:11s\n",
      "epoch 80 | loss: 0.0153  | val_logits_ll: 0.01808 |  0:03:37s\n",
      "\n",
      "Early stopping occured at epoch 80 with best_epoch = 60 and best_val_logits_ll = 0.01764\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/14_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.41713 | val_logits_ll: 0.07377 |  0:00:02s\n",
      "epoch 10 | loss: 0.02005 | val_logits_ll: 0.01995 |  0:00:29s\n",
      "epoch 20 | loss: 0.01805 | val_logits_ll: 0.02047 |  0:00:56s\n",
      "epoch 30 | loss: 0.01729 | val_logits_ll: 0.01795 |  0:01:23s\n",
      "epoch 40 | loss: 0.01675 | val_logits_ll: 0.01795 |  0:01:49s\n",
      "epoch 50 | loss: 0.01633 | val_logits_ll: 0.01766 |  0:02:16s\n",
      "epoch 60 | loss: 0.01589 | val_logits_ll: 0.01785 |  0:02:43s\n",
      "epoch 70 | loss: 0.01532 | val_logits_ll: 0.018   |  0:03:12s\n",
      "\n",
      "Early stopping occured at epoch 71 with best_epoch = 51 and best_val_logits_ll = 0.01752\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/14_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47162 | val_logits_ll: 0.10151 |  0:00:02s\n",
      "epoch 10 | loss: 0.01956 | val_logits_ll: 0.01998 |  0:00:28s\n",
      "epoch 20 | loss: 0.01784 | val_logits_ll: 0.02203 |  0:00:55s\n",
      "epoch 30 | loss: 0.01732 | val_logits_ll: 0.01834 |  0:01:23s\n",
      "epoch 40 | loss: 0.01694 | val_logits_ll: 0.0182  |  0:01:49s\n",
      "epoch 50 | loss: 0.01664 | val_logits_ll: 0.01837 |  0:02:16s\n",
      "epoch 60 | loss: 0.0163  | val_logits_ll: 0.01802 |  0:02:44s\n",
      "epoch 70 | loss: 0.01604 | val_logits_ll: 0.01803 |  0:03:11s\n",
      "epoch 80 | loss: 0.0159  | val_logits_ll: 0.01787 |  0:03:38s\n",
      "epoch 90 | loss: 0.01519 | val_logits_ll: 0.01799 |  0:04:06s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 72 and best_val_logits_ll = 0.0178\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/14_2.zip\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46131 | val_logits_ll: 0.09448 |  0:00:02s\n",
      "epoch 10 | loss: 0.02006 | val_logits_ll: 0.01938 |  0:00:29s\n",
      "epoch 20 | loss: 0.01802 | val_logits_ll: 0.02179 |  0:00:57s\n",
      "epoch 30 | loss: 0.01751 | val_logits_ll: 0.01788 |  0:01:24s\n",
      "epoch 40 | loss: 0.01708 | val_logits_ll: 0.01749 |  0:01:52s\n",
      "epoch 50 | loss: 0.01643 | val_logits_ll: 0.01776 |  0:02:18s\n",
      "epoch 60 | loss: 0.01619 | val_logits_ll: 0.01718 |  0:02:46s\n",
      "epoch 70 | loss: 0.01569 | val_logits_ll: 0.01732 |  0:03:14s\n",
      "epoch 80 | loss: 0.01516 | val_logits_ll: 0.01749 |  0:03:41s\n",
      "\n",
      "Early stopping occured at epoch 80 with best_epoch = 60 and best_val_logits_ll = 0.01718\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/14_3.zip\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.43121 | val_logits_ll: 0.07714 |  0:00:02s\n",
      "epoch 10 | loss: 0.01993 | val_logits_ll: 0.01969 |  0:00:30s\n",
      "epoch 20 | loss: 0.01789 | val_logits_ll: 0.01922 |  0:00:58s\n",
      "epoch 30 | loss: 0.01727 | val_logits_ll: 0.01798 |  0:01:24s\n",
      "epoch 40 | loss: 0.01684 | val_logits_ll: 0.01876 |  0:01:51s\n",
      "epoch 50 | loss: 0.01651 | val_logits_ll: 0.01774 |  0:02:18s\n",
      "epoch 60 | loss: 0.01608 | val_logits_ll: 0.01804 |  0:02:46s\n",
      "epoch 70 | loss: 0.01561 | val_logits_ll: 0.01774 |  0:03:12s\n",
      "epoch 80 | loss: 0.01516 | val_logits_ll: 0.01795 |  0:03:38s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 61 and best_val_logits_ll = 0.01773\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/14_4.zip\n",
      "seed 14 , cv score : 0.0175747283574728\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.41799 | val_logits_ll: 0.07596 |  0:00:02s\n",
      "epoch 10 | loss: 0.02009 | val_logits_ll: 0.02008 |  0:00:31s\n",
      "epoch 20 | loss: 0.01818 | val_logits_ll: 0.01822 |  0:00:59s\n",
      "epoch 30 | loss: 0.01719 | val_logits_ll: 0.01764 |  0:01:23s\n",
      "epoch 40 | loss: 0.0168  | val_logits_ll: 0.01864 |  0:01:51s\n",
      "epoch 50 | loss: 0.01621 | val_logits_ll: 0.01746 |  0:02:19s\n",
      "epoch 60 | loss: 0.01584 | val_logits_ll: 0.0175  |  0:02:48s\n",
      "epoch 70 | loss: 0.01544 | val_logits_ll: 0.01761 |  0:03:15s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 54 and best_val_logits_ll = 0.01742\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/15_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47585 | val_logits_ll: 0.11453 |  0:00:03s\n",
      "epoch 10 | loss: 0.02028 | val_logits_ll: 0.01988 |  0:00:31s\n",
      "epoch 20 | loss: 0.01863 | val_logits_ll: 0.01896 |  0:00:57s\n",
      "epoch 30 | loss: 0.01766 | val_logits_ll: 0.01844 |  0:01:24s\n",
      "epoch 40 | loss: 0.01711 | val_logits_ll: 0.01797 |  0:01:52s\n",
      "epoch 50 | loss: 0.01663 | val_logits_ll: 0.01786 |  0:02:20s\n",
      "epoch 60 | loss: 0.01635 | val_logits_ll: 0.01756 |  0:02:48s\n",
      "epoch 70 | loss: 0.01606 | val_logits_ll: 0.01753 |  0:03:15s\n",
      "epoch 80 | loss: 0.01561 | val_logits_ll: 0.01744 |  0:03:43s\n",
      "epoch 90 | loss: 0.01521 | val_logits_ll: 0.01768 |  0:04:11s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 75 and best_val_logits_ll = 0.0174\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/15_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45807 | val_logits_ll: 0.08998 |  0:00:02s\n",
      "epoch 10 | loss: 0.01996 | val_logits_ll: 0.01983 |  0:00:30s\n",
      "epoch 20 | loss: 0.01783 | val_logits_ll: 0.02173 |  0:00:58s\n",
      "epoch 30 | loss: 0.01712 | val_logits_ll: 0.01806 |  0:01:26s\n",
      "epoch 40 | loss: 0.01662 | val_logits_ll: 0.01785 |  0:01:54s\n",
      "epoch 50 | loss: 0.01624 | val_logits_ll: 0.0178  |  0:02:20s\n",
      "epoch 60 | loss: 0.01566 | val_logits_ll: 0.01782 |  0:02:48s\n",
      "epoch 70 | loss: 0.01521 | val_logits_ll: 0.01793 |  0:03:16s\n",
      "\n",
      "Early stopping occured at epoch 75 with best_epoch = 55 and best_val_logits_ll = 0.01757\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/15_2.zip\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.43671 | val_logits_ll: 0.08    |  0:00:02s\n",
      "epoch 10 | loss: 0.02    | val_logits_ll: 0.02024 |  0:00:29s\n",
      "epoch 20 | loss: 0.01775 | val_logits_ll: 0.02143 |  0:00:57s\n",
      "epoch 30 | loss: 0.01714 | val_logits_ll: 0.01886 |  0:01:24s\n",
      "epoch 40 | loss: 0.0168  | val_logits_ll: 0.01832 |  0:01:52s\n",
      "epoch 50 | loss: 0.01678 | val_logits_ll: 0.02059 |  0:02:20s\n",
      "epoch 60 | loss: 0.01613 | val_logits_ll: 0.0184  |  0:02:47s\n",
      "epoch 70 | loss: 0.01562 | val_logits_ll: 0.01833 |  0:03:14s\n",
      "\n",
      "Early stopping occured at epoch 72 with best_epoch = 52 and best_val_logits_ll = 0.01822\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/15_3.zip\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46257 | val_logits_ll: 0.09508 |  0:00:02s\n",
      "epoch 10 | loss: 0.01942 | val_logits_ll: 0.01919 |  0:00:29s\n",
      "epoch 20 | loss: 0.01789 | val_logits_ll: 0.02224 |  0:00:57s\n",
      "epoch 30 | loss: 0.01725 | val_logits_ll: 0.02021 |  0:01:25s\n",
      "epoch 40 | loss: 0.01687 | val_logits_ll: 0.01775 |  0:01:52s\n",
      "epoch 50 | loss: 0.01639 | val_logits_ll: 0.01758 |  0:02:19s\n",
      "epoch 60 | loss: 0.01615 | val_logits_ll: 0.01752 |  0:02:47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70 | loss: 0.01561 | val_logits_ll: 0.01768 |  0:03:16s\n",
      "\n",
      "Early stopping occured at epoch 75 with best_epoch = 55 and best_val_logits_ll = 0.0175\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/15_4.zip\n",
      "seed 15 , cv score : 0.01762389289498126\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47551 | val_logits_ll: 0.10341 |  0:00:02s\n",
      "epoch 10 | loss: 0.01984 | val_logits_ll: 0.01973 |  0:00:30s\n",
      "epoch 20 | loss: 0.01799 | val_logits_ll: 0.01856 |  0:00:57s\n",
      "epoch 30 | loss: 0.01764 | val_logits_ll: 0.01827 |  0:01:23s\n",
      "epoch 40 | loss: 0.01724 | val_logits_ll: 0.01789 |  0:01:52s\n",
      "epoch 50 | loss: 0.01687 | val_logits_ll: 0.01789 |  0:02:19s\n",
      "epoch 60 | loss: 0.01648 | val_logits_ll: 0.01768 |  0:02:47s\n",
      "epoch 70 | loss: 0.01626 | val_logits_ll: 0.01777 |  0:03:15s\n",
      "epoch 80 | loss: 0.01586 | val_logits_ll: 0.0177  |  0:03:42s\n",
      "\n",
      "Early stopping occured at epoch 86 with best_epoch = 66 and best_val_logits_ll = 0.01751\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/16_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4605  | val_logits_ll: 0.08767 |  0:00:03s\n",
      "epoch 10 | loss: 0.0201  | val_logits_ll: 0.01998 |  0:00:30s\n",
      "epoch 20 | loss: 0.01801 | val_logits_ll: 0.01905 |  0:00:58s\n",
      "epoch 30 | loss: 0.01735 | val_logits_ll: 0.01855 |  0:01:26s\n",
      "epoch 40 | loss: 0.01677 | val_logits_ll: 0.01804 |  0:01:54s\n",
      "epoch 50 | loss: 0.01632 | val_logits_ll: 0.01796 |  0:02:22s\n",
      "epoch 60 | loss: 0.0161  | val_logits_ll: 0.01775 |  0:02:48s\n",
      "epoch 70 | loss: 0.01553 | val_logits_ll: 0.01772 |  0:03:17s\n",
      "\n",
      "Early stopping occured at epoch 71 with best_epoch = 51 and best_val_logits_ll = 0.01766\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/16_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.42864 | val_logits_ll: 0.07243 |  0:00:02s\n",
      "epoch 10 | loss: 0.01993 | val_logits_ll: 0.01917 |  0:00:29s\n",
      "epoch 20 | loss: 0.01781 | val_logits_ll: 0.01786 |  0:00:57s\n",
      "epoch 30 | loss: 0.01735 | val_logits_ll: 0.0176  |  0:01:23s\n",
      "epoch 40 | loss: 0.01664 | val_logits_ll: 0.01724 |  0:01:51s\n",
      "epoch 50 | loss: 0.01654 | val_logits_ll: 0.01741 |  0:02:19s\n",
      "epoch 60 | loss: 0.01577 | val_logits_ll: 0.01725 |  0:02:45s\n",
      "epoch 70 | loss: 0.01523 | val_logits_ll: 0.01742 |  0:03:12s\n",
      "\n",
      "Early stopping occured at epoch 71 with best_epoch = 51 and best_val_logits_ll = 0.01711\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/16_2.zip\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45924 | val_logits_ll: 0.09429 |  0:00:02s\n",
      "epoch 10 | loss: 0.02014 | val_logits_ll: 0.0205  |  0:00:30s\n",
      "epoch 20 | loss: 0.01819 | val_logits_ll: 0.01997 |  0:00:57s\n",
      "epoch 30 | loss: 0.0176  | val_logits_ll: 0.01837 |  0:01:24s\n",
      "epoch 40 | loss: 0.01708 | val_logits_ll: 0.01819 |  0:01:51s\n",
      "epoch 50 | loss: 0.0167  | val_logits_ll: 0.01793 |  0:02:19s\n",
      "epoch 60 | loss: 0.01639 | val_logits_ll: 0.01785 |  0:02:47s\n",
      "epoch 70 | loss: 0.01607 | val_logits_ll: 0.01759 |  0:03:14s\n",
      "epoch 80 | loss: 0.01569 | val_logits_ll: 0.01779 |  0:03:41s\n",
      "epoch 90 | loss: 0.01524 | val_logits_ll: 0.01788 |  0:04:08s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 70 and best_val_logits_ll = 0.01759\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/16_3.zip\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44466 | val_logits_ll: 0.08066 |  0:00:03s\n",
      "epoch 10 | loss: 0.02006 | val_logits_ll: 0.02147 |  0:00:29s\n",
      "epoch 20 | loss: 0.01811 | val_logits_ll: 0.01903 |  0:00:56s\n",
      "epoch 30 | loss: 0.01742 | val_logits_ll: 0.01917 |  0:01:23s\n",
      "epoch 40 | loss: 0.01711 | val_logits_ll: 0.01839 |  0:01:50s\n",
      "epoch 50 | loss: 0.01661 | val_logits_ll: 0.01801 |  0:02:18s\n",
      "epoch 60 | loss: 0.01614 | val_logits_ll: 0.01796 |  0:02:46s\n",
      "epoch 70 | loss: 0.01599 | val_logits_ll: 0.01814 |  0:03:13s\n",
      "epoch 80 | loss: 0.01549 | val_logits_ll: 0.01813 |  0:03:40s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 65 and best_val_logits_ll = 0.01794\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/16_4.zip\n",
      "seed 16 , cv score : 0.017561799444993514\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46337 | val_logits_ll: 0.09715 |  0:00:02s\n",
      "epoch 10 | loss: 0.02018 | val_logits_ll: 0.0199  |  0:00:30s\n",
      "epoch 20 | loss: 0.01798 | val_logits_ll: 0.01853 |  0:00:58s\n",
      "epoch 30 | loss: 0.01744 | val_logits_ll: 0.01989 |  0:01:25s\n",
      "epoch 40 | loss: 0.0171  | val_logits_ll: 0.0179  |  0:01:52s\n",
      "epoch 50 | loss: 0.01659 | val_logits_ll: 0.01785 |  0:02:19s\n",
      "epoch 60 | loss: 0.01626 | val_logits_ll: 0.01779 |  0:02:45s\n",
      "epoch 70 | loss: 0.0161  | val_logits_ll: 0.0177  |  0:03:11s\n",
      "epoch 80 | loss: 0.01548 | val_logits_ll: 0.01779 |  0:03:38s\n",
      "\n",
      "Early stopping occured at epoch 82 with best_epoch = 62 and best_val_logits_ll = 0.01758\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/17_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.42817 | val_logits_ll: 0.07118 |  0:00:02s\n",
      "epoch 10 | loss: 0.01976 | val_logits_ll: 0.01943 |  0:00:29s\n",
      "epoch 20 | loss: 0.01779 | val_logits_ll: 0.01872 |  0:00:55s\n",
      "epoch 30 | loss: 0.01706 | val_logits_ll: 0.01864 |  0:01:22s\n",
      "epoch 40 | loss: 0.01652 | val_logits_ll: 0.01798 |  0:01:48s\n",
      "epoch 50 | loss: 0.01623 | val_logits_ll: 0.01762 |  0:02:15s\n",
      "epoch 60 | loss: 0.01583 | val_logits_ll: 0.01767 |  0:02:42s\n",
      "epoch 70 | loss: 0.01512 | val_logits_ll: 0.01772 |  0:03:09s\n",
      "\n",
      "Early stopping occured at epoch 72 with best_epoch = 52 and best_val_logits_ll = 0.01757\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/17_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46013 | val_logits_ll: 0.09769 |  0:00:02s\n",
      "epoch 10 | loss: 0.0197  | val_logits_ll: 0.02084 |  0:00:29s\n",
      "epoch 20 | loss: 0.01772 | val_logits_ll: 0.01864 |  0:00:55s\n",
      "epoch 30 | loss: 0.01719 | val_logits_ll: 0.01835 |  0:01:20s\n",
      "epoch 40 | loss: 0.01664 | val_logits_ll: 0.01821 |  0:01:48s\n",
      "epoch 50 | loss: 0.01622 | val_logits_ll: 0.01819 |  0:02:15s\n",
      "epoch 60 | loss: 0.01594 | val_logits_ll: 0.01822 |  0:02:42s\n",
      "epoch 70 | loss: 0.01526 | val_logits_ll: 0.01871 |  0:03:09s\n",
      "epoch 80 | loss: 0.0149  | val_logits_ll: 0.01857 |  0:03:36s\n",
      "\n",
      "Early stopping occured at epoch 84 with best_epoch = 64 and best_val_logits_ll = 0.01814\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/17_2.zip\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44304 | val_logits_ll: 0.07897 |  0:00:02s\n",
      "epoch 10 | loss: 0.02001 | val_logits_ll: 0.0195  |  0:00:28s\n",
      "epoch 20 | loss: 0.01796 | val_logits_ll: 0.01895 |  0:00:55s\n",
      "epoch 30 | loss: 0.01743 | val_logits_ll: 0.01787 |  0:01:23s\n",
      "epoch 40 | loss: 0.01722 | val_logits_ll: 0.01758 |  0:01:49s\n",
      "epoch 50 | loss: 0.01663 | val_logits_ll: 0.01729 |  0:02:16s\n",
      "epoch 60 | loss: 0.0163  | val_logits_ll: 0.01731 |  0:02:43s\n",
      "epoch 70 | loss: 0.01584 | val_logits_ll: 0.0176  |  0:03:09s\n",
      "\n",
      "Early stopping occured at epoch 78 with best_epoch = 58 and best_val_logits_ll = 0.0172\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/17_3.zip\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45559 | val_logits_ll: 0.09023 |  0:00:03s\n",
      "epoch 10 | loss: 0.01984 | val_logits_ll: 0.02005 |  0:00:29s\n",
      "epoch 20 | loss: 0.01802 | val_logits_ll: 0.02121 |  0:00:56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.01739 | val_logits_ll: 0.01819 |  0:01:22s\n",
      "epoch 40 | loss: 0.01692 | val_logits_ll: 0.01815 |  0:01:49s\n",
      "epoch 50 | loss: 0.01696 | val_logits_ll: 0.0179  |  0:02:16s\n",
      "epoch 60 | loss: 0.0163  | val_logits_ll: 0.0187  |  0:02:43s\n",
      "epoch 70 | loss: 0.01633 | val_logits_ll: 0.01771 |  0:03:09s\n",
      "epoch 80 | loss: 0.01578 | val_logits_ll: 0.01783 |  0:03:37s\n",
      "epoch 90 | loss: 0.01528 | val_logits_ll: 0.01787 |  0:04:04s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 71 and best_val_logits_ll = 0.01751\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/17_4.zip\n",
      "seed 17 , cv score : 0.017601160001146\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.42772 | val_logits_ll: 0.07401 |  0:00:02s\n",
      "epoch 10 | loss: 0.01971 | val_logits_ll: 0.01964 |  0:00:29s\n",
      "epoch 20 | loss: 0.01787 | val_logits_ll: 0.01852 |  0:00:57s\n",
      "epoch 30 | loss: 0.01721 | val_logits_ll: 0.01819 |  0:01:24s\n",
      "epoch 40 | loss: 0.01677 | val_logits_ll: 0.01814 |  0:01:52s\n",
      "epoch 50 | loss: 0.01632 | val_logits_ll: 0.01801 |  0:02:19s\n",
      "epoch 60 | loss: 0.01582 | val_logits_ll: 0.01777 |  0:02:45s\n",
      "epoch 70 | loss: 0.01553 | val_logits_ll: 0.01819 |  0:03:10s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 54 and best_val_logits_ll = 0.01765\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/18_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45731 | val_logits_ll: 0.0951  |  0:00:03s\n",
      "epoch 10 | loss: 0.01994 | val_logits_ll: 0.01966 |  0:00:29s\n",
      "epoch 20 | loss: 0.01826 | val_logits_ll: 0.0186  |  0:00:56s\n",
      "epoch 30 | loss: 0.01738 | val_logits_ll: 0.01992 |  0:01:22s\n",
      "epoch 40 | loss: 0.01679 | val_logits_ll: 0.01777 |  0:01:48s\n",
      "epoch 50 | loss: 0.0163  | val_logits_ll: 0.01775 |  0:02:15s\n",
      "epoch 60 | loss: 0.01594 | val_logits_ll: 0.01753 |  0:02:43s\n",
      "epoch 70 | loss: 0.01545 | val_logits_ll: 0.01771 |  0:03:09s\n",
      "epoch 80 | loss: 0.01504 | val_logits_ll: 0.01767 |  0:03:36s\n",
      "\n",
      "Early stopping occured at epoch 80 with best_epoch = 60 and best_val_logits_ll = 0.01753\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/18_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44923 | val_logits_ll: 0.08809 |  0:00:02s\n",
      "epoch 10 | loss: 0.02    | val_logits_ll: 0.01931 |  0:00:30s\n",
      "epoch 20 | loss: 0.01796 | val_logits_ll: 0.02029 |  0:00:57s\n",
      "epoch 30 | loss: 0.01736 | val_logits_ll: 0.01999 |  0:01:25s\n",
      "epoch 40 | loss: 0.01684 | val_logits_ll: 0.01819 |  0:01:52s\n",
      "epoch 50 | loss: 0.01652 | val_logits_ll: 0.01755 |  0:02:19s\n",
      "epoch 60 | loss: 0.01588 | val_logits_ll: 0.01743 |  0:02:46s\n",
      "epoch 70 | loss: 0.01557 | val_logits_ll: 0.01745 |  0:03:14s\n",
      "epoch 80 | loss: 0.01538 | val_logits_ll: 0.01748 |  0:03:42s\n",
      "epoch 90 | loss: 0.01469 | val_logits_ll: 0.01767 |  0:04:09s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 72 and best_val_logits_ll = 0.01726\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/18_2.zip\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46312 | val_logits_ll: 0.0981  |  0:00:02s\n",
      "epoch 10 | loss: 0.01987 | val_logits_ll: 0.02002 |  0:00:29s\n",
      "epoch 20 | loss: 0.01787 | val_logits_ll: 0.0224  |  0:00:57s\n",
      "epoch 30 | loss: 0.01775 | val_logits_ll: 0.01863 |  0:01:24s\n",
      "epoch 40 | loss: 0.01724 | val_logits_ll: 0.01862 |  0:01:51s\n",
      "epoch 50 | loss: 0.01677 | val_logits_ll: 0.01841 |  0:02:18s\n",
      "epoch 60 | loss: 0.0167  | val_logits_ll: 0.01825 |  0:02:45s\n",
      "epoch 70 | loss: 0.01615 | val_logits_ll: 0.01807 |  0:03:13s\n",
      "epoch 80 | loss: 0.0159  | val_logits_ll: 0.01813 |  0:03:41s\n",
      "epoch 90 | loss: 0.01553 | val_logits_ll: 0.01815 |  0:04:06s\n",
      "\n",
      "Early stopping occured at epoch 94 with best_epoch = 74 and best_val_logits_ll = 0.01796\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/18_3.zip\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.46703 | val_logits_ll: 0.09133 |  0:00:02s\n",
      "epoch 10 | loss: 0.01948 | val_logits_ll: 0.01929 |  0:00:30s\n",
      "epoch 20 | loss: 0.01796 | val_logits_ll: 0.02064 |  0:00:58s\n",
      "epoch 30 | loss: 0.01752 | val_logits_ll: 0.02001 |  0:01:26s\n",
      "epoch 40 | loss: 0.01731 | val_logits_ll: 0.01806 |  0:01:52s\n",
      "epoch 50 | loss: 0.01686 | val_logits_ll: 0.01768 |  0:02:20s\n",
      "epoch 60 | loss: 0.01638 | val_logits_ll: 0.01777 |  0:02:47s\n",
      "epoch 70 | loss: 0.01629 | val_logits_ll: 0.01762 |  0:03:15s\n",
      "epoch 80 | loss: 0.01586 | val_logits_ll: 0.0178  |  0:03:43s\n",
      "epoch 90 | loss: 0.01543 | val_logits_ll: 0.01778 |  0:04:11s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 72 and best_val_logits_ll = 0.01761\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/18_4.zip\n",
      "seed 18 , cv score : 0.017602071354772025\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45992 | val_logits_ll: 0.10086 |  0:00:02s\n",
      "epoch 10 | loss: 0.01968 | val_logits_ll: 0.01916 |  0:00:30s\n",
      "epoch 20 | loss: 0.018   | val_logits_ll: 0.02035 |  0:00:58s\n",
      "epoch 30 | loss: 0.01756 | val_logits_ll: 0.01996 |  0:01:24s\n",
      "epoch 40 | loss: 0.0168  | val_logits_ll: 0.01783 |  0:01:52s\n",
      "epoch 50 | loss: 0.01651 | val_logits_ll: 0.01765 |  0:02:19s\n",
      "epoch 60 | loss: 0.01616 | val_logits_ll: 0.01749 |  0:02:45s\n",
      "epoch 70 | loss: 0.01567 | val_logits_ll: 0.01754 |  0:03:14s\n",
      "epoch 80 | loss: 0.01507 | val_logits_ll: 0.0178  |  0:03:41s\n",
      "\n",
      "Early stopping occured at epoch 80 with best_epoch = 60 and best_val_logits_ll = 0.01749\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/19_0.zip\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44683 | val_logits_ll: 0.0855  |  0:00:03s\n",
      "epoch 10 | loss: 0.02022 | val_logits_ll: 0.01958 |  0:00:32s\n",
      "epoch 20 | loss: 0.01807 | val_logits_ll: 0.01803 |  0:01:02s\n",
      "epoch 30 | loss: 0.01738 | val_logits_ll: 0.01883 |  0:01:31s\n",
      "epoch 40 | loss: 0.01697 | val_logits_ll: 0.01762 |  0:01:59s\n",
      "epoch 50 | loss: 0.01642 | val_logits_ll: 0.01746 |  0:02:28s\n",
      "epoch 60 | loss: 0.01606 | val_logits_ll: 0.01744 |  0:02:57s\n",
      "\n",
      "Early stopping occured at epoch 62 with best_epoch = 42 and best_val_logits_ll = 0.01723\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_weights2/19_1.zip\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "feature num is 927\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.45861 | val_logits_ll: 0.09432 |  0:00:04s\n",
      "epoch 10 | loss: 0.01943 | val_logits_ll: 0.0199  |  0:00:34s\n",
      "epoch 20 | loss: 0.01757 | val_logits_ll: 0.02017 |  0:01:06s\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 200\n",
    "#MAX_EPOCH = 25\n",
    "tabnet_params = dict(\n",
    "    n_d = 21,\n",
    "    n_a = 21,\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    \n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    scheduler_params = dict(mode = \"min\", patience = 3, min_lr = 1e-5, factor = 0.9),\n",
    "    #scheduler_fn = optim.lr_scheduler.OneCycleLR,\n",
    "    #scheduler_params = dict(pct_start=0.1, div_factor=1e3, max_lr=1e-1, epochs=MAX_EPOCH),\n",
    "    \n",
    "    seed = 42,\n",
    "    verbose = 10\n",
    ")\n",
    "\n",
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_df.shape[0], y_nonv.shape[1]))\n",
    "imps = []\n",
    "imp_cols = []\n",
    "folds = []\n",
    "test_cv_preds = []\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 5\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # LOAD LIBRARIES\n",
    "    targets = SCORED_MOAS.copy()\n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = scored[\"drug_id\"].value_counts()\n",
    "    vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "    vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values} # drug id がどのフォールドに属すか格納\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.loc[scored[\"drug_id\"].isin(vc2)].reset_index(drop=True)\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp[\"sig_id\"][idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN K\n",
    "    scored['fold'] = scored.drug_id.map(dct1)\n",
    "    scored.loc[scored[\"fold\"].isna(),'fold'] = scored.loc[scored[\"fold\"].isna(),'sig_id'].map(dct2)\n",
    "    scored[\"fold\"] = scored[\"fold\"].astype('int8')\n",
    "    ###############################################################################################\n",
    "\n",
    "    \n",
    "\n",
    "    #for fold, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):\n",
    "    for fold in range(K):\n",
    "        train_index = scored[scored[\"fold\"] != fold].index.to_list()\n",
    "        valid_index = scored[scored[\"fold\"] == fold].index.to_list()\n",
    "        print(\"======================== fold {} ========================\".format(fold+1))\n",
    "        tabnet_params[\"seed\"] = seed+(fold+1)\n",
    "        folds.append(valid_index)\n",
    "                \n",
    "        # split data\n",
    "        train_X = X.iloc[train_index]\n",
    "        train_y = y_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index]\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        test_X = (test_df.drop(\"sig_id\", axis=1))\n",
    "        pub_test_X = (pub_test_df.drop(\"sig_id\", axis=1))\n",
    "        \n",
    "\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        # scaler\n",
    "        print(SCALE)\n",
    "        scale_cols = BIOS_+PRODS\n",
    "        scaler = make_scaler(SCALE, seed).fit(train_X.append(pub_test_X)[scale_cols])\n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "\n",
    "        \n",
    "        print(\"PCA\")\n",
    "        #PCA\n",
    "        n_decom_g = 80\n",
    "        n_decom_c = 10\n",
    "        decom_g_cols = [f\"pca_g-{i}\" for i in range(n_decom_g)]\n",
    "        decom_c_cols = [f\"pca_c-{i}\" for i in range(n_decom_c)]\n",
    "        \n",
    "        pca_g = PCA(n_components = n_decom_g, random_state = seed).fit(train_X.append(pub_test_X)[GENES_])\n",
    "        pca_c = PCA(n_components = n_decom_c, random_state = seed).fit(train_X.append(pub_test_X)[CELLS_])\n",
    "        \n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[decom_g_cols] = pca_g.transform(df[GENES_])\n",
    "            df[decom_c_cols] = pca_c.transform(df[CELLS_])\n",
    "            #df.drop(GENES_+CELLS_, axis=1, inplace=True)        \n",
    "\n",
    "        # kmeans\n",
    "        #print(\"kmeans clustering\")\n",
    "        #n_clusters = 5\n",
    "        #kmeans = KMeans(n_clusters=n_clusters, random_state=seed).fit(train_X.append(pub_test_X)[GENES_])  \n",
    "        #for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "        #    df[\"kmeans\"] = kmeans.predict(df[GENES_])\n",
    "\n",
    "        #train_X = pd.concat([pd.get_dummies(train_X[\"kmeans\"], prefix=\"kmeans\", drop_first=True), train_X.drop(\"kmeans\", axis=1) ], axis=1)\n",
    "        #valid_X = pd.concat([pd.get_dummies(valid_X[\"kmeans\"], prefix=\"kmeans\", drop_first=True), valid_X.drop(\"kmeans\", axis=1) ], axis=1)\n",
    "        #test_X = pd.concat([pd.get_dummies(test_X[\"kmeans\"], prefix=\"kmeans\", drop_first=True), test_X.drop(\"kmeans\", axis=1) ], axis=1)\n",
    "        #pub_test_X = pd.concat([pd.get_dummies(pub_test_X[\"kmeans\"], prefix=\"kmeans\", drop_first=True), pub_test_X.drop(\"kmeans\", axis=1) ], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        print(f\"feature num is {train_X.shape[1]}\")\n",
    "        imp_cols = train_X.columns\n",
    "        # prepare data for training\n",
    "        train_X = train_X.values\n",
    "        valid_X = valid_X.values\n",
    "        test_X = test_X.values\n",
    "        \n",
    "        \n",
    "        # model training\n",
    "        #tabnet_params[\"scheduler_params\"][\"steps_per_epoch\"] = int(np.ceil(train_X.shape[0]/1024))\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "        device = torch.device(\"cuda:1\")\n",
    "        model.device=device\n",
    "\n",
    "        model.fit(\n",
    "            X_train=train_X,\n",
    "            y_train=train_y,\n",
    "            eval_set=[(valid_X, valid_y)],\n",
    "            eval_name = [\"val\"],\n",
    "            eval_metric = [\"logits_ll\"],\n",
    "            max_epochs=MAX_EPOCH,\n",
    "            patience=20, \n",
    "            batch_size=1024, \n",
    "            virtual_batch_size=32,\n",
    "            num_workers=1, \n",
    "            drop_last=False,\n",
    "            #loss_fn=F.binary_cross_entropy_with_logits\n",
    "            #loss_fn=LabelSmoothing(1e-5)\n",
    "            loss_fn=LabelSmoothing(1e-5)\n",
    "        )\n",
    "        #model.load_model(\"../input/tabnetweights/tabnet_weights/{}_{}\".format(seed, fold))\n",
    "\n",
    "        val_preds = 1 / (1 + np.exp(-model.predict(valid_X)))\n",
    "        train_pred[valid_index] +=  val_preds\n",
    "        \n",
    "        test_preds = 1 / (1 + np.exp(-model.predict(test_X)))\n",
    "        preds += test_preds / (K*len(seeds))\n",
    "        \n",
    "        name = \"{}_{}\".format(seed, fold)\n",
    "        model.save_model(\"tabnet_weights2/\"+name)\n",
    "        imps.append(model.feature_importances_)\n",
    "\n",
    "        \n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE,  y.shape[1]))\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "\n",
    "\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"../../../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "cols = [col for col in train_sub.columns if col != \"sig_id\"]\n",
    "train_sub[cols] = train_preds2\n",
    "train_sub.to_csv(\"train_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(np.array(imps).sum(axis=0), index=imp_cols, columns=['importance']).sort_values('importance', ascending=False)\n",
    "importance[:500]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cols = []\n",
    "size = 500\n",
    "for col in importance[:size].index:\n",
    "        if col.startswith(\"prod-\"):\n",
    "            col = col.replace(\"prod-\", \"\")\n",
    "            product_cols.append(col.split(\" * \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zip tabnet_weights.zip tabnet_weights/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
