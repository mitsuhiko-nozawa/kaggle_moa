{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex1 : cv score : 0.015197305813776704  \n",
    "..........................................................................................................  \n",
    "ex2:  \n",
    "not scaling, remove vehicle, not use nonscored_target  : 0.015448837474049428  \n",
    "not scaling, remove vehicle, not use nonscored_target, 7 seeds  : cv score : 0.015381427768256765  \n",
    "not scaling, remove vehicle, not use nonscored_target, drop high corr cols  : 0.01545810170388885\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 60% : 0.015533423204263394\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 50% : 0.015430995160848253\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 40% : 0.015354553654126092\n",
    "all : 0.01515220913930692(7 seeds)\n",
    "all : 0.015207047278156766 (10 seeds)\n",
    "all : model tuning + 3 layer (7seeds) cv score : 0.014899280863407222,  public : 0.01877 (←　0.01907, target 0.1840)\n",
    "\n",
    "混ぜるほど良いスコア。。。？  \n",
    "high corr な特徴落としたら少し悪化\n",
    "95%追加で少し改善、なんか全部やった方がいい説\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "#from Models.DNN import DNN_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def make_scaler(flag):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "SCALE = \"quantile\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate, Embedding, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import AdamW, Lookahead\n",
    "\n",
    "\n",
    "def DNN_3lmodel(input_size, output_size):\n",
    "    inp = Input(shape = (input_size, ))\n",
    "    \n",
    "    x = BatchNormalization()(inp)\n",
    "    x = Dropout(0.4914099166744246)(x)\n",
    "    x = WeightNormalization(Dense(1159, activation = 'relu'))(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.18817607797795838)(x)\n",
    "    x = WeightNormalization(Dense(960, activation = 'relu'))(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.12542057776853896)(x)\n",
    "    x = WeightNormalization(Dense(1811, activation = 'relu'))(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20175242230280122)(x)\n",
    "    out = WeightNormalization(Dense(output_size, activation = 'sigmoid'))(x)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    opt = Lookahead(opt, sync_period = 10)\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def DNN_model(input_size, output_size):\n",
    "    inputs = Input((input_size, ))\n",
    "\n",
    "    outputs = BatchNormalization()(inputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = WeightNormalization(Dense(output_size, activation=\"sigmoid\"))(outputs)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=AdamW(lr=0.001, weight_decay=1e-5, clipvalue=756), \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015)\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../Data/Raw/test_features.csv\")\n",
    "\n",
    "y = pd.read_csv(\"../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "CELLS_50 = CELLS[:50]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\"]\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "train_sigid = train_df[\"sig_id\"]\n",
    "test_sigid = test_df[\"sig_id\"]\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3982\n"
     ]
    }
   ],
   "source": [
    "print(TE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"g_li = [['g-616', 'g-48'], ['g-432',  'g-360',  'g-135',  'g-257',  'g-503',  'g-664',  'g-486',  'g-498',  'g-491',  'g-433',  'g-761'], ['g-443', 'g-566', 'g-615'], ['g-597', 'g-256'], ['g-90'], ['g-406'], ['g-525', 'g-272'], ['g-235'], ['g-392', 'g-708', 'g-689'], ['g-122', 'g-175', 'g-202'], ['g-547'], ['g-392', 'g-423', 'g-122'], ['g-302'], ['g-27', 'g-10', 'g-266'], ['g-635', 'g-745', 'g-108', 'g-626', 'g-0', 'g-8'], ['g-392', 'g-229', 'g-117', 'g-718'], ['g-566'], ['g-424', 'g-207', 'g-39', 'g-768', 'g-317'], ['g-392', 'g-206', 'g-600'], ['g-466', 'g-449'], ['g-2'], ['g-392',  'g-90',  'g-113',  'g-744',  'g-135',  'g-353',  'g-91',  'g-65',  'g-724',  'g-414'], ['g-443', 'g-322', 'g-211'], ['g-753', 'g-7', 'g-401', 'g-593', 'g-291'], ['g-178', 'g-435'], ['g-322', 'g-206'], ['g-27', 'g-600'], ['g-431',  'g-522',  'g-75',  'g-207',  'g-119',  'g-279',  'g-22',  'g-47',  'g-167',  'g-102',  'g-769',  'g-221'], ['g-104', 'g-587'], ['g-322', 'g-142'], ['g-569', 'g-690'], ['g-450'], ['g-241'], ['g-335'], ['g-206'], ['g-91', 'g-90'], ['g-606', 'g-484'], ['g-731', 'g-105'], ['g-596'], ['g-537', 'g-479'], ['g-446'], ['g-739'], ['g-466', 'g-38'], ['g-482', 'g-197', 'g-122'], ['g-150', 'g-29', 'g-570', 'g-58', 'g-764'], ['g-48'], ['g-466', 'g-428'], ['g-699',  'g-207',  'g-594',  'g-614',  'g-124',  'g-150',  'g-409',  'g-179',  'g-146',  'g-616',  'g-492'], ['g-70', 'g-628'], ['g-153'], ['g-75', 'g-696', 'g-431', 'g-102', 'g-207', 'g-697'], ['g-546', 'g-524', 'g-470'], ['g-628', 'g-701', 'g-376'], ['g-381', 'g-718', 'g-642'], ['g-417'], ['g-355'], ['g-94', 'g-636', 'g-335'], ['g-402', 'g-133'], ['g-368', 'g-481'], ['g-357'], ['g-122', 'g-210', 'g-206'], ['g-500'], ['g-355', 'g-201', 'g-50', 'g-63'], ['g-84', 'g-392', 'g-135'], ['g-615', 'g-417'], ['g-594'], ['g-362'], ['g-344', 'g-82', 'g-351'], ['g-134'], ['g-421'], ['g-29', 'g-47', 'g-670', 'g-117', 'g-525', 'g-36'], ['g-153', 'g-740', 'g-138'], ['g-601', 'g-664'], ['g-138', 'g-170', 'g-414', 'g-17', 'g-75', 'g-84', 'g-117'], ['g-405', 'g-583', 'g-367'], ['g-699',  'g-492',  'g-174',  'g-165',  'g-150',  'g-664',  'g-48',  'g-323',  'g-290',  'g-410',  'g-720',  'g-724',  'g-81'], ['g-75', 'g-178'], ['g-470', 'g-311', 'g-200'], ['g-392', 'g-175', 'g-593', 'g-14'], ['g-60', 'g-497'], ['g-169', 'g-29'], ['g-623', 'g-394', 'g-465'], ['g-533'], ['g-373'], ['g-392', 'g-175'], ['g-329', 'g-574', 'g-536', 'g-206'], ['g-431', 'g-128', 'g-75', 'g-187'], ['g-258', 'g-630', 'g-226'], ['g-36', 'g-675', 'g-207', 'g-761', 'g-554', 'g-59'], ['g-345', 'g-489']]\n",
    "for li in g_li:\n",
    "    if len(li) <= 1:\n",
    "        continue\n",
    "    for i in range(len(li)-1):\n",
    "        for j in range(i+1, len(li)):\n",
    "            col1 = li[i]\n",
    "            col2 = li[j]\n",
    "            train_df[\"p-\"+col1+\"-\"+col2] = train_df[col1] * train_df[col2]\n",
    "            test_df[\"p-\"+col1+\"-\"+col2] = test_df[col1] * test_df[col2]\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"p-\")]\n",
    "BIOS += PRODS\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "drop_cols = [\n",
    "    'g-37', 'g-38', 'g-50', 'g-63', 'g-121', 'g-123', 'g-195', 'g-228', 'g-248', 'g-261', 'g-328', 'g-349', \n",
    "    'g-369', 'g-460', 'c-1', 'c-2', 'c-4', 'c-6', 'c-8', 'c-10', 'c-11', 'c-13', 'c-17', 'c-18', 'c-26', 'c-31', 'c-33', \n",
    "    'c-38', 'c-40', 'c-42', 'c-51', 'c-52', 'c-55', 'c-60', 'c-66', 'c-73', 'c-75'\n",
    "]\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "BIOS = list(set(BIOS) - set(drop_cols))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 片側95%に入ったときに50%以上の確率で判別できるMoAの組み合わせが存在した列\n",
    "n_cols = ['g-270', 'g-170', 'g-718', 'g-163', 'g-377', 'g-712', 'g-529', 'g-178', 'g-251', 'g-74', 'g-424', 'g-330', 'g-97', 'g-417', 'g-144', 'g-689', 'g-215', 'g-317', 'g-201', 'g-731', 'g-300', 'g-500', 'g-368', 'g-578', 'g-468', 'g-338', 'g-296', 'g-157', 'g-116', 'g-478', 'g-629', 'g-100', 'g-65', 'g-243', 'g-53', 'g-443', 'g-226', 'g-46', 'g-158', 'g-389', 'g-577', 'g-147', 'g-90', 'g-131', 'g-745', 'g-750', 'g-619', 'g-646', 'g-12', 'g-130', 'g-392', 'g-106', 'g-419', 'g-768', 'g-386', 'g-596', 'g-31', 'g-546', 'g-600', 'g-541', 'g-512', 'g-723', 'g-764', 'g-208', 'g-467', 'g-264', 'g-148', 'g-86', 'g-744', 'g-266', 'g-91', 'g-146', 'g-102']\n",
    "p_cols = ['c-16', 'c-57', 'g-726', 'c-53', 'g-423', 'c-97', 'g-624', 'g-178', 'g-257', 'c-29', 'c-50', 'g-128', 'c-63', 'g-674', 'c-99', 'c-43', 'c-46', 'c-72', 'c-20', 'c-39', 'g-439', 'g-390', 'g-540', 'c-64', 'c-47', 'c-89', 'g-497', 'c-95', 'g-55', 'g-293', 'g-643', 'g-759', 'g-590', 'g-411', 'g-508', 'c-71', 'g-683', 'c-81', 'g-534', 'g-503', 'g-298', 'g-760', 'g-231', 'g-300', 'c-82', 'c-22', 'g-276', 'c-92', 'g-306', 'c-86', 'c-59', 'c-88', 'c-3', 'g-250', 'g-67', 'g-428', 'g-691', 'c-19', 'g-489', 'g-667', 'c-34', 'c-54', 'c-27', 'c-44', 'g-385', 'c-62', 'g-769', 'g-704', 'g-353', 'c-68', 'c-84', 'c-7', 'g-332', 'c-28', 'g-735', 'c-98', 'c-12', 'c-78', 'c-24', 'c-79', 'g-558', 'g-491', 'c-65', 'g-644', 'g-370', 'c-48', 'c-41', 'c-56', 'c-30', 'c-85', 'c-21', 'g-40', 'c-94', 'c-49', 'c-35', 'c-96', 'c-5', 'c-61', 'g-743', 'g-62', 'g-8', 'c-93', 'c-37', 'c-70', 'g-705', 'g-438', 'c-0', 'c-36', 'c-90', 'g-66', 'g-185', 'g-421', 'g-291', 'g-321', 'c-80', 'g-75', 'g-202', 'g-685', 'c-67', 'c-83', 'g-161', 'c-15', 'g-568', 'c-32', 'g-375', 'g-513', 'g-72', 'g-58', 'c-87', 'c-25', 'g-477', 'g-406', 'c-14', 'c-77', 'g-113', 'g-588', 'g-672', 'g-112', 'g-204', 'c-91', 'c-45']\n",
    "for col in n_cols:\n",
    "    train_df[\"n_\"+col] = train_df[col].map(lambda x : 1 if x <= -9.5 else 0)\n",
    "    test_df[\"n_\"+col] = test_df[col].map(lambda x : 1 if x <= -9.5 else 0)\n",
    "    \n",
    "for col in p_cols:\n",
    "    train_df[\"p_\"+col] = train_df[col].map(lambda x : 1 if x >= 9.5 else 0)\n",
    "    test_df[\"p_\"+col] = test_df[col].map(lambda x : 1 if x >= 9.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_df[\"time_dose\"].unique():\n",
    "    temp = pd.concat([train_df[train_df[\"time_dose\"] == val], test_df[test_df[\"time_dose\"] == val]])\n",
    "    scaler = make_scaler(SCALE)\n",
    "    scaler.fit(temp[BIOS].values.reshape(-1, len(BIOS)))\n",
    "    tr_ind = train_df[train_df[\"time_dose\"] == val].index\n",
    "    te_ind = test_df[test_df[\"time_dose\"] == val].index\n",
    "    tr_trans = scaler.transform(train_df[BIOS].iloc[tr_ind].values.reshape(-1, len(BIOS))).T\n",
    "    te_trans = scaler.transform(test_df[BIOS].iloc[te_ind].values.reshape(-1, len(BIOS))).T\n",
    "    print(\"transformed\")\n",
    "    for i, col in enumerate(BIOS):\n",
    "        train_df[col].iloc[tr_ind] = tr_trans[i]\n",
    "        test_df[col].iloc[te_ind] = te_trans[i]\n",
    "\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\"), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\"), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print()\n",
    "\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "test_X = test_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y[mask].drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "print(X.shape)\n",
    "print(test_X.shape)\n",
    "print(y_nonv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_X.shape[0], y_nonv.shape[1]))\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 5\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    for itr, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):\n",
    "        print(\"======================== fold {} ========================\".format(itr+1))\n",
    "        train_X = X.iloc[train_index].values\n",
    "        train_y = y_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index].values\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        \n",
    "        model = DNN_3lmodel(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        #model = DNN_model(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        cb = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                                          mode = 'min',\n",
    "                                                          patience = 10,\n",
    "                                                          restore_best_weights = False,)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                         mode = 'min',\n",
    "                                                         factor = 0.3,\n",
    "                                                         patience = 3,)\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'_{itr}_{seed}.h5',\n",
    "                                                        monitor='val_loss',\n",
    "                                                        save_best_only = True,\n",
    "                                                        save_weights_only = True)\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            train_X, \n",
    "            train_y,\n",
    "            batch_size=128,\n",
    "            epochs=80,\n",
    "            verbose=1,\n",
    "            #callbacks=[cb],\n",
    "            callbacks = [early_stopping, reduce_lr,  checkpoint],\n",
    "            validation_data=(valid_X, valid_y),\n",
    "        )\n",
    "        model.load_weights(f'_{itr}_{seed}.h5')\n",
    "        \n",
    "        train_pred[valid_index] += model.predict(valid_X, batch_size=128)\n",
    "        preds += model.predict(test_X.values, batch_size=128) / (K*len(seeds))\n",
    "\n",
    "\n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE, y.shape[1]))\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [:,:tag_size]\n",
    "sub_df = pd.read_csv(\"../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "#sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_preds.copy()\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))\n",
    "t_ = train_df[train_df[\"cp_type\"] == 0]\n",
    "t[t_.index] = np.zeros((t_.shape[0], t.shape[1]))\n",
    "t = np.where(t > 1, 1, t)\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "not_li = []\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(y.shape[1]):\n",
    "        if y[i][j] == 1:\n",
    "            #print(\"====={}, {}====\".format(i,j))\n",
    "            rank = np.where(train_preds[i].argsort()[::-1] == j)[0][0]+1\n",
    "            #print(\"rank {}\".format(rank))\n",
    "            if rank <= 20:\n",
    "                li.append(j)\n",
    "            else:\n",
    "                not_li.append(j)\n",
    "            #print(train_preds[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(set(li))\n",
    "not_li = list(set(not_li))\n",
    "for i in li:\n",
    "    if i not in not_li:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\").columns\n",
    "pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\")[cols[256-206]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
