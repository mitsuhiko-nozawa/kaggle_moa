{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex1 : cv score : 0.015197305813776704  \n",
    "..........................................................................................................  \n",
    "ex2:  \n",
    "not scaling, remove vehicle, not use nonscored_target  : 0.015448837474049428  \n",
    "not scaling, remove vehicle, not use nonscored_target, 7 seeds  : cv score : 0.015381427768256765  \n",
    "not scaling, remove vehicle, not use nonscored_target, drop high corr cols  : 0.01545810170388885\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 60% : 0.015533423204263394\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 50% : 0.015430995160848253\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 40% : 0.015354553654126092\n",
    "all : 0.01515220913930692(7 seeds)\n",
    "all : 0.015207047278156766 (10 seeds)\n",
    "all : model tuning + 3 layer (7seeds) cv score : 0.014899280863407222 (LB : 0.1877)\n",
    "\n",
    "混ぜるほど良いスコア。。。？  \n",
    "high corr な特徴落としたら少し悪化\n",
    "95%追加で少し改善、なんか全部やった方がいい説\n",
    "..........................................................................................................  \n",
    "ex3:\n",
    "not scaling, remove vehicle, not use nonscored_target, use DAE(variance 0.5,　0.2776,　0.2227)  : 0.015192768673310545\n",
    "not scaling, remove vehicle, not use nonscored_target, use DAE(variance 1.0,　0.3396,　0.2673)  : 0.015260472747032183 \n",
    "\n",
    "not scaling, remove vehicle, not use nonscored_target, use DAE2(variance 0.5,　0.1886,　0.0274)  : 0.015196597745138379\n",
    "not scaling, remove vehicle, not use nonscored_target, use DAE2(variance 1.0,　0.2707,　0.0456)  : 0.015279887596214419\n",
    "\n",
    "all + DAE - drop : 0.014972256803365827\n",
    "all + DAE - scale - drop : 0.015037600164910332\n",
    "all + DAE : 0.014993220391337197\n",
    "\n",
    "悪化はした\n",
    "そこまで悪くはなさそう\n",
    "cellの方がロスが落ちないから、チューニング次第な感じはする\n",
    "ノイズのバリアンスを大きくするとロスは落ちにくくなる、0.5の時が良さそう\n",
    "CVとLBの相関はある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def make_scaler(flag):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "seeds = [0, 1, 2, 3, 4, 5, 6]\n",
    "SCALE = \"quantile\"\n",
    "USE_DAE = True\n",
    "USE_SCALE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate, Embedding, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import AdamW, Lookahead\n",
    "\n",
    "\n",
    "def DNN_3lmodel(input_size, output_size):\n",
    "    inp = Input(shape = (input_size, ))\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = Dropout(0.4914099166744246)(x)\n",
    "    x = WeightNormalization(Dense(1159, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.18817607797795838)(x)\n",
    "    x = WeightNormalization(Dense(960, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.12542057776853896)(x)\n",
    "    x = WeightNormalization(Dense(1811, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20175242230280122)(x)\n",
    "    out = WeightNormalization(Dense(output_size, activation = 'sigmoid'))(x)\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    opt = Lookahead(opt, sync_period = 10)\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def DNN_model(input_size, output_size):\n",
    "    inputs = Input((input_size, ))\n",
    "\n",
    "    outputs = BatchNormalization()(inputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = WeightNormalization(Dense(output_size, activation=\"sigmoid\"))(outputs)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=AdamW(lr=0.001, weight_decay=1e-5, clipvalue=756), \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def DAE(input_size):\n",
    "    emb_size = input_size//2\n",
    "    inputs = Input((input_size, ))\n",
    "    x = Dense(emb_size, activation=\"relu\")(inputs)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    outputs = Dense(input_size, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "def DAE2(input_size):\n",
    "    emb_size = 1500\n",
    "    inputs = Input((input_size, ))\n",
    "    x = Dense(emb_size, activation=\"relu\")(inputs)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    outputs = Dense(input_size, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../Data/Raw/test_features.csv\")\n",
    "\n",
    "y = pd.read_csv(\"../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "CELLS_50 = CELLS[:50]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\"]\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "train_sigid = train_df[\"sig_id\"]\n",
    "test_sigid = test_df[\"sig_id\"]\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DAE\n",
    "if USE_DAE:\n",
    "    genes = pd.concat([train_df, test_df])[GENES].values\n",
    "    cells = pd.concat([train_df, test_df])[CELLS].values\n",
    "    \n",
    "    factor = 0.5\n",
    "    variance = 0.5\n",
    "    gene_noise = np.random.normal(loc=0., scale=variance, size=genes.shape)\n",
    "    cell_noise = np.random.normal(loc=0., scale=variance, size=cells.shape)\n",
    "\n",
    "    noisy_genes = genes + factor*gene_noise\n",
    "    noisy_cells = cells +  factor * cell_noise\n",
    "\n",
    "    epochs = 300\n",
    "    g_chck = ModelCheckpoint('keras_dae_g.h5', monitor='loss', save_best_only=True)\n",
    "    c_chck = ModelCheckpoint('keras_dae_c.h5', monitor='loss', save_best_only=True)\n",
    "    es = EarlyStopping(monitor='loss', patience=10, min_delta=0)\n",
    "\n",
    "    g_model = DAE(genes.shape[1])\n",
    "    c_model = DAE(cells.shape[1])\n",
    "\n",
    "    #g_model.fit(noisy_genes, genes, batch_size=128, verbose=1, epochs=epochs, callbacks=[g_chck, es])\n",
    "    #c_model.fit(noisy_cells, cells, batch_size=128, verbose=1, epochs=epochs, callbacks=[c_chck, es])\n",
    "\n",
    "    g_model.load_weights('keras_dae_g.h5')\n",
    "    c_model.load_weights('keras_dae_c.h5')\n",
    "\n",
    "    trans_genes = g_model.predict(genes)\n",
    "    trans_cells = c_model.predict(cells)\n",
    "    print(trans_genes.shape)\n",
    "    print(trans_cells.shape)\n",
    "    \n",
    "    train_trans_genes = trans_genes[:TR_SIZE]\n",
    "    test_trans_genes = trans_genes[TR_SIZE:]\n",
    "    train_trans_cells = trans_cells[:TR_SIZE]\n",
    "    test_trans_cells = trans_cells[TR_SIZE:]\n",
    "    for i, col in enumerate(GENES):\n",
    "        train_df[col] = train_trans_genes[:,i]\n",
    "        test_df[col] = test_trans_genes[:,i]\n",
    "    for i, col in enumerate(CELLS):\n",
    "        train_df[col] = train_trans_cells[:,i]\n",
    "        test_df[col] = test_trans_cells[:,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"c-0\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "drop_cols = [\n",
    "    'g-37', 'g-38', 'g-50', 'g-63', 'g-121', 'g-123', 'g-195', 'g-228', 'g-248', 'g-261', 'g-328', 'g-349', \n",
    "    'g-369', 'g-460', 'c-1', 'c-2', 'c-4', 'c-6', 'c-8', 'c-10', 'c-11', 'c-13', 'c-17', 'c-18', 'c-26', 'c-31', 'c-33', \n",
    "    'c-38', 'c-40', 'c-42', 'c-51', 'c-52', 'c-55', 'c-60', 'c-66', 'c-73', 'c-75'\n",
    "]\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "BIOS = list(set(BIOS) - set(drop_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 片側95%に入ったときに50%以上の確率で判別できるMoAの組み合わせが存在した列\n",
    "n_cols = ['g-270', 'g-170', 'g-718', 'g-163', 'g-377', 'g-712', 'g-529', 'g-178', 'g-251', 'g-74', 'g-424', 'g-330', 'g-97', 'g-417', 'g-144', 'g-689', 'g-215', 'g-317', 'g-201', 'g-731', 'g-300', 'g-500', 'g-368', 'g-578', 'g-468', 'g-338', 'g-296', 'g-157', 'g-116', 'g-478', 'g-629', 'g-100', 'g-65', 'g-243', 'g-53', 'g-443', 'g-226', 'g-46', 'g-158', 'g-389', 'g-577', 'g-147', 'g-90', 'g-131', 'g-745', 'g-750', 'g-619', 'g-646', 'g-12', 'g-130', 'g-392', 'g-106', 'g-419', 'g-768', 'g-386', 'g-596', 'g-31', 'g-546', 'g-600', 'g-541', 'g-512', 'g-723', 'g-764', 'g-208', 'g-467', 'g-264', 'g-148', 'g-86', 'g-744', 'g-266', 'g-91', 'g-146', 'g-102']\n",
    "p_cols = ['c-16', 'c-57', 'g-726', 'c-53', 'g-423', 'c-97', 'g-624', 'g-178', 'g-257', 'c-29', 'c-50', 'g-128', 'c-63', 'g-674', 'c-99', 'c-43', 'c-46', 'c-72', 'c-20', 'c-39', 'g-439', 'g-390', 'g-540', 'c-64', 'c-47', 'c-89', 'g-497', 'c-95', 'g-55', 'g-293', 'g-643', 'g-759', 'g-590', 'g-411', 'g-508', 'c-71', 'g-683', 'c-81', 'g-534', 'g-503', 'g-298', 'g-760', 'g-231', 'g-300', 'c-82', 'c-22', 'g-276', 'c-92', 'g-306', 'c-86', 'c-59', 'c-88', 'c-3', 'g-250', 'g-67', 'g-428', 'g-691', 'c-19', 'g-489', 'g-667', 'c-34', 'c-54', 'c-27', 'c-44', 'g-385', 'c-62', 'g-769', 'g-704', 'g-353', 'c-68', 'c-84', 'c-7', 'g-332', 'c-28', 'g-735', 'c-98', 'c-12', 'c-78', 'c-24', 'c-79', 'g-558', 'g-491', 'c-65', 'g-644', 'g-370', 'c-48', 'c-41', 'c-56', 'c-30', 'c-85', 'c-21', 'g-40', 'c-94', 'c-49', 'c-35', 'c-96', 'c-5', 'c-61', 'g-743', 'g-62', 'g-8', 'c-93', 'c-37', 'c-70', 'g-705', 'g-438', 'c-0', 'c-36', 'c-90', 'g-66', 'g-185', 'g-421', 'g-291', 'g-321', 'c-80', 'g-75', 'g-202', 'g-685', 'c-67', 'c-83', 'g-161', 'c-15', 'g-568', 'c-32', 'g-375', 'g-513', 'g-72', 'g-58', 'c-87', 'c-25', 'g-477', 'g-406', 'c-14', 'c-77', 'g-113', 'g-588', 'g-672', 'g-112', 'g-204', 'c-91', 'c-45']\n",
    "for col in n_cols:\n",
    "    train_df[\"n_\"+col] = train_df[col].map(lambda x : 1 if x <= -9.5 else 0)\n",
    "    test_df[\"n_\"+col] = test_df[col].map(lambda x : 1 if x <= -9.5 else 0)\n",
    "    \n",
    "for col in p_cols:\n",
    "    train_df[\"p_\"+col] = train_df[col].map(lambda x : 1 if x >= 9.5 else 0)\n",
    "    test_df[\"p_\"+col] = test_df[col].map(lambda x : 1 if x >= 9.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SCALE:\n",
    "    for val in train_df[\"time_dose\"].unique():\n",
    "        temp = pd.concat([train_df[train_df[\"time_dose\"] == val], test_df[test_df[\"time_dose\"] == val]])\n",
    "        scaler = make_scaler(SCALE)\n",
    "        scaler.fit(temp[BIOS].values.reshape(-1, len(BIOS)))\n",
    "        tr_ind = train_df[train_df[\"time_dose\"] == val].index\n",
    "        te_ind = test_df[test_df[\"time_dose\"] == val].index\n",
    "        tr_trans = scaler.transform(train_df[BIOS].iloc[tr_ind].values.reshape(-1, len(BIOS))).T\n",
    "        te_trans = scaler.transform(test_df[BIOS].iloc[te_ind].values.reshape(-1, len(BIOS))).T\n",
    "\n",
    "        for i, col in enumerate(BIOS):\n",
    "            train_df[col].iloc[tr_ind] = tr_trans[i]\n",
    "            test_df[col].iloc[te_ind] = te_trans[i]\n",
    "\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\"), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\"), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print()\n",
    "\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "test_X = test_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y[mask].drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "print(X.shape)\n",
    "print(test_X.shape)\n",
    "print(y_nonv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_X.shape[0], y_nonv.shape[1]))\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 7\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    for itr, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):\n",
    "        print(\"======================== fold {} ========================\".format(itr+1))\n",
    "        train_X = X.iloc[train_index].values\n",
    "        train_y = y_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index].values\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        \n",
    "        model = DNN_3lmodel(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        #model = DNN_model(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        cb = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                                          mode = 'min',\n",
    "                                                          patience = 10,\n",
    "                                                          restore_best_weights = False,)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                         mode = 'min',\n",
    "                                                         factor = 0.3,\n",
    "                                                         patience = 3,)\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'_{itr}_{seed}.h5',\n",
    "                                                        monitor='val_loss',\n",
    "                                                        save_best_only = True,\n",
    "                                                        save_weights_only = True)\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            train_X, \n",
    "            train_y,\n",
    "            batch_size=128,\n",
    "            epochs=80,\n",
    "            verbose=0,\n",
    "            #callbacks=[cb],\n",
    "            callbacks = [early_stopping, reduce_lr,  checkpoint],\n",
    "            validation_data=(valid_X, valid_y),\n",
    "        )\n",
    "        #model.load_weights(f'_{itr}_{seed}.h5')\n",
    "        \n",
    "        train_pred[valid_index] += model.predict(valid_X, batch_size=128)\n",
    "        preds += model.predict(test_X.values, batch_size=128) / (K*len(seeds))\n",
    "\n",
    "\n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE, y.shape[1]))\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.001\n",
    "t = 0\n",
    "def func(t, p):\n",
    "    \n",
    "    return -(t*np.log(p) + (1-t) * np.log(1-p))\n",
    "print(func(y[:,0], train_preds2[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2[-2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1*np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0*np.log(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [:,:tag_size]\n",
    "sub_df = pd.read_csv(\"../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "#sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_preds.copy()\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))\n",
    "t_ = train_df[train_df[\"cp_type\"] == 0]\n",
    "t[t_.index] = np.zeros((t_.shape[0], t.shape[1]))\n",
    "t = np.where(t > 1, 1, t)\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "not_li = []\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(y.shape[1]):\n",
    "        if y[i][j] == 1:\n",
    "            #print(\"====={}, {}====\".format(i,j))\n",
    "            rank = np.where(train_preds[i].argsort()[::-1] == j)[0][0]+1\n",
    "            #print(\"rank {}\".format(rank))\n",
    "            if rank <= 20:\n",
    "                li.append(j)\n",
    "            else:\n",
    "                not_li.append(j)\n",
    "            #print(train_preds[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(set(li))\n",
    "not_li = list(set(not_li))\n",
    "for i in li:\n",
    "    if i not in not_li:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\").columns\n",
    "pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\")[cols[256-206]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
