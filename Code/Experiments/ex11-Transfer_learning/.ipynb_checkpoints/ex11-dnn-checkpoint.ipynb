{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex6:  \n",
    "cv score : 0.01582363008925492  \n",
    "cv score : 0.014583733652430035  \n",
    "\n",
    "ex7:  \n",
    "prodしたら良さそうだった  \n",
    "CVを書き直しましょう  \n",
    "かき直した  \n",
    "cv score : 0.01696220617131911  \n",
    "cv score : 0.015633094022344574  \n",
    "\n",
    "0.01564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys, tqdm, time\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n",
    "        \n",
    "    \n",
    "def make_scaler(flag, seed):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    elif flag == \"gauss\":\n",
    "        return GaussRankScaler()\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "seeds = [0, 1, 2, 3, 4, 5, 6]\n",
    "SCALE = \"quantile\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "#pub_test_df = pd.read_csv(\"../input/moapublictest/test_features.csv\")\n",
    "pub_test_df = pd.read_csv(\"../../../Data/Raw/test_features.csv\")\n",
    "drug_df = pd.read_csv(\"../../../Data/Raw/train_drug.csv\")#\n",
    "\n",
    "y = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "y = y.merge(drug_df, on='sig_id', how='left') #\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\" and col != \"drug_id\"]#\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]\n",
    "pub_test_df[\"time_dose\"] = pub_test_df[\"cp_time\"].astype(str) + \" * \" + pub_test_df[\"cp_dose\"]\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "pub_test_df = pub_test_df[pub_test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "y_nonv = y[mask].reset_index(drop=True)#\n",
    "y_all_nonv = y_all[mask].reset_index(drop=True)\n",
    "\n",
    "scored = y_nonv.copy()#\n",
    "y_nonv.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "y.drop(\"drug_id\", axis=1, inplace=True)#\n",
    "\n",
    "TR_NONV_SIZE = train_df.shape[0]\n",
    "TE_NONV_SHAPE = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上位500こ\n",
    "#prod_cols = [['g-145', 'g-201', 'g-208'], ['g-370', 'g-508', 'g-37'], ['g-38', 'g-392', 'g-707'], ['g-328', 'g-28', 'g-392'], ['g-441', 'g-157', 'g-392'], ['g-181', 'g-100', 'g-392'], ['g-67', 'g-760', 'g-50'], ['g-731', 'g-100', 'g-707'], ['g-478', 'g-468', 'g-310'], ['g-91', 'g-145', 'g-208'], ['g-106', 'g-744', 'g-91'], ['g-131', 'g-208', 'g-392'], ['g-144', 'g-123', 'g-86'], ['g-228', 'g-72', 'g-67'], ['g-31', 'g-328', 'g-460'], ['g-392', 'g-731', 'g-100'], ['g-732', 'g-744', 'g-707'], ['g-705', 'g-375', 'g-704'], ['g-508', 'g-50', 'g-411'], ['g-234', 'g-58', 'g-520'], ['g-503', 'g-761', 'g-50'], ['g-113', 'g-75', 'g-178'], ['g-50', 'g-508', 'g-113'], ['g-113', 'g-375', 'g-75'], ['g-576', 'g-452', 'g-392'], ['g-50', 'g-37', 'g-36'], ['g-707', 'g-133', 'g-392'], ['g-484', 'g-392', 'g-544'], ['g-508', 'g-67', 'g-370'], ['g-123', 'g-731', 'g-100'], ['g-298', 'g-477', 'g-644'], ['g-72', 'g-370', 'g-50'], ['g-67', 'g-178', 'g-113'], ['g-744', 'g-608', 'g-100'], ['g-91', 'g-100', 'g-707'], ['g-37', 'g-228', 'g-202'], ['g-37', 'g-300', 'g-370'], ['g-234', 'g-508', 'g-595'], ['g-596', 'g-744', 'g-707'], ['g-300', 'g-227', 'g-591'], ['g-135', 'g-392', 'g-512'], ['g-731', 'g-744', 'g-158'], ['g-69', 'g-707', 'g-100'], ['g-276', 'g-653', 'g-291'], ['g-624', 'g-615', 'g-189'], ['g-181', 'g-707', 'g-38'], ['g-72', 'g-75', 'g-508'], ['g-231', 'g-707', 'g-392'], ['g-508', 'g-37', 'g-72'], ['g-725', 'g-712', 'g-640'], ['g-67', 'g-644', 'g-113'], ['g-508', 'g-228', 'g-656'], ['g-185', 'g-37', 'g-672'], ['g-370', 'g-50', 'g-503'], ['g-201', 'g-745', 'g-599'], ['g-332', 'g-50', 'g-571'], ['g-50', 'g-37', 'g-59'], ['g-508', 'g-113', 'g-231'], ['g-707', 'g-158', 'g-100'], ['g-257', 'g-50', 'g-72']]\n",
    "prod_cols = [    ['g-712',  'g-208',  'g-38',  'g-100',  'g-123',  'g-328',  'g-744',  'g-248',  'g-460',  'g-731',  'g-417',  'g-349',  'g-131'], ['g-228',  'g-75',  'g-67',  'g-760',  'g-37',  'g-406',  'g-50',  'g-672',  'g-63',  'g-72',  'g-195'], ['g-100', 'g-157', 'g-178'],['g-183', 'g-300', 'g-767'],['g-50', 'g-37', 'g-489', 'g-257', 'g-332'],['g-270', 'g-135', 'g-231', 'g-158', 'g-478', 'g-146', 'g-491', 'g-392'],['g-745', 'g-635', 'g-235'], ['g-300', 'g-414', 'g-62', 'g-34'], ['g-91', 'g-392'], ['g-75', 'g-113'], ['g-599', 'g-261', 'g-38', 'g-146', 'g-392', 'g-512', 'g-744'], ['g-50', 'g-332', 'g-37', 'g-58', 'g-705'], ['g-157', 'g-178'],['g-759', 'g-100', 'g-167', 'g-75', 'g-431', 'g-189', 'g-522', 'g-91'],['g-202', 'g-385', 'g-769'],           ]\n",
    "\n",
    "for cols in prod_cols:\n",
    "    name = \"prod-\" + \" * \".join(cols)\n",
    "    train_df[name] = train_df[cols].mean(axis=1)\n",
    "    test_df[name] = test_df[cols].mean(axis=1)\n",
    "    pub_test_df[name] = pub_test_df[cols].mean(axis=1)\n",
    "\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop cols num : 70\n",
      "agg\n"
     ]
    }
   ],
   "source": [
    "#out fold preprocessing\n",
    "\n",
    "#variance threshold\n",
    "VAR_THRESHOLD = 0.8\n",
    "drop_cols = []\n",
    "temp = pd.concat([train_df, pub_test_df])\n",
    "for col in BIOS+PRODS:\n",
    "    if temp[col].var() <= VAR_THRESHOLD:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "print(\"drop cols num : {}\".format(len(drop_cols)))\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "pub_test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "GENES_ = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS_ = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "BIOS_ = GENES_ + CELLS_\n",
    "PRODS = [col for col in train_df.columns if col.startswith(\"prod-\")]\n",
    "del temp\n",
    "\n",
    "# onehot encode of categorical feature and drop\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "pub_test_df = pd.concat([pd.get_dummies(pub_test_df[\"time_dose\"], prefix=\"onehot\", drop_first=True), pub_test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "# aggregation feature\n",
    "print(\"agg\")\n",
    "\"\"\"\n",
    "for df in [train_df, pub_test_df, test_df]:\n",
    "    df[\"agg-sum-g\"] = (df[GENES_].sum(axis=1) - df[GENES_].sum(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-mean-g\"] = (df[GENES_].mean(axis=1) - df[GENES_].mean(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-std-g\"] = df[GENES_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-g\"] = (df[GENES_].kurt(axis=1) - df[GENES_].kurt(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-skew-g\"] = (df[GENES_].skew(axis=1) - df[GENES_].skew(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-sum-c\"] = (df[CELLS_].sum(axis=1) - df[CELLS_].sum(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-mean-c\"] = (df[CELLS_].mean(axis=1) - df[CELLS_].mean(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-std-c\"] = df[CELLS_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-c\"] = (df[CELLS_].kurt(axis=1) - df[CELLS_].kurt(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-skew-c\"] = (df[CELLS_].skew(axis=1) - df[CELLS_].skew(axis=1).min() +1).map(np.log)\n",
    "    df[\"agg-sum-gc\"] = (df[BIOS_].sum(axis=1) - df[BIOS_].sum(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-mean-gc\"] = (df[BIOS_].mean(axis=1) - df[BIOS_].mean(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-std-gc\"] = df[BIOS_].std(axis=1).map(np.log)\n",
    "    df[\"agg-kurt-gc\"] = (df[BIOS_].kurt(axis=1) - df[BIOS_].kurt(axis=1).min() + 1).map(np.log)\n",
    "    df[\"agg-skew-gc\"] = (df[BIOS_].skew(axis=1) - df[BIOS_].skew(axis=1).min() + 1).map(np.log)\n",
    "\n",
    "\"\"\"\n",
    "for df in [train_df, pub_test_df, test_df]:\n",
    "    df[\"agg-sum-g\"] = df[GENES_].sum(axis=1)\n",
    "    df[\"agg-mean-g\"] = df[GENES_].mean(axis=1)\n",
    "    df[\"agg-std-g\"] = df[GENES_].std(axis=1)\n",
    "    df[\"agg-kurt-g\"] = df[GENES_].kurt(axis=1)\n",
    "    df[\"agg-skew-g\"] = df[GENES_].skew(axis=1)\n",
    "    df[\"agg-sum-c\"] = df[CELLS_].sum(axis=1)\n",
    "    df[\"agg-mean-c\"] = df[CELLS_].mean(axis=1)\n",
    "    df[\"agg-std-c\"] = df[CELLS_].std(axis=1)\n",
    "    df[\"agg-kurt-c\"] = df[CELLS_].kurt(axis=1)\n",
    "    df[\"agg-skew-c\"] = df[CELLS_].skew(axis=1)\n",
    "    df[\"agg-sum-gc\"] = df[BIOS_].sum(axis=1)\n",
    "    df[\"agg-mean-gc\"] = df[BIOS_].mean(axis=1)\n",
    "    df[\"agg-std-gc\"] = df[BIOS_].std(axis=1)\n",
    "    df[\"agg-kurt-gc\"] = df[BIOS_].kurt(axis=1)\n",
    "    df[\"agg-skew-gc\"] = df[BIOS_].skew(axis=1)\n",
    "\n",
    "AGG = [col for col in train_df.columns if col.startswith(\"agg-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y_nonv.drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values\n",
    "y_all_nonv = y_all_nonv.drop(\"sig_id\", axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dateset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if cycle\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\nclass Model(nn.Module):\\n    def __init__(self, num_features, num_targets):\\n        super(Model, self).__init__()\\n        self.hidden1 = 1024\\n        self.hidden2 =  512\\n        self.hidden3 = 512\\n        \\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden1))\\n        \\n        self.batch_norm2 = nn.BatchNorm1d(self.hidden1)\\n        self.dropout2 = nn.Dropout(0.45)\\n        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2))\\n        \\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden2)\\n        self.dropout3 = nn.Dropout(0.35)\\n        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden2, self.hidden3))\\n        \\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden3)\\n        self.dropout4 = nn.Dropout(0.45)\\n        self.dense4 = nn.utils.weight_norm(nn.Linear(self.hidden3, num_targets))\\n    \\n    def forward(self, x):\\n        x = self.batch_norm1(x)\\n        x = F.leaky_relu(self.dense1(x))\\n        \\n        x = self.batch_norm2(x)\\n        x = self.dropout2(x)\\n        x = F.leaky_relu(self.dense2(x))\\n\\n        x = self.batch_norm3(x)\\n        x = self.dropout3(x)\\n        x = F.leaky_relu(self.dense3(x))\\n        \\n        x = self.batch_norm4(x)\\n        x = self.dropout4(x)\\n        x = self.dense4(x)\\n        \\n        return x\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size=1500):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = [1500, 1250, 1000, 750]\n",
    "        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "        self.dropout2 = nn.Dropout(self.dropout_value[0])\n",
    "        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "        self.dropout3 = nn.Dropout(self.dropout_value[1])\n",
    "        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n",
    "\n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "        self.dropout4 = nn.Dropout(self.dropout_value[2])\n",
    "        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n",
    "\n",
    "        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "        self.dropout5 = nn.Dropout(self.dropout_value[3])\n",
    "        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.leaky_relu(self.dense4(x))\n",
    "\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense5(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 =  512\n",
    "        self.hidden3 = 512\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, self.hidden1))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.hidden1)\n",
    "        self.dropout2 = nn.Dropout(0.45)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(self.hidden1, self.hidden2))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.hidden2)\n",
    "        self.dropout3 = nn.Dropout(0.35)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(self.hidden2, self.hidden3))\n",
    "        \n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.hidden3)\n",
    "        self.dropout4 = nn.Dropout(0.45)\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(self.hidden3, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "        \n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, trainloader, validloader, tag, epochs, optimizer, scheduler, fine_tune_scheduler, loss_fn, loss_tr, early_stopping_steps, verbose, device, fold, seed):\n",
    "    \n",
    "    early_step = 0\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    weight_path = 'dnn_weights2/{}_{}_{}.pt'.format(tag, seed, fold)\n",
    "    \n",
    "    start = time.time()\n",
    "    t = time.time() - start\n",
    "    for epoch in range(epochs):\n",
    "        # fine tune \n",
    "        if fine_tune_scheduler is not None:\n",
    "            fine_tune_scheduler.step(epoch, model)\n",
    "            \n",
    "        train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, device)\n",
    "        valid_loss = valid_fn(model, loss_fn, validloader, device)\n",
    "\n",
    "        if epoch % verbose==0:\n",
    "            t = time.time() - start\n",
    "            print(f\"EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}, time: {t}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(),weight_path)\n",
    "            early_step = 0\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        elif early_stopping_steps != 0:\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                t = time.time() - start\n",
    "                print(f\"early stopping in iteration {epoch},  : best itaration is {best_epoch}, valid loss is {best_loss}, time: {t}\")\n",
    "                return model\n",
    "            \n",
    "    print(f\"training until max epoch {epochs},  : best itaration is {best_epoch}, valid loss is {best_loss}, time: {t}\")\n",
    "    return model\n",
    "            \n",
    "    \n",
    "def predict(model, testloader, device):\n",
    "    model.to(device)\n",
    "    predictions = inference_fn(model, testloader, device)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs, layer_num):\n",
    "        self.epochs = epochs\n",
    "        self.epochs_per_step = 0\n",
    "        self.frozen_layers = []\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "        self.frozen_layers = []\n",
    "\n",
    "        model_new = Model(num_features, num_targets)\n",
    "        model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in model_new.named_parameters():\n",
    "            layer_index = int(name.split('.')[0][-1])\n",
    "\n",
    "            if layer_index == self.layer_num:  #最後の層以外は凍結する\n",
    "                continue\n",
    "\n",
    "            param.requires_grad = False\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "        print(self.frozen_layers)\n",
    "        #self.epochs_per_step = self.epochs // len(self.frozen_layers)+1  # 24 // 4 = 6\n",
    "        #self.epochs_per_step = self.epochs // (len(self.frozen_layers)+1)  # 24 // 4 = 6\n",
    "        self.epochs_per_step = 6\n",
    "        \n",
    "        # Replace the top layers with another ones, 最後に追加されてく\n",
    "        model_new.batch_norm4 = nn.BatchNorm1d(model.hidden3)\n",
    "        model_new.dropout4 = nn.Dropout(0.45)\n",
    "        model_new.dense4 = nn.utils.weight_norm(nn.Linear(model.hidden3, num_targets_new))\n",
    "        model_new.to(DEVICE)\n",
    "        return model_new\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(self.frozen_layers) == 0 or epoch == 0:\n",
    "            return\n",
    "\n",
    "        if epoch % self.epochs_per_step == 0:\n",
    "            last_frozen_index = self.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = int(name.split('.')[0][-1])\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    print(epoch, \"   \",name)\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n",
    "\"\"\"\n",
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs, layer_num):\n",
    "        self.epochs = epochs\n",
    "        self.epochs_per_step = 0\n",
    "        self.frozen_layers = []\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "        self.frozen_layers = []\n",
    "\n",
    "        model_new = Model(num_features, num_targets)\n",
    "        model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in model_new.named_parameters():\n",
    "            layer_index = int(name.split('.')[0][-1])\n",
    "\n",
    "            if layer_index == self.layer_num:  #最後の層以外は凍結する\n",
    "                continue\n",
    "\n",
    "            param.requires_grad = False\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "        print(self.frozen_layers)\n",
    "        #self.epochs_per_step = self.epochs // len(self.frozen_layers)+1  # 24 // 4 = 6\n",
    "        #self.epochs_per_step = self.epochs // (len(self.frozen_layers)+1)  # 24 // 4 = 6\n",
    "        self.epochs_per_step = 5\n",
    "        \n",
    "        # Replace the top layers with another ones, 最後に追加されてく\n",
    "        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n",
    "        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n",
    "        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n",
    "        model_new.to(DEVICE)\n",
    "        return model_new\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(self.frozen_layers) == 0 or epoch == 0:\n",
    "            return\n",
    "\n",
    "        if epoch % self.epochs_per_step == 0:\n",
    "            last_frozen_index = self.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = int(name.split('.')[0][-1])\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    print(epoch, \"   \",name)\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del self.frozen_layers[-1]  # Remove the last layer as unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.506901868568683, valid_loss: 0.02345714032649994, time: 3.2073888778686523\n",
      "EPOCH: 5, train_loss: 0.012885574597512951, valid_loss: 0.009215763043612242, time: 16.687735557556152\n",
      "EPOCH: 10, train_loss: 0.01285973089319226, valid_loss: 0.008911467790603637, time: 30.85879611968994\n",
      "EPOCH: 15, train_loss: 0.012748851283528918, valid_loss: 0.00893119340762496, time: 46.845471143722534\n",
      "EPOCH: 20, train_loss: 0.012380346089500832, valid_loss: 0.008756922166794539, time: 61.11815094947815\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008611255586147308, time: 61.11815094947815\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6489306700390738, valid_loss: 0.2962953042984009, time: 1.6471059322357178\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02304983580787037, valid_loss: 0.0163007203489542, time: 10.500337839126587\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019672844923932006, valid_loss: 0.016449654325842857, time: 23.379770040512085\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019361021588682324, valid_loss: 0.016436234582215548, time: 33.801857709884644\n",
      "early stopping in iteration 16,  : best itaration is 6, valid loss is 0.01622976778075099, time: 36.0190646648407\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5073609450946048, valid_loss: 0.02226234070956707, time: 2.468707799911499\n",
      "EPOCH: 5, train_loss: 0.012827637624375674, valid_loss: 0.009700358286499977, time: 17.54885244369507\n",
      "EPOCH: 10, train_loss: 0.012783993403015493, valid_loss: 0.009632610343396663, time: 32.40707015991211\n",
      "EPOCH: 15, train_loss: 0.012683804077254671, valid_loss: 0.00958299718797207, time: 46.247682094573975\n",
      "EPOCH: 20, train_loss: 0.012346021628298728, valid_loss: 0.009295742623507977, time: 60.29241228103638\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009170635938644409, time: 60.29241228103638\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6513824395987452, valid_loss: 0.3030121123790741, time: 1.9418675899505615\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.022863058183266192, valid_loss: 0.016758761294186117, time: 14.08755350112915\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019803569911896777, valid_loss: 0.016836641058325767, time: 22.957091569900513\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01947692127860322, valid_loss: 0.016825360544025898, time: 34.34877872467041\n",
      "early stopping in iteration 18,  : best itaration is 8, valid loss is 0.016701636537909507, time: 42.3353271484375\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5070501679328043, valid_loss: 0.019949970319867134, time: 3.9222934246063232\n",
      "EPOCH: 5, train_loss: 0.013313908205443137, valid_loss: 0.00976743370294571, time: 17.655454635620117\n",
      "EPOCH: 10, train_loss: 0.012760848197079188, valid_loss: 0.009788251444697381, time: 32.25184154510498\n",
      "EPOCH: 15, train_loss: 0.012645239236991148, valid_loss: 0.00956011215224862, time: 45.949201345443726\n",
      "EPOCH: 20, train_loss: 0.012290259779154046, valid_loss: 0.009362481143325567, time: 61.68221116065979\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009237341526895761, time: 61.68221116065979\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6469852221575944, valid_loss: 0.2970060706138611, time: 1.697540521621704\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02070752816979547, valid_loss: 0.01754273533821106, time: 9.919296979904175\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019569962461655203, valid_loss: 0.017671527154743672, time: 21.40806269645691\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019247453735285514, valid_loss: 0.017663988247513772, time: 31.577633380889893\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018600312565025444, valid_loss: 0.01749995317310095, time: 44.369852781295776\n",
      "training until max epoch 25,  : best itaration is 19, valid loss is 0.01738875862210989, time: 44.369852781295776\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5078817917300122, valid_loss: 0.024622422382235526, time: 2.479991912841797\n",
      "EPOCH: 5, train_loss: 0.012734350273195577, valid_loss: 0.009832784011960029, time: 15.693445205688477\n",
      "EPOCH: 10, train_loss: 0.012746439727187968, valid_loss: 0.009796972200274468, time: 32.917765855789185\n",
      "EPOCH: 15, train_loss: 0.012640621544805919, valid_loss: 0.009743859060108662, time: 45.75849914550781\n",
      "EPOCH: 20, train_loss: 0.012271276238013287, valid_loss: 0.009522333778440951, time: 61.31173539161682\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009398814849555492, time: 61.31173539161682\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6504195971148354, valid_loss: 0.3007331728935242, time: 1.725334882736206\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.023465218136505206, valid_loss: 0.01819427352398634, time: 11.463056802749634\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019477493604835198, valid_loss: 0.018153166845440865, time: 23.94922947883606\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019200531525068544, valid_loss: 0.018099128752946853, time: 34.47049832344055\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018524536457495626, valid_loss: 0.0180223785340786, time: 48.17585754394531\n",
      "training until max epoch 25,  : best itaration is 18, valid loss is 0.017997002191841603, time: 48.17585754394531\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5090374670111809, valid_loss: 0.02570093348622322, time: 2.456373691558838\n",
      "EPOCH: 5, train_loss: 0.012857635201392124, valid_loss: 0.009567027315497398, time: 16.856587648391724\n",
      "EPOCH: 10, train_loss: 0.012817486322351865, valid_loss: 0.009410285465419293, time: 30.38255214691162\n",
      "EPOCH: 15, train_loss: 0.012711204081571021, valid_loss: 0.00933463454246521, time: 43.507099628448486\n",
      "EPOCH: 20, train_loss: 0.012365531055953633, valid_loss: 0.009046535305678845, time: 58.90217423439026\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008918840438127518, time: 58.90217423439026\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6515727641225673, valid_loss: 0.30564491391181947, time: 1.6911892890930176\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021816965547345933, valid_loss: 0.016950978226959706, time: 12.636456966400146\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01959646599633353, valid_loss: 0.01705886159092188, time: 20.90293526649475\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019299884637196858, valid_loss: 0.016998601034283637, time: 30.59253168106079\n",
      "early stopping in iteration 19,  : best itaration is 9, valid loss is 0.0168786484003067, time: 42.028725385665894\n",
      "======================== fold 6 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5093884626593517, valid_loss: 0.022403928488492965, time: 2.8674590587615967\n",
      "EPOCH: 5, train_loss: 0.012940798759409766, valid_loss: 0.009610157124698163, time: 16.214087963104248\n",
      "EPOCH: 10, train_loss: 0.012801744126188917, valid_loss: 0.009515793956816197, time: 30.14249610900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 15, train_loss: 0.012686029830504032, valid_loss: 0.009420575276017189, time: 44.997360944747925\n",
      "EPOCH: 20, train_loss: 0.01233456432692656, valid_loss: 0.009257198311388493, time: 59.08932876586914\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009129507169127464, time: 59.08932876586914\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6533503461451757, valid_loss: 0.3021587657928467, time: 1.530287265777588\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021742991178112778, valid_loss: 0.01776206836104393, time: 9.155895948410034\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01953325443425957, valid_loss: 0.01771173045039177, time: 18.584632396697998\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019230103931155335, valid_loss: 0.01763345304876566, time: 31.046825170516968\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018491496579075346, valid_loss: 0.017569139823317526, time: 43.003169536590576\n",
      "training until max epoch 25,  : best itaration is 19, valid loss is 0.01756193093955517, time: 43.003169536590576\n",
      "======================== fold 7 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5075641262541417, valid_loss: 0.021301761493086813, time: 2.942349433898926\n",
      "EPOCH: 5, train_loss: 0.01276715260733958, valid_loss: 0.009741518683731555, time: 17.87302589416504\n",
      "EPOCH: 10, train_loss: 0.012773484042307147, valid_loss: 0.009818401411175728, time: 32.18318033218384\n",
      "EPOCH: 15, train_loss: 0.012646420180898945, valid_loss: 0.00963097222149372, time: 45.78336715698242\n",
      "EPOCH: 20, train_loss: 0.012310358283876562, valid_loss: 0.009432277120649814, time: 59.343350887298584\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009318549446761607, time: 59.343350887298584\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6505147238977912, valid_loss: 0.296779465675354, time: 1.6806325912475586\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.020863621996170808, valid_loss: 0.017776734270155428, time: 10.171531915664673\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.0196405436691581, valid_loss: 0.017819390892982484, time: 20.016140460968018\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019334761590576496, valid_loss: 0.01768051940947771, time: 30.79631495475769\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018632855621122178, valid_loss: 0.01760991517454386, time: 42.20924925804138\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017526663728058337, time: 42.20924925804138\n",
      "seed 0 , cv score : 0.017279183745562397\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.50659303320255, valid_loss: 0.022836777344346047, time: 3.3825125694274902\n",
      "EPOCH: 5, train_loss: 0.012896687698354121, valid_loss: 0.00994925420731306, time: 16.02355146408081\n",
      "EPOCH: 10, train_loss: 0.01276648226415827, valid_loss: 0.009922778159379959, time: 30.46062397956848\n",
      "EPOCH: 15, train_loss: 0.012656775094112572, valid_loss: 0.009787879697978497, time: 44.30372929573059\n",
      "EPOCH: 20, train_loss: 0.012331011521370233, valid_loss: 0.009498375095427036, time: 58.432809591293335\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.00933388840407133, time: 58.432809591293335\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6485842433916468, valid_loss: 0.30298066735267637, time: 1.5275542736053467\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021381377540275352, valid_loss: 0.017299264073371887, time: 9.428448915481567\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019654550228495986, valid_loss: 0.017267015762627125, time: 19.298752069473267\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01938656755552, valid_loss: 0.017267907857894896, time: 31.807079076766968\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018681023890773456, valid_loss: 0.01716729164123535, time: 43.82360577583313\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.017101229056715964, time: 43.82360577583313\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5083293932288684, valid_loss: 0.022906788066029547, time: 3.628878593444824\n",
      "EPOCH: 5, train_loss: 0.0130330639878741, valid_loss: 0.009400176629424096, time: 16.8588604927063\n",
      "EPOCH: 10, train_loss: 0.012854716268594604, valid_loss: 0.00943775825202465, time: 32.32810306549072\n",
      "EPOCH: 15, train_loss: 0.012735284172703285, valid_loss: 0.00941970381885767, time: 45.07555794715881\n",
      "EPOCH: 20, train_loss: 0.012393090741809559, valid_loss: 0.009083822146058082, time: 58.75817346572876\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.008947167694568634, time: 58.75817346572876\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6530508739320008, valid_loss: 0.2978569746017456, time: 1.6081252098083496\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021502771062424052, valid_loss: 0.01737416859716177, time: 10.27901554107666\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01964585220038488, valid_loss: 0.01735947161912918, time: 21.140431880950928\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019334297388087254, valid_loss: 0.017281711250543594, time: 31.558594942092896\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01862199421393106, valid_loss: 0.017348470501601695, time: 43.25013041496277\n",
      "training until max epoch 25,  : best itaration is 17, valid loss is 0.017275042049586773, time: 43.25013041496277\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5085400003356998, valid_loss: 0.022655973359942436, time: 2.780959129333496\n",
      "EPOCH: 5, train_loss: 0.012788428025332843, valid_loss: 0.00979505319148302, time: 15.240005016326904\n",
      "EPOCH: 10, train_loss: 0.012812588991103123, valid_loss: 0.009744310900568962, time: 30.041207551956177\n",
      "EPOCH: 15, train_loss: 0.012668318571332765, valid_loss: 0.009527159184217452, time: 43.73381853103638\n",
      "EPOCH: 20, train_loss: 0.012325549086391115, valid_loss: 0.009322864785790443, time: 58.25450921058655\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009160778224468232, time: 58.25450921058655\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6512105396004761, valid_loss: 0.29957844138145445, time: 1.6728887557983398\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.022377184699891375, valid_loss: 0.01765384081751108, time: 9.717871189117432\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019682504680185092, valid_loss: 0.017671679556369783, time: 19.91795802116394\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019303063374190105, valid_loss: 0.017778708040714263, time: 31.14866828918457\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018640229516491597, valid_loss: 0.017515595741569995, time: 43.42755627632141\n",
      "training until max epoch 25,  : best itaration is 20, valid loss is 0.017515595741569995, time: 43.42755627632141\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5066797528035787, valid_loss: 0.024175927117466925, time: 3.3437132835388184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, train_loss: 0.012908147008088575, valid_loss: 0.009491555616259575, time: 16.124446868896484\n",
      "EPOCH: 10, train_loss: 0.012821794696608367, valid_loss: 0.009528787583112716, time: 30.851086139678955\n",
      "EPOCH: 15, train_loss: 0.01269340930747337, valid_loss: 0.009422136060893536, time: 43.703686475753784\n",
      "EPOCH: 20, train_loss: 0.012338918792147215, valid_loss: 0.009127405509352683, time: 57.70524883270264\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009041870646178722, time: 57.70524883270264\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6451359119545035, valid_loss: 0.29789329767227174, time: 1.5327351093292236\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021287946790760875, valid_loss: 0.017311766780912875, time: 10.217296361923218\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019577128014394214, valid_loss: 0.017562824077904226, time: 21.323516368865967\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019323929903559946, valid_loss: 0.01732331547886133, time: 31.098058223724365\n",
      "early stopping in iteration 15,  : best itaration is 5, valid loss is 0.017311766780912875, time: 31.098270654678345\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5081619784793481, valid_loss: 0.023553743809461593, time: 2.5771915912628174\n",
      "EPOCH: 5, train_loss: 0.01303478735847538, valid_loss: 0.009320574831217527, time: 16.022495985031128\n",
      "EPOCH: 10, train_loss: 0.012863127533726546, valid_loss: 0.009253424555063249, time: 30.314656972885132\n",
      "EPOCH: 15, train_loss: 0.012724937490966856, valid_loss: 0.009224628116935491, time: 42.73101997375488\n",
      "EPOCH: 20, train_loss: 0.012375073950915109, valid_loss: 0.008997496645897628, time: 58.86882829666138\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008878421671688556, time: 58.86882829666138\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6533127473730619, valid_loss: 0.3061361598968506, time: 1.8294520378112793\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.0216617046722344, valid_loss: 0.017063687928020953, time: 11.41971468925476\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019534209743142128, valid_loss: 0.01702581740915775, time: 21.4432852268219\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019280405220936755, valid_loss: 0.016953050196170806, time: 31.402500867843628\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018558499219567596, valid_loss: 0.016866851374506952, time: 45.230183124542236\n",
      "training until max epoch 25,  : best itaration is 21, valid loss is 0.01685654804110527, time: 45.230183124542236\n",
      "======================== fold 6 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5076463369879167, valid_loss: 0.02447215203816692, time: 2.3845338821411133\n",
      "EPOCH: 5, train_loss: 0.012963880979883912, valid_loss: 0.00993515485121558, time: 15.139880418777466\n",
      "EPOCH: 10, train_loss: 0.012742021932845583, valid_loss: 0.009781302961831292, time: 31.233792781829834\n",
      "EPOCH: 15, train_loss: 0.01262920261108996, valid_loss: 0.009706837784809371, time: 44.395344734191895\n",
      "EPOCH: 20, train_loss: 0.012278965428924642, valid_loss: 0.009482987729522089, time: 59.16118407249451\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009350814313317338, time: 59.16118407249451\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6501427770063684, valid_loss: 0.297246590256691, time: 1.6238937377929688\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02137713343207095, valid_loss: 0.017701303237117827, time: 11.137657403945923\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019532363194770908, valid_loss: 0.017589775923018653, time: 22.005687713623047\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01923333722594622, valid_loss: 0.01762188831344247, time: 32.27281999588013\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018538798633459454, valid_loss: 0.01741242183682819, time: 46.36199331283569\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017412175036345918, time: 46.36199331283569\n",
      "======================== fold 7 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5078276363233117, valid_loss: 0.02307222656905651, time: 2.6386780738830566\n",
      "EPOCH: 5, train_loss: 0.012846420690113184, valid_loss: 0.01835202008485794, time: 16.84641671180725\n",
      "EPOCH: 10, train_loss: 0.012748096569054792, valid_loss: 0.009917467534542083, time: 30.4730703830719\n",
      "EPOCH: 15, train_loss: 0.012677075239975437, valid_loss: 0.009710399247705937, time: 42.9282066822052\n",
      "EPOCH: 20, train_loss: 0.012289543496761597, valid_loss: 0.009455320946872235, time: 59.36457705497742\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009309867192059755, time: 59.36457705497742\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.650985550110032, valid_loss: 0.301126092672348, time: 1.700535535812378\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02252476458393392, valid_loss: 0.017517806403338908, time: 11.949980020523071\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019562451289269794, valid_loss: 0.017599451020359994, time: 21.08557152748108\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01924175581559032, valid_loss: 0.01746426660567522, time: 31.076936721801758\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018587339960900295, valid_loss: 0.0172788517922163, time: 46.04659032821655\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017219546884298324, time: 46.04659032821655\n",
      "seed 1 , cv score : 0.017253073172238658\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5090754420376148, valid_loss: 0.02078362725675106, time: 2.388352870941162\n",
      "EPOCH: 5, train_loss: 0.012775863748880066, valid_loss: 0.010317492038011552, time: 16.53607678413391\n",
      "EPOCH: 10, train_loss: 0.01269780455448595, valid_loss: 0.010179796144366265, time: 31.151639223098755\n",
      "EPOCH: 15, train_loss: 0.012588045445784015, valid_loss: 0.010069325119256973, time: 45.222066164016724\n",
      "EPOCH: 20, train_loss: 0.01225116039898728, valid_loss: 0.009799547791481018, time: 59.15807890892029\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.00968959990888834, time: 59.15807890892029\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6514240878374398, valid_loss: 0.3025695168972015, time: 1.792365312576294\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.022788381875574994, valid_loss: 0.017893144153058527, time: 12.696391105651855\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01953319604603612, valid_loss: 0.018100637905299664, time: 22.28189253807068\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01922674805280708, valid_loss: 0.017846404761075973, time: 33.86390686035156\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018515782221695597, valid_loss: 0.0179278676584363, time: 46.40886211395264\n",
      "training until max epoch 25,  : best itaration is 17, valid loss is 0.01783317942172289, time: 46.40886211395264\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5052620728399504, valid_loss: 0.024260968267917633, time: 2.496222496032715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, train_loss: 0.013251940927443069, valid_loss: 0.009812856167554855, time: 17.44755721092224\n",
      "EPOCH: 10, train_loss: 0.012825156473932234, valid_loss: 0.009585694409906864, time: 30.452864170074463\n",
      "EPOCH: 15, train_loss: 0.012709730619413627, valid_loss: 0.009400157779455185, time: 44.067471981048584\n",
      "EPOCH: 20, train_loss: 0.012348984487707148, valid_loss: 0.009227526262402535, time: 59.30727195739746\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.00906321432441473, time: 59.30727195739746\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.644111501204001, valid_loss: 0.2913012456893921, time: 2.027900218963623\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.0213010796421283, valid_loss: 0.01713778033852577, time: 10.604247093200684\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019736508637465334, valid_loss: 0.017253720872104167, time: 19.140014171600342\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01941453548450325, valid_loss: 0.017158823013305666, time: 29.253897190093994\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.0187358571329733, valid_loss: 0.017025504782795905, time: 43.808987855911255\n",
      "training until max epoch 25,  : best itaration is 21, valid loss is 0.01695512805134058, time: 43.808987855911255\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5090154016225719, valid_loss: 0.023782807737588882, time: 2.465728521347046\n",
      "EPOCH: 5, train_loss: 0.012841009807323112, valid_loss: 0.009302256144583226, time: 17.2874116897583\n",
      "EPOCH: 10, train_loss: 0.012832483979968392, valid_loss: 0.009448604770004749, time: 31.28778862953186\n",
      "EPOCH: 15, train_loss: 0.012724653646654012, valid_loss: 0.009149018563330174, time: 45.53495144844055\n",
      "EPOCH: 20, train_loss: 0.01238050777465105, valid_loss: 0.009093354530632496, time: 59.4847617149353\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.008939709570258856, time: 59.4847617149353\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6495947095812583, valid_loss: 0.3037752425670624, time: 2.464071750640869\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02271760579477362, valid_loss: 0.017102859877049924, time: 12.185869455337524\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019675349892706286, valid_loss: 0.017223080359399318, time: 20.596084594726562\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019373596677569306, valid_loss: 0.017147280611097812, time: 32.17033863067627\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01865607330917704, valid_loss: 0.017059182710945607, time: 44.92262887954712\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.016976141445338724, time: 44.92262887954712\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5082359338546691, valid_loss: 0.024549094662070274, time: 2.4592339992523193\n",
      "EPOCH: 5, train_loss: 0.012968562891846206, valid_loss: 0.010034540928900241, time: 17.397218942642212\n",
      "EPOCH: 10, train_loss: 0.012772401192916089, valid_loss: 0.009866081811487674, time: 30.134324550628662\n",
      "EPOCH: 15, train_loss: 0.012618555256552031, valid_loss: 0.009752823822200298, time: 44.080429553985596\n",
      "EPOCH: 20, train_loss: 0.012260854446969064, valid_loss: 0.009530530348420142, time: 58.96276140213013\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009389912486076355, time: 58.96276140213013\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6525234448261001, valid_loss: 0.29573622703552244, time: 1.6326744556427002\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021804450396575085, valid_loss: 0.017448678500950335, time: 10.355703830718994\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019566569071845945, valid_loss: 0.01734538786113262, time: 18.917486906051636\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019293856496612232, valid_loss: 0.01719763483852148, time: 29.8183376789093\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018554313748213303, valid_loss: 0.017260343991219996, time: 44.001274824142456\n",
      "training until max epoch 25,  : best itaration is 15, valid loss is 0.01719763483852148, time: 44.001274824142456\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.50709067102572, valid_loss: 0.024107426479458807, time: 2.5356404781341553\n",
      "EPOCH: 5, train_loss: 0.012894210531502156, valid_loss: 0.009487999565899373, time: 18.6890230178833\n",
      "EPOCH: 10, train_loss: 0.012817547008445536, valid_loss: 0.009539384432137012, time: 31.964978456497192\n",
      "EPOCH: 15, train_loss: 0.012724412523361074, valid_loss: 0.009371569827198983, time: 46.90595459938049\n",
      "EPOCH: 20, train_loss: 0.012363312721906885, valid_loss: 0.009156364053487777, time: 59.910492181777954\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009032133296132087, time: 59.910492181777954\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6499017587787396, valid_loss: 0.3018385374546051, time: 1.9128742218017578\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021878294420202036, valid_loss: 0.017485166527330877, time: 11.781028270721436\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019694559301274853, valid_loss: 0.017504083886742593, time: 20.78110432624817\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019345365565370868, valid_loss: 0.01726572562009096, time: 32.45157480239868\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018670065290722494, valid_loss: 0.017250348329544068, time: 45.25995755195618\n",
      "training until max epoch 25,  : best itaration is 16, valid loss is 0.01723624214529991, time: 45.25995755195618\n",
      "======================== fold 6 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5066447457726542, valid_loss: 0.023756911531090736, time: 2.8026931285858154\n",
      "EPOCH: 5, train_loss: 0.01307151302997325, valid_loss: 0.009522195607423782, time: 17.983814239501953\n",
      "EPOCH: 10, train_loss: 0.01285430317039828, valid_loss: 0.009383360743522645, time: 30.485058784484863\n",
      "EPOCH: 15, train_loss: 0.01274771211870216, valid_loss: 0.0094272206351161, time: 46.1622211933136\n",
      "EPOCH: 20, train_loss: 0.01233375898084125, valid_loss: 0.009074405916035175, time: 59.67797565460205\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.008975416943430901, time: 59.67797565460205\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6491474023944622, valid_loss: 0.29919665813446045, time: 1.854029655456543\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02338948370133703, valid_loss: 0.017451825626194475, time: 10.80739426612854\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019690685188146057, valid_loss: 0.017645134404301644, time: 19.083179712295532\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019270347280276788, valid_loss: 0.017580591067671777, time: 30.173189640045166\n",
      "early stopping in iteration 15,  : best itaration is 5, valid loss is 0.017451825626194475, time: 30.17439556121826\n",
      "======================== fold 7 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5077137447336093, valid_loss: 0.02123492456972599, time: 3.1259279251098633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, train_loss: 0.01333198227545842, valid_loss: 0.009442693963646888, time: 16.85776710510254\n",
      "EPOCH: 10, train_loss: 0.012871310336604005, valid_loss: 0.009374531544744968, time: 29.78839659690857\n",
      "EPOCH: 15, train_loss: 0.012758883349850874, valid_loss: 0.009240847788751125, time: 45.11252760887146\n",
      "EPOCH: 20, train_loss: 0.012404850747461628, valid_loss: 0.00901655562222004, time: 58.46634483337402\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.008863490298390389, time: 58.46634483337402\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6473901352914823, valid_loss: 0.2992212378978729, time: 1.529860496520996\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02175626262616949, valid_loss: 0.017275490425527097, time: 9.897100925445557\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019741528825897747, valid_loss: 0.01742801979184151, time: 18.527202367782593\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01942816427370318, valid_loss: 0.01730685546994209, time: 31.57147192955017\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018749600276350975, valid_loss: 0.017213347032666207, time: 43.47932267189026\n",
      "training until max epoch 25,  : best itaration is 16, valid loss is 0.017146696262061598, time: 43.47932267189026\n",
      "seed 2 , cv score : 0.01727402070630279\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5069878970247668, valid_loss: 0.02193944126367569, time: 2.8268918991088867\n",
      "EPOCH: 5, train_loss: 0.013019046299411235, valid_loss: 0.009587954618036748, time: 17.938719272613525\n",
      "EPOCH: 10, train_loss: 0.012919820014488053, valid_loss: 0.009525566026568413, time: 32.12480545043945\n",
      "EPOCH: 15, train_loss: 0.012799969452710168, valid_loss: 0.009262547828257084, time: 45.95217490196228\n",
      "EPOCH: 20, train_loss: 0.012430242225024346, valid_loss: 0.009024295546114445, time: 59.109915256500244\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.00889088898897171, time: 59.109915256500244\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6533894166350365, valid_loss: 0.3023369991779327, time: 1.5774006843566895\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021588185673101327, valid_loss: 0.017236192114651203, time: 9.607612133026123\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01964126348596167, valid_loss: 0.017469318360090257, time: 19.54301691055298\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01927360621708873, valid_loss: 0.017276970222592353, time: 30.698832035064697\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01870564597570715, valid_loss: 0.017301008217036725, time: 42.0869517326355\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.017198534458875658, time: 42.0869517326355\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5072983840898592, valid_loss: 0.02511421211063862, time: 3.571770191192627\n",
      "EPOCH: 5, train_loss: 0.012865658862148823, valid_loss: 0.00941775307059288, time: 16.165157079696655\n",
      "EPOCH: 10, train_loss: 0.012812237413663443, valid_loss: 0.009461466297507286, time: 30.14072060585022\n",
      "EPOCH: 15, train_loss: 0.012710302174851602, valid_loss: 0.009410475380718708, time: 45.042489528656006\n",
      "EPOCH: 20, train_loss: 0.01233890622246022, valid_loss: 0.009103866256773473, time: 59.39003920555115\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009000614657998085, time: 59.39003920555115\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6500279305338048, valid_loss: 0.30178351879119875, time: 1.547795057296753\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02318834121890214, valid_loss: 0.017139728143811227, time: 9.428392171859741\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01969731397622702, valid_loss: 0.017058218866586684, time: 19.06121826171875\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01936738631453644, valid_loss: 0.017009585238993166, time: 30.858754873275757\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018681799733162332, valid_loss: 0.016924336589872836, time: 43.19356179237366\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.01688980359584093, time: 43.19356179237366\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5058956431426309, valid_loss: 0.021531090214848517, time: 3.7460856437683105\n",
      "EPOCH: 5, train_loss: 0.012886426644399762, valid_loss: 0.009229925833642483, time: 17.055319786071777\n",
      "EPOCH: 10, train_loss: 0.012887243886251707, valid_loss: 0.0093770232796669, time: 32.73705863952637\n",
      "EPOCH: 15, train_loss: 0.012767972657456994, valid_loss: 0.009091128259897233, time: 45.153929710388184\n",
      "EPOCH: 20, train_loss: 0.012441617940124628, valid_loss: 0.008884787149727345, time: 59.56903386116028\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008783263191580772, time: 59.56903386116028\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6483092593985635, valid_loss: 0.29305188775062563, time: 1.537088394165039\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021731491804727027, valid_loss: 0.017066113501787186, time: 9.465431928634644\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.0197177365598445, valid_loss: 0.017063527442514895, time: 19.67351794242859\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01940350200527826, valid_loss: 0.017066813856363296, time: 29.63858914375305\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018758431278370524, valid_loss: 0.01694658137857914, time: 41.62895941734314\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.016943949908018112, time: 41.62895941734314\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.508315172502581, valid_loss: 0.024073483273386955, time: 2.6957898139953613\n",
      "EPOCH: 5, train_loss: 0.012931407131508093, valid_loss: 0.009714712426066398, time: 15.296256303787231\n",
      "EPOCH: 10, train_loss: 0.0127741691155904, valid_loss: 0.009692620299756528, time: 30.577813386917114\n",
      "EPOCH: 15, train_loss: 0.012668226319713658, valid_loss: 0.009481018371880055, time: 44.26590299606323\n",
      "EPOCH: 20, train_loss: 0.012319546319594999, valid_loss: 0.009305788986384868, time: 59.1141037940979\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009124735202640295, time: 59.1141037940979\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6538197197476212, valid_loss: 0.3092359983921051, time: 1.5302238464355469\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02088156175248477, valid_loss: 0.01754376821219921, time: 9.698343515396118\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01960246041923964, valid_loss: 0.017702447474002837, time: 20.412944078445435\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019292130432769554, valid_loss: 0.01746068600565195, time: 31.740408658981323\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018582901302851788, valid_loss: 0.017437477558851243, time: 44.9180428981781\n",
      "training until max epoch 25,  : best itaration is 19, valid loss is 0.017372208908200265, time: 44.9180428981781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5083425769924509, valid_loss: 0.022792975306510924, time: 2.972943067550659\n",
      "EPOCH: 5, train_loss: 0.012694534598564615, valid_loss: 0.010192180648446084, time: 16.234962463378906\n",
      "EPOCH: 10, train_loss: 0.012692329290063203, valid_loss: 0.010281567983329297, time: 31.507572412490845\n",
      "EPOCH: 15, train_loss: 0.012590398593824737, valid_loss: 0.010025952309370041, time: 43.99626088142395\n",
      "EPOCH: 20, train_loss: 0.012226752734103171, valid_loss: 0.009819352403283119, time: 58.94817304611206\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009687237218022346, time: 58.94817304611206\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6507117409689896, valid_loss: 0.3065789401531219, time: 1.460878849029541\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021210256419112894, valid_loss: 0.018066270537674428, time: 10.175531387329102\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01938354625639056, valid_loss: 0.018289353437721728, time: 20.972143411636353\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01909336803772417, valid_loss: 0.018009164556860923, time: 30.81452703475952\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018402111165377558, valid_loss: 0.017897760309278966, time: 43.29559898376465\n",
      "training until max epoch 25,  : best itaration is 20, valid loss is 0.017897760309278966, time: 43.29559898376465\n",
      "======================== fold 6 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5073619375911516, valid_loss: 0.02143889106810093, time: 2.4773261547088623\n",
      "EPOCH: 5, train_loss: 0.013845864614169742, valid_loss: 0.010803102813661099, time: 15.12742567062378\n",
      "EPOCH: 10, train_loss: 0.013008609533662329, valid_loss: 0.010221744813024998, time: 30.918875455856323\n",
      "EPOCH: 15, train_loss: 0.012841626599028304, valid_loss: 0.009945143684744836, time: 43.89959096908569\n",
      "EPOCH: 20, train_loss: 0.012569888916521057, valid_loss: 0.009674875400960445, time: 59.24781584739685\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009569132216274739, time: 59.24781584739685\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6593034736610748, valid_loss: 0.3117044627666473, time: 1.496265172958374\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.024110866937081556, valid_loss: 0.017850985564291478, time: 10.2617666721344\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.020433418641521317, valid_loss: 0.017891791313886643, time: 21.43847346305847\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.02002895366702531, valid_loss: 0.017673363462090492, time: 31.51592993736267\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.019484497808121348, valid_loss: 0.017500754594802857, time: 44.463080167770386\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017455487586557863, time: 44.463080167770386\n",
      "======================== fold 7 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.507335726775833, valid_loss: 0.02127169780433178, time: 2.605773687362671\n",
      "EPOCH: 5, train_loss: 0.013016310224712297, valid_loss: 0.010267372466623783, time: 15.951638221740723\n",
      "EPOCH: 10, train_loss: 0.012856139694466381, valid_loss: 0.009685533046722412, time: 30.98398995399475\n",
      "EPOCH: 15, train_loss: 0.012759538480659595, valid_loss: 0.009423678107559681, time: 43.64877915382385\n",
      "EPOCH: 20, train_loss: 0.012384851576408019, valid_loss: 0.009223929941654205, time: 59.553631067276\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009102324545383454, time: 59.553631067276\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6514989302770512, valid_loss: 0.300614675283432, time: 1.5305759906768799\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02418720903428825, valid_loss: 0.017209117673337458, time: 11.369233131408691\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01978637415261285, valid_loss: 0.017219938822090625, time: 21.67488145828247\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019479022034116694, valid_loss: 0.017214796356856823, time: 31.618287324905396\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018881337339612277, valid_loss: 0.01708606895059347, time: 44.82068943977356\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.017024078890681266, time: 44.82068943977356\n",
      "seed 3 , cv score : 0.017288506100530766\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5071275739401981, valid_loss: 0.02352388121187687, time: 2.4645957946777344\n",
      "EPOCH: 5, train_loss: 0.012703719467075693, valid_loss: 0.010220602042973042, time: 15.777077436447144\n",
      "EPOCH: 10, train_loss: 0.012750371145336208, valid_loss: 0.010138200297951698, time: 31.56969904899597\n",
      "EPOCH: 15, train_loss: 0.012635760707780719, valid_loss: 0.009890043623745442, time: 45.040913105010986\n",
      "EPOCH: 20, train_loss: 0.012269330273910955, valid_loss: 0.009641066826879978, time: 59.95147228240967\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009539145454764366, time: 59.95147228240967\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6492337479784682, valid_loss: 0.298905553817749, time: 1.617082118988037\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021450973581522703, valid_loss: 0.017638436704874038, time: 10.659492254257202\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019548274111002684, valid_loss: 0.01774808146059513, time: 20.701099395751953\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.0192150247371378, valid_loss: 0.01755501501262188, time: 31.447941303253174\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018519488188463287, valid_loss: 0.017482576556503773, time: 45.09849238395691\n",
      "early stopping in iteration 22,  : best itaration is 12, valid loss is 0.017447024881839752, time: 49.94869112968445\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5080062872064965, valid_loss: 0.03523666314780712, time: 2.5488860607147217\n",
      "EPOCH: 5, train_loss: 0.01275018833856396, valid_loss: 0.009881603568792342, time: 15.92598009109497\n",
      "EPOCH: 10, train_loss: 0.012745891628013987, valid_loss: 0.009850438758730888, time: 30.839543104171753\n",
      "EPOCH: 15, train_loss: 0.012622686578150915, valid_loss: 0.009604829140007496, time: 43.34502387046814\n",
      "EPOCH: 20, train_loss: 0.012298648619428783, valid_loss: 0.009448636919260026, time: 59.322449684143066\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.009315915256738663, time: 59.322449684143066\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6504293167266716, valid_loss: 0.3031548845767975, time: 1.6833975315093994\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.022427007809382716, valid_loss: 0.018214605264365673, time: 11.626813411712646\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01956040658006052, valid_loss: 0.018153638429939747, time: 20.75891089439392\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019193912634537333, valid_loss: 0.01799437664449215, time: 30.412800550460815\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20, train_loss: 0.018462958169125375, valid_loss: 0.017836253307759762, time: 42.94672179222107\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.01782307717949152, time: 42.94672179222107\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.507982586047986, valid_loss: 0.024857906266473807, time: 2.4249989986419678\n",
      "EPOCH: 5, train_loss: 0.01282108700549116, valid_loss: 0.00944729926637732, time: 15.094511270523071\n",
      "EPOCH: 10, train_loss: 0.012848632087056734, valid_loss: 0.009301507344039587, time: 30.98451828956604\n",
      "EPOCH: 15, train_loss: 0.012750107985065907, valid_loss: 0.009245881595863746, time: 44.16541862487793\n",
      "EPOCH: 20, train_loss: 0.01238684908670633, valid_loss: 0.008965600765525149, time: 59.79376459121704\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.00883783158273078, time: 59.79376459121704\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6494355057778002, valid_loss: 0.2979393085608116, time: 1.6477177143096924\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021948908905492347, valid_loss: 0.01731187466961833, time: 11.286418437957764\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01975329837375352, valid_loss: 0.0174639536640965, time: 22.047529458999634\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019393510508294007, valid_loss: 0.017339579295367002, time: 32.3863320350647\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018696488521131528, valid_loss: 0.017291871186059255, time: 46.00320267677307\n",
      "early stopping in iteration 24,  : best itaration is 14, valid loss is 0.017230866405253228, time: 56.444199323654175\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5071175894322427, valid_loss: 0.024847887534027297, time: 2.752042055130005\n",
      "EPOCH: 5, train_loss: 0.012936713167692761, valid_loss: 0.00967846616792182, time: 16.405872583389282\n",
      "EPOCH: 10, train_loss: 0.012795969252348752, valid_loss: 0.009502205648459494, time: 31.094603776931763\n",
      "EPOCH: 15, train_loss: 0.012700245432505334, valid_loss: 0.009495864001413187, time: 43.60827660560608\n",
      "EPOCH: 20, train_loss: 0.012343884766655596, valid_loss: 0.009327226163198551, time: 59.58790159225464\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.00919221310565869, time: 59.58790159225464\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6498195388027139, valid_loss: 0.29605365296204883, time: 1.8733272552490234\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.0220964750451212, valid_loss: 0.01737421591921399, time: 11.500852823257446\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01968746696523315, valid_loss: 0.01727711510223647, time: 21.69771695137024\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019353041129589483, valid_loss: 0.017422450085481007, time: 31.69360089302063\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018724974874105002, valid_loss: 0.017215607726636033, time: 45.8923122882843\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.01718190514172117, time: 45.8923122882843\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5073250860077183, valid_loss: 0.03592744454741478, time: 2.42647385597229\n",
      "EPOCH: 5, train_loss: 0.012816146282213075, valid_loss: 0.00934821393340826, time: 15.298974990844727\n",
      "EPOCH: 10, train_loss: 0.012808632162608663, valid_loss: 0.00947110828012228, time: 30.825050592422485\n",
      "EPOCH: 15, train_loss: 0.012718014976605267, valid_loss: 0.009326679445803166, time: 44.19238615036011\n",
      "EPOCH: 20, train_loss: 0.01234784617158426, valid_loss: 0.009043956324458122, time: 58.59308385848999\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008931133411824704, time: 58.59308385848999\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6494078851070534, valid_loss: 0.3009469389915466, time: 1.653031826019287\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021074686465518817, valid_loss: 0.01725248098373413, time: 11.463006258010864\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01961208219785674, valid_loss: 0.017251912616193295, time: 21.341166019439697\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019243126304275323, valid_loss: 0.017319503203034402, time: 31.672779083251953\n",
      "early stopping in iteration 16,  : best itaration is 6, valid loss is 0.017164507396519185, time: 34.02445149421692\n",
      "======================== fold 6 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5076355395108663, valid_loss: 0.07837030246853828, time: 2.498460292816162\n",
      "EPOCH: 5, train_loss: 0.013223735659660117, valid_loss: 0.00931688753888011, time: 18.56445026397705\n",
      "EPOCH: 10, train_loss: 0.012861984284796022, valid_loss: 0.009313043430447579, time: 31.59874939918518\n",
      "EPOCH: 15, train_loss: 0.012738415600127867, valid_loss: 0.009304862283170224, time: 47.110634088516235\n",
      "EPOCH: 20, train_loss: 0.012369767794184186, valid_loss: 0.009042925033718348, time: 59.623632192611694\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.00890189353376627, time: 59.623632192611694\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6465449351313952, valid_loss: 0.2964407908916473, time: 1.9773790836334229\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021491095876774273, valid_loss: 0.017193381153047086, time: 12.203891038894653\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019670839484378293, valid_loss: 0.017242470383644105, time: 21.43009877204895\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019358816405607236, valid_loss: 0.01711511619389057, time: 33.49143075942993\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01868811447324382, valid_loss: 0.01705371741205454, time: 45.40697264671326\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.017027305625379085, time: 45.40697264671326\n",
      "======================== fold 7 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5089482263389493, valid_loss: 0.02559061996638775, time: 2.613368511199951\n",
      "EPOCH: 5, train_loss: 0.012852302657402292, valid_loss: 0.009756568893790245, time: 17.689593076705933\n",
      "EPOCH: 10, train_loss: 0.012793034581201417, valid_loss: 0.00964254081249237, time: 30.34034776687622\n",
      "EPOCH: 15, train_loss: 0.012686496353423109, valid_loss: 0.00959316473454237, time: 46.22793483734131\n",
      "EPOCH: 20, train_loss: 0.012313277631693957, valid_loss: 0.00931056335568428, time: 59.381643772125244\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009172231182456017, time: 59.381643772125244\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6493044138765659, valid_loss: 0.29526508450508115, time: 2.379768133163452\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.02220807790908278, valid_loss: 0.016969190128147602, time: 9.847041606903076\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019687908277118287, valid_loss: 0.017037201933562757, time: 17.617994785308838\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.01939596084966546, valid_loss: 0.01698047574609518, time: 29.321954250335693\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 20, train_loss: 0.018678958065250294, valid_loss: 0.016791032254695894, time: 43.106783390045166\n",
      "training until max epoch 25,  : best itaration is 20, valid loss is 0.016791032254695894, time: 43.106783390045166\n",
      "seed 4 , cv score : 0.017271561940967493\n",
      "======================== fold 1 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5095184259496781, valid_loss: 0.0256805157661438, time: 2.5702662467956543\n",
      "EPOCH: 5, train_loss: 0.012859652997279654, valid_loss: 0.00949396576732397, time: 18.51184892654419\n",
      "EPOCH: 10, train_loss: 0.012852495365148904, valid_loss: 0.009522290006279945, time: 31.622470378875732\n",
      "EPOCH: 15, train_loss: 0.012707604310747718, valid_loss: 0.009374356046319008, time: 46.43000054359436\n",
      "EPOCH: 20, train_loss: 0.012386958311204196, valid_loss: 0.009132291711866856, time: 59.19681239128113\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.008983158990740776, time: 59.19681239128113\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6489239920969723, valid_loss: 0.3010489702224731, time: 1.8841962814331055\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.024437256600885165, valid_loss: 0.017336491346359253, time: 10.023335933685303\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01978366881558279, valid_loss: 0.017088118940591812, time: 19.10658621788025\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019397334949601264, valid_loss: 0.017218632586300374, time: 31.787513494491577\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01875313232038297, valid_loss: 0.01703001659363508, time: 43.04496216773987\n",
      "training until max epoch 25,  : best itaration is 22, valid loss is 0.016959469616413116, time: 43.04496216773987\n",
      "======================== fold 2 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5070546686246589, valid_loss: 0.023182942643761636, time: 3.465893507003784\n",
      "EPOCH: 5, train_loss: 0.013297645438059762, valid_loss: 0.01030006643384695, time: 17.256990671157837\n",
      "EPOCH: 10, train_loss: 0.01287174115360186, valid_loss: 0.0094874557107687, time: 29.899230003356934\n",
      "EPOCH: 15, train_loss: 0.012772114022408385, valid_loss: 0.00935214288532734, time: 45.85235285758972\n",
      "EPOCH: 20, train_loss: 0.01247334762199505, valid_loss: 0.009074548929929734, time: 59.20550751686096\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.00896425634622574, time: 59.20550751686096\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.652985738942752, valid_loss: 0.2978003656864166, time: 1.459411382675171\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.022467459703015315, valid_loss: 0.017739875726401805, time: 8.99215817451477\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.01988211747359585, valid_loss: 0.01768842224031687, time: 17.104463577270508\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019594905004408712, valid_loss: 0.017734527327120303, time: 30.486459732055664\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018947608038984442, valid_loss: 0.017763562835752963, time: 42.090617656707764\n",
      "early stopping in iteration 21,  : best itaration is 11, valid loss is 0.017660834714770316, time: 44.88680100440979\n",
      "======================== fold 3 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5068354979283303, valid_loss: 0.02351307839155197, time: 2.602313280105591\n",
      "EPOCH: 5, train_loss: 0.013092955561807833, valid_loss: 0.010070173516869545, time: 17.575622081756592\n",
      "EPOCH: 10, train_loss: 0.012809573616077369, valid_loss: 0.009609441161155701, time: 31.868794679641724\n",
      "EPOCH: 15, train_loss: 0.012700782182651598, valid_loss: 0.009469201229512692, time: 46.625579833984375\n",
      "EPOCH: 20, train_loss: 0.012373993676659223, valid_loss: 0.009290876537561417, time: 60.14070010185242\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.009148895666003227, time: 60.14070010185242\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6489186393650802, valid_loss: 0.29753453493118287, time: 1.871664047241211\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.021219786508260546, valid_loss: 0.01731047801673412, time: 12.024249076843262\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019677525335872494, valid_loss: 0.017136308588087557, time: 21.32792377471924\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.019392727093922126, valid_loss: 0.017112205252051354, time: 32.917338609695435\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.01867988323037689, valid_loss: 0.016932324953377247, time: 45.61607122421265\n",
      "training until max epoch 25,  : best itaration is 24, valid loss is 0.016882874965667725, time: 45.61607122421265\n",
      "======================== fold 4 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5079099416200604, valid_loss: 0.02697031982243061, time: 2.8360767364501953\n",
      "EPOCH: 5, train_loss: 0.012855794306109551, valid_loss: 0.010165607072412968, time: 17.950895309448242\n",
      "EPOCH: 10, train_loss: 0.012735042537302793, valid_loss: 0.009999054186046123, time: 30.364125728607178\n",
      "EPOCH: 15, train_loss: 0.012620694517176978, valid_loss: 0.010021394751966, time: 45.695363998413086\n",
      "EPOCH: 20, train_loss: 0.012284641705301344, valid_loss: 0.00970967460423708, time: 59.11319160461426\n",
      "training until max epoch 25,  : best itaration is 23, valid loss is 0.009589528664946556, time: 59.11319160461426\n",
      "[1, 2, 3, 4]\n",
      "EPOCH: 0, train_loss: 0.6521070561035961, valid_loss: 0.30074036478996274, time: 2.2426857948303223\n",
      "5     batch_norm4.weight\n",
      "5     batch_norm4.bias\n",
      "5     dense4.weight\n",
      "5     dense4.bias\n",
      "EPOCH: 5, train_loss: 0.024510569870471954, valid_loss: 0.017191289775073528, time: 10.849949359893799\n",
      "10     batch_norm3.weight\n",
      "10     batch_norm3.bias\n",
      "10     dense3.weight\n",
      "10     dense3.bias\n",
      "EPOCH: 10, train_loss: 0.019670506571831347, valid_loss: 0.01720509324222803, time: 19.606176376342773\n",
      "15     batch_norm2.weight\n",
      "15     batch_norm2.bias\n",
      "15     dense2.weight\n",
      "15     dense2.bias\n",
      "EPOCH: 15, train_loss: 0.0193616217319049, valid_loss: 0.017006964422762394, time: 31.83193874359131\n",
      "20     batch_norm1.weight\n",
      "20     batch_norm1.bias\n",
      "20     dense1.weight\n",
      "20     dense1.bias\n",
      "EPOCH: 20, train_loss: 0.018709530445690056, valid_loss: 0.01704395990818739, time: 44.9855797290802\n",
      "early stopping in iteration 24,  : best itaration is 14, valid loss is 0.01692565079778433, time: 55.62468504905701\n",
      "======================== fold 5 ========================\n",
      "quantile\n",
      "PCA\n",
      "EPOCH: 0, train_loss: 0.5061729545579166, valid_loss: 0.022062222473323345, time: 2.5200958251953125\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "DEVICE = ('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 25\n",
    "#EPOCHS = 24\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_df.shape[0], y_nonv.shape[1]))\n",
    "imps = []\n",
    "imp_cols = []\n",
    "folds = []\n",
    "test_cv_preds = []\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 7\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    \n",
    "    ###############################################################################################\n",
    "    # LOAD LIBRARIES\n",
    "    targets = SCORED_MOAS.copy()\n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = scored[\"drug_id\"].value_counts()\n",
    "    vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "    vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values} # drug id がどのフォールドに属すか格納\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    tmp = scored.loc[scored[\"drug_id\"].isin(vc2)].reset_index(drop=True)\n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp[\"sig_id\"][idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN K\n",
    "    scored['fold'] = scored.drug_id.map(dct1)\n",
    "    scored.loc[scored[\"fold\"].isna(),'fold'] = scored.loc[scored[\"fold\"].isna(),'sig_id'].map(dct2)\n",
    "    scored[\"fold\"] = scored[\"fold\"].astype('int8')\n",
    "    ###############################################################################################\n",
    "\n",
    "    #for fold, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):    \n",
    "    for fold in range(K):\n",
    "        train_index = scored[scored[\"fold\"] != fold].index.to_list()\n",
    "        valid_index = scored[scored[\"fold\"] == fold].index.to_list()\n",
    "        print(\"======================== fold {} ========================\".format(fold+1))\n",
    "        folds.append(train_index)\n",
    "                \n",
    "        # split data\n",
    "        train_X = X.iloc[train_index]\n",
    "        train_y = y_nonv[train_index]\n",
    "        train_y_all = y_all_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index]\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        valid_y_all = y_all_nonv[valid_index]\n",
    "        test_X = (test_df.drop(\"sig_id\", axis=1))\n",
    "        pub_test_X = (pub_test_df.drop(\"sig_id\", axis=1))\n",
    "\n",
    "        \n",
    "        ### scaler ##########\n",
    "        print(SCALE)\n",
    "        scale_cols = BIOS_+PRODS\n",
    "        scaler = make_scaler(SCALE, seed).fit(train_X.append(pub_test_X)[scale_cols])\n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "\n",
    "    \n",
    "        ### PCA ##########\n",
    "        print(\"PCA\")\n",
    "        n_decom_g = 80\n",
    "        n_decom_c = 10\n",
    "        decom_g_cols = [f\"pca_g-{i}\" for i in range(n_decom_g)]\n",
    "        decom_c_cols = [f\"pca_c-{i}\" for i in range(n_decom_c)]\n",
    "        \n",
    "        pca_g = PCA(n_components = n_decom_g, random_state = seed).fit(train_X.append(pub_test_X)[GENES_])\n",
    "        pca_c = PCA(n_components = n_decom_c, random_state = seed).fit(train_X.append(pub_test_X)[CELLS_])\n",
    "        for df in [train_X, valid_X, test_X, pub_test_X]:\n",
    "            df[decom_g_cols] = pca_g.transform(df[GENES_])\n",
    "            df[decom_c_cols] = pca_c.transform(df[CELLS_])\n",
    "            \n",
    "            \n",
    "        # prepare data for training\n",
    "        train_X = train_X.values\n",
    "        valid_X = valid_X.values\n",
    "        test_X = test_X.values\n",
    "        \n",
    "        \n",
    "        # ================================model training===========================\n",
    "        train_dataset = MoADataset(train_X, train_y_all)\n",
    "        valid_dataset = MoADataset(valid_X, valid_y_all)\n",
    "        test_dataset = TestDataset(test_X)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        \n",
    "        train_dataset2 = MoADataset(train_X, train_y)\n",
    "        valid_dataset2 = MoADataset(valid_X, valid_y)\n",
    "        trainloader2 = torch.utils.data.DataLoader(train_dataset2, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        validloader2 = torch.utils.data.DataLoader(valid_dataset2, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = Model(\n",
    "            num_features=train_X.shape[1],\n",
    "            num_targets=train_y_all.shape[1],\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader) )\n",
    "        fine_tune_scheduler = FineTuneScheduler(EPOCHS, 5)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_tr = SmoothBCEwLogits(smoothing=1e-3)\n",
    "        \n",
    "        # pre train\n",
    "        model = run_training(\n",
    "            model=model,\n",
    "            trainloader=trainloader,\n",
    "            validloader=validloader,\n",
    "            tag=\"ALL\",\n",
    "            epochs=EPOCHS,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            fine_tune_scheduler=None,\n",
    "            loss_fn=loss_fn,\n",
    "            loss_tr=loss_tr,\n",
    "            early_stopping_steps=EARLY_STOPPING_STEPS,\n",
    "            device=DEVICE,\n",
    "            verbose=5,\n",
    "            fold=fold,\n",
    "            seed=seed,)\n",
    "        \n",
    "        model.load_state_dict(torch.load('dnn_weights2/ALL_{}_{}.pt'.format(seed, fold)), DEVICE)\n",
    "        model = fine_tune_scheduler.copy_without_top(model, train_X.shape[1], train_y_all.shape[1], train_y.shape[1])\n",
    "        \n",
    "        # train\n",
    "        optimizer = torch.optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=3e-6, eps=1e-6)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e2, max_lr=3e-3, epochs=EPOCHS, steps_per_epoch=len(trainloader) )\n",
    "        model = run_training(\n",
    "            model=model,\n",
    "            trainloader=trainloader2,\n",
    "            validloader=validloader2,\n",
    "            tag=\"SCORED\",\n",
    "            epochs=EPOCHS,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            fine_tune_scheduler=fine_tune_scheduler,\n",
    "            loss_fn=loss_fn,\n",
    "            loss_tr=loss_tr,\n",
    "            early_stopping_steps=EARLY_STOPPING_STEPS,\n",
    "            device=DEVICE,\n",
    "            verbose=5,\n",
    "            fold=fold,\n",
    "            seed=seed,)\n",
    "        \n",
    "        \n",
    "        model.load_state_dict(torch.load('dnn_weights2/SCORED_{}_{}.pt'.format(seed, fold)), DEVICE)\n",
    "        \n",
    "        #valid predict\n",
    "        val_preds = predict(\n",
    "            model=model,\n",
    "            testloader=validloader,\n",
    "            device=DEVICE,)\n",
    "        \n",
    "        #test predict\n",
    "        test_preds = predict(\n",
    "            model=model,\n",
    "            testloader=testloader,\n",
    "            device=DEVICE)\n",
    "        # ================================model training===========================\n",
    "\n",
    "        train_pred[valid_index] +=  val_preds\n",
    "        preds += test_preds / (K*len(seeds))\n",
    "\n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE,  y.shape[1]))\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "\n",
    "\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"../../../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "#sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub = pd.read_csv(\"../../../Data/Raw/train_targets_scored.csv\")\n",
    "cols = [col for col in train_sub.columns if col != \"sig_id\"]\n",
    "train_sub[cols] = train_preds2\n",
    "train_sub.to_csv(\"dnn_train_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip dnn_weights2.zip dnn_weights2/* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
