{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex1 : cv score : 0.015197305813776704  \n",
    "..........................................................................................................  \n",
    "ex2:  \n",
    "not scaling, remove vehicle, not use nonscored_target  : 0.015448837474049428  \n",
    "not scaling, remove vehicle, not use nonscored_target, 7 seeds  : cv score : 0.015381427768256765  \n",
    "not scaling, remove vehicle, not use nonscored_target, drop high corr cols  : 0.01545810170388885\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 60% : 0.015533423204263394\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 50% : 0.015430995160848253\n",
    "not scaling, remove vehicle, not use nonscored_target, add 95% , 40% : 0.015354553654126092\n",
    "all : 0.01515220913930692(7 seeds)\n",
    "all : 0.015207047278156766 (10 seeds)\n",
    "all : model tuning + 3 layer (7seeds) cv score : 0.014899280863407222\n",
    "\n",
    "混ぜるほど良いスコア。。。？  \n",
    "high corr な特徴落としたら少し悪化\n",
    "95%追加で少し改善、なんか全部やった方がいい説\n",
    "..........................................................................................................  \n",
    "ex3:\n",
    "not scaling, remove vehicle, not use nonscored_target, use DAE  : 0.015816593548836414 (variance 1.0, 2711, 2311)\n",
    "\n",
    "\n",
    "\n",
    "悪化はした\n",
    "そこまで悪くはなさそう\n",
    "cellの方がロスが落ちないから、チューニング次第な感じはする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, random, os, sys\n",
    "sys.path.append(\"../\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1200)\n",
    "pd.set_option(\"display.max_rows\", 1200)\n",
    "%matplotlib inline\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    res = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        y = y_true[:,i]\n",
    "        pred = y_pred[:,i]\n",
    "        res.append(log_loss(y, pred))\n",
    "    return np.mean(res)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def make_scaler(flag):\n",
    "    if flag == \"quantile\":\n",
    "        return QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    elif flag == \"standard\":\n",
    "        return StandardScaler()\n",
    "    elif flag == \"minmax\":\n",
    "        return MinMaxScaler()\n",
    "    elif flag == \"robust\":\n",
    "        return RobustScaler()\n",
    "    \n",
    "seeds = [0, 1, 2, 3, 4, 5, 6]\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "SCALE = \"quantile\"\n",
    "USE_DAE = True\n",
    "USE_SCALE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Concatenate, Embedding, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow_addons.optimizers import AdamW, Lookahead\n",
    "\n",
    "\n",
    "def DNN_3lmodel(input_size, output_size):\n",
    "    inp = Input(shape = (input_size, ))\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = Dropout(0.4914099166744246)(x)\n",
    "    x = WeightNormalization(Dense(1159, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.18817607797795838)(x)\n",
    "    x = WeightNormalization(Dense(960, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.12542057776853896)(x)\n",
    "    x = WeightNormalization(Dense(1811, activation = 'relu'))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20175242230280122)(x)\n",
    "    out = WeightNormalization(Dense(output_size, activation = 'sigmoid'))(x)\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    opt = Lookahead(opt, sync_period = 10)\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def DNN_model(input_size, output_size):\n",
    "    inputs = Input((input_size, ))\n",
    "\n",
    "    outputs = BatchNormalization()(inputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Dropout(0.20)(outputs)\n",
    "    outputs = WeightNormalization(Dense(1024, activation=\"relu\"))(outputs)\n",
    "\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = WeightNormalization(Dense(output_size, activation=\"sigmoid\"))(outputs)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=AdamW(lr=0.001, weight_decay=1e-5, clipvalue=756), \n",
    "        loss=BinaryCrossentropy(label_smoothing = 0.0015)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def DAE(input_size):\n",
    "    emb_size = input_size//2\n",
    "    inputs = Input((input_size, ))\n",
    "    x = Dense(emb_size, activation=\"relu\")(inputs)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    outputs = Dense(input_size, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "def DAE2(input_size):\n",
    "    emb_size = 1500\n",
    "    inputs = Input((input_size, ))\n",
    "    x = Dense(emb_size, activation=\"relu\")(inputs)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    x = Dense(emb_size, activation=\"relu\")(x)\n",
    "    outputs = Dense(input_size, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g772, c100, 206クラス、402クラスの分類\n",
    "\n",
    "train_df = pd.read_csv(\"../../Data/Raw/train_features.csv\")\n",
    "test_df = pd.read_csv(\"../../Data/Raw/test_features.csv\")\n",
    "\n",
    "y = pd.read_csv(\"../../Data/Raw/train_targets_scored.csv\")\n",
    "y_non = pd.read_csv(\"../../Data/Raw/train_targets_nonscored.csv\")\n",
    "y_all = pd.concat([y, y_non.drop(\"sig_id\", axis=1)], axis=1)\n",
    "\n",
    "GENES = [col for col in train_df.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in train_df.columns if col.startswith(\"c-\")]\n",
    "CELLS_50 = CELLS[:50]\n",
    "BIOS = GENES + CELLS\n",
    "\n",
    "SCORED_MOAS = [col for col in y.columns if col != \"sig_id\"]\n",
    "NONSCORED_MOAS = [col for col in y_non.columns if col != \"sig_id\"]\n",
    "ALL_MOAS = SCORED_MOAS + NONSCORED_MOAS\n",
    "\n",
    "train_sigid = train_df[\"sig_id\"]\n",
    "test_sigid = test_df[\"sig_id\"]\n",
    "\n",
    "TR_SIZE = train_df.shape[0]\n",
    "TE_SIZE = test_df.shape[0]\n",
    "\n",
    "train_nonvehicle_index = train_df[train_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "test_nonvehicle_index = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].index\n",
    "\n",
    "train_df[\"time_dose\"] = train_df[\"cp_time\"].astype(str) + \" * \" + train_df[\"cp_dose\"]\n",
    "test_df[\"time_dose\"] = test_df[\"cp_time\"].astype(str) + \" * \" + test_df[\"cp_dose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27796 samples\n",
      "Epoch 1/120\n",
      "27796/27796 [==============================] - 2s 60us/sample - loss: 0.7490\n",
      "Epoch 2/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.5603\n",
      "Epoch 3/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.4970\n",
      "Epoch 4/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.4560\n",
      "Epoch 5/120\n",
      "27796/27796 [==============================] - 1s 36us/sample - loss: 0.4296\n",
      "Epoch 6/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.4148\n",
      "Epoch 7/120\n",
      "27796/27796 [==============================] - 2s 61us/sample - loss: 0.3858\n",
      "Epoch 8/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.3691\n",
      "Epoch 9/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.3682\n",
      "Epoch 10/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.3497\n",
      "Epoch 11/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.3409\n",
      "Epoch 12/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.3336\n",
      "Epoch 13/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.3312\n",
      "Epoch 14/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.3261\n",
      "Epoch 15/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.3221\n",
      "Epoch 16/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.3319\n",
      "Epoch 17/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.3150\n",
      "Epoch 18/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.3133\n",
      "Epoch 19/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.3093\n",
      "Epoch 20/120\n",
      "27796/27796 [==============================] - 1s 45us/sample - loss: 0.3066\n",
      "Epoch 21/120\n",
      "27796/27796 [==============================] - 1s 50us/sample - loss: 0.3084\n",
      "Epoch 22/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.3065\n",
      "Epoch 23/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.3035\n",
      "Epoch 24/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.3016\n",
      "Epoch 25/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.3019\n",
      "Epoch 26/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2994\n",
      "Epoch 27/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.2985\n",
      "Epoch 28/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.3008\n",
      "Epoch 29/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2968\n",
      "Epoch 30/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2969\n",
      "Epoch 31/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2946\n",
      "Epoch 32/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2944\n",
      "Epoch 33/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2935\n",
      "Epoch 34/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2938\n",
      "Epoch 35/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2928\n",
      "Epoch 36/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2938\n",
      "Epoch 37/120\n",
      "27796/27796 [==============================] - 1s 46us/sample - loss: 0.2900\n",
      "Epoch 38/120\n",
      "27796/27796 [==============================] - 1s 50us/sample - loss: 0.2908\n",
      "Epoch 39/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2935\n",
      "Epoch 40/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2963\n",
      "Epoch 41/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2906\n",
      "Epoch 42/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2898\n",
      "Epoch 43/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.2878\n",
      "Epoch 44/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2901\n",
      "Epoch 45/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2885\n",
      "Epoch 46/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2917\n",
      "Epoch 47/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2912\n",
      "Epoch 48/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2876\n",
      "Epoch 49/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2863\n",
      "Epoch 50/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2876\n",
      "Epoch 51/120\n",
      "27796/27796 [==============================] - 1s 43us/sample - loss: 0.2908\n",
      "Epoch 52/120\n",
      "27796/27796 [==============================] - 2s 58us/sample - loss: 0.2861\n",
      "Epoch 53/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2852\n",
      "Epoch 54/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2842\n",
      "Epoch 55/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2854\n",
      "Epoch 56/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2868\n",
      "Epoch 57/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2849\n",
      "Epoch 58/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.2848\n",
      "Epoch 59/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2856\n",
      "Epoch 60/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2846\n",
      "Epoch 61/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2848\n",
      "Epoch 62/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2835\n",
      "Epoch 63/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2854\n",
      "Epoch 64/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2832\n",
      "Epoch 65/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2825\n",
      "Epoch 66/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2835\n",
      "Epoch 67/120\n",
      "27796/27796 [==============================] - 1s 36us/sample - loss: 0.2840\n",
      "Epoch 68/120\n",
      "27796/27796 [==============================] - 2s 55us/sample - loss: 0.2852\n",
      "Epoch 69/120\n",
      "27796/27796 [==============================] - 1s 43us/sample - loss: 0.2827\n",
      "Epoch 70/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2842\n",
      "Epoch 71/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2829\n",
      "Epoch 72/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2818\n",
      "Epoch 73/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2823\n",
      "Epoch 74/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2818\n",
      "Epoch 75/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2803\n",
      "Epoch 76/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2823\n",
      "Epoch 77/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2839\n",
      "Epoch 78/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2829\n",
      "Epoch 79/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2861\n",
      "Epoch 80/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2814\n",
      "Epoch 81/120\n",
      "27796/27796 [==============================] - 1s 36us/sample - loss: 0.2792\n",
      "Epoch 82/120\n",
      "27796/27796 [==============================] - 2s 55us/sample - loss: 0.2797\n",
      "Epoch 83/120\n",
      "27796/27796 [==============================] - 1s 44us/sample - loss: 0.2795\n",
      "Epoch 84/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2797\n",
      "Epoch 85/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2810\n",
      "Epoch 86/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2803\n",
      "Epoch 87/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.2791\n",
      "Epoch 88/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2801\n",
      "Epoch 89/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2805\n",
      "Epoch 90/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2807\n",
      "Epoch 91/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2804\n",
      "Epoch 92/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2807\n",
      "Epoch 93/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2803\n",
      "Epoch 94/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2797\n",
      "Epoch 95/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2784\n",
      "Epoch 96/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2797\n",
      "Epoch 97/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2786\n",
      "Epoch 98/120\n",
      "27796/27796 [==============================] - 1s 36us/sample - loss: 0.3106\n",
      "Epoch 99/120\n",
      "27796/27796 [==============================] - 1s 52us/sample - loss: 0.2960\n",
      "Epoch 100/120\n",
      "27796/27796 [==============================] - 1s 47us/sample - loss: 0.2840\n",
      "Epoch 101/120\n",
      "27796/27796 [==============================] - 1s 27us/sample - loss: 0.2793\n",
      "Epoch 102/120\n",
      "27796/27796 [==============================] - 1s 29us/sample - loss: 0.2788\n",
      "Epoch 103/120\n",
      "27796/27796 [==============================] - 1s 31us/sample - loss: 0.2777\n",
      "Epoch 104/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2772\n",
      "Epoch 105/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2767\n",
      "Epoch 106/120\n",
      "27796/27796 [==============================] - 1s 28us/sample - loss: 0.2772\n",
      "Epoch 107/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2780\n",
      "Epoch 108/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2753\n",
      "Epoch 109/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2800\n",
      "Epoch 110/120\n",
      "27796/27796 [==============================] - 1s 35us/sample - loss: 0.2772\n",
      "Epoch 111/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2769\n",
      "Epoch 112/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2761\n",
      "Epoch 113/120\n",
      "27796/27796 [==============================] - 1s 38us/sample - loss: 0.2778\n",
      "Epoch 114/120\n",
      "27796/27796 [==============================] - 2s 64us/sample - loss: 0.2753\n",
      "Epoch 115/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.2777\n",
      "Epoch 116/120\n",
      "27796/27796 [==============================] - 1s 36us/sample - loss: 0.2745\n",
      "Epoch 117/120\n",
      "27796/27796 [==============================] - 1s 27us/sample - loss: 0.2856\n",
      "Epoch 118/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2813\n",
      "Epoch 119/120\n",
      "27796/27796 [==============================] - 1s 32us/sample - loss: 0.2764\n",
      "Epoch 120/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.2775\n",
      "Train on 27796 samples\n",
      "Epoch 1/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.8960\n",
      "Epoch 2/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.4688\n",
      "Epoch 3/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.4201\n",
      "Epoch 4/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.3862\n",
      "Epoch 5/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.3567\n",
      "Epoch 6/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.3342\n",
      "Epoch 7/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.3175\n",
      "Epoch 8/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.3047\n",
      "Epoch 9/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2950\n",
      "Epoch 10/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2865\n",
      "Epoch 11/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2798\n",
      "Epoch 12/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2736\n",
      "Epoch 13/120\n",
      "27796/27796 [==============================] - 1s 33us/sample - loss: 0.2684\n",
      "Epoch 14/120\n",
      "27796/27796 [==============================] - 1s 30us/sample - loss: 0.2654\n",
      "Epoch 15/120\n",
      "27796/27796 [==============================] - 1s 45us/sample - loss: 0.2607\n",
      "Epoch 16/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2583\n",
      "Epoch 17/120\n",
      "27796/27796 [==============================] - 1s 21us/sample - loss: 0.2558\n",
      "Epoch 18/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2550\n",
      "Epoch 19/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2535\n",
      "Epoch 20/120\n",
      "27796/27796 [==============================] - 1s 26us/sample - loss: 0.2510\n",
      "Epoch 21/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2505\n",
      "Epoch 22/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2501\n",
      "Epoch 23/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2484\n",
      "Epoch 24/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2463\n",
      "Epoch 25/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2446\n",
      "Epoch 26/120\n",
      "27796/27796 [==============================] - 1s 27us/sample - loss: 0.2439\n",
      "Epoch 27/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2438\n",
      "Epoch 28/120\n",
      "27796/27796 [==============================] - 1s 27us/sample - loss: 0.2428\n",
      "Epoch 29/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2421\n",
      "Epoch 30/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2418\n",
      "Epoch 31/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2412\n",
      "Epoch 32/120\n",
      "27796/27796 [==============================] - 1s 37us/sample - loss: 0.2409\n",
      "Epoch 33/120\n",
      "27796/27796 [==============================] - 1s 45us/sample - loss: 0.2418\n",
      "Epoch 34/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2402\n",
      "Epoch 35/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2397\n",
      "Epoch 36/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2398\n",
      "Epoch 37/120\n",
      "27796/27796 [==============================] - 1s 26us/sample - loss: 0.2392\n",
      "Epoch 38/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2389\n",
      "Epoch 39/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2384\n",
      "Epoch 40/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2372\n",
      "Epoch 41/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2359\n",
      "Epoch 42/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2352\n",
      "Epoch 43/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2347\n",
      "Epoch 44/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2346\n",
      "Epoch 45/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2346\n",
      "Epoch 46/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2353\n",
      "Epoch 47/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2340\n",
      "Epoch 48/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2345\n",
      "Epoch 49/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2338\n",
      "Epoch 50/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2335\n",
      "Epoch 51/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2338\n",
      "Epoch 52/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2336\n",
      "Epoch 53/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2342\n",
      "Epoch 54/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2332\n",
      "Epoch 55/120\n",
      "27796/27796 [==============================] - 1s 44us/sample - loss: 0.2329\n",
      "Epoch 56/120\n",
      "27796/27796 [==============================] - 1s 34us/sample - loss: 0.2331\n",
      "Epoch 57/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2326\n",
      "Epoch 58/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2330\n",
      "Epoch 59/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2328\n",
      "Epoch 60/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2327\n",
      "Epoch 61/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2328\n",
      "Epoch 62/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2328\n",
      "Epoch 63/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2324\n",
      "Epoch 64/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2329\n",
      "Epoch 65/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2322\n",
      "Epoch 66/120\n",
      "27796/27796 [==============================] - 1s 26us/sample - loss: 0.2324\n",
      "Epoch 67/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2321\n",
      "Epoch 68/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2321\n",
      "Epoch 69/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2321\n",
      "Epoch 70/120\n",
      "27796/27796 [==============================] - 1s 25us/sample - loss: 0.2320\n",
      "Epoch 71/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2322\n",
      "Epoch 72/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2324\n",
      "Epoch 73/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2319\n",
      "Epoch 74/120\n",
      "27796/27796 [==============================] - 1s 26us/sample - loss: 0.2317\n",
      "Epoch 75/120\n",
      "27796/27796 [==============================] - 1s 53us/sample - loss: 0.2319\n",
      "Epoch 76/120\n",
      "27796/27796 [==============================] - 1s 29us/sample - loss: 0.2319\n",
      "Epoch 77/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2316\n",
      "Epoch 78/120\n",
      "27796/27796 [==============================] - 1s 26us/sample - loss: 0.2319\n",
      "Epoch 79/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2313\n",
      "Epoch 80/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2323\n",
      "Epoch 81/120\n",
      "27796/27796 [==============================] - 1s 24us/sample - loss: 0.2310\n",
      "Epoch 82/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2314\n",
      "Epoch 83/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2313\n",
      "Epoch 84/120\n",
      "27796/27796 [==============================] - 1s 27us/sample - loss: 0.2323\n",
      "Epoch 85/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2314\n",
      "Epoch 86/120\n",
      "27796/27796 [==============================] - 1s 21us/sample - loss: 0.2313\n",
      "Epoch 87/120\n",
      "27796/27796 [==============================] - 1s 23us/sample - loss: 0.2312\n",
      "Epoch 88/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2318\n",
      "Epoch 89/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2311\n",
      "Epoch 90/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2312\n",
      "Epoch 91/120\n",
      "27796/27796 [==============================] - 1s 22us/sample - loss: 0.2323\n",
      "(27796, 772)\n",
      "(27796, 100)\n"
     ]
    }
   ],
   "source": [
    "# DAE\n",
    "if USE_DAE:\n",
    "    genes = pd.concat([train_df, test_df])[GENES].values\n",
    "    cells = pd.concat([train_df, test_df])[CELLS].values\n",
    "    \n",
    "    factor = 0.5\n",
    "    variance = 1.0\n",
    "    gene_noise = np.random.normal(loc=0., scale=variance, size=genes.shape)\n",
    "    cell_noise = np.random.normal(loc=0., scale=variance, size=cells.shape)\n",
    "\n",
    "    noisy_genes = genes + factor*gene_noise\n",
    "    noisy_cells = cells +  factor * cell_noise\n",
    "\n",
    "    epochs = 300\n",
    "    g_chck = ModelCheckpoint('keras_dae_g.h5', monitor='loss', save_best_only=True)\n",
    "    c_chck = ModelCheckpoint('keras_dae_c.h5', monitor='loss', save_best_only=True)\n",
    "    es = EarlyStopping(monitor='loss', patience=10, min_delta=0)\n",
    "\n",
    "    g_model = DAE(genes.shape[1])\n",
    "    c_model = DAE(cells.shape[1])\n",
    "\n",
    "    g_model.fit(noisy_genes, genes, batch_size=128, verbose=1, epochs=epochs, callbacks=[g_chck, es])\n",
    "    c_model.fit(noisy_cells, cells, batch_size=128, verbose=1, epochs=epochs, callbacks=[c_chck, es])\n",
    "\n",
    "    g_model.load_weights('keras_dae_g.h5')\n",
    "    c_model.load_weights('keras_dae_c.h5')\n",
    "\n",
    "    trans_genes = g_model.predict(genes)\n",
    "    trans_cells = c_model.predict(cells)\n",
    "    print(trans_genes.shape)\n",
    "    print(trans_cells.shape)\n",
    "    \n",
    "    train_trans_genes = trans_genes[:TR_SIZE]\n",
    "    test_trans_genes = trans_genes[TR_SIZE:]\n",
    "    train_trans_cells = trans_cells[:TR_SIZE]\n",
    "    test_trans_cells = trans_cells[TR_SIZE:]\n",
    "    for i, col in enumerate(GENES):\n",
    "        train_df[col] = train_trans_genes[:,i]\n",
    "        test_df[col] = test_trans_genes[:,i]\n",
    "    for i, col in enumerate(CELLS):\n",
    "        train_df[col] = train_trans_cells[:,i]\n",
    "        test_df[col] = test_trans_cells[:,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 880)\n",
      "(3982, 880)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SCALE:\n",
    "    for val in train_df[\"time_dose\"].unique():\n",
    "        temp = pd.concat([train_df[train_df[\"time_dose\"] == val], test_df[test_df[\"time_dose\"] == val]])\n",
    "        scaler = make_scaler(SCALE)\n",
    "        scaler.fit(temp[BIOS].values.reshape(-1, len(BIOS)))\n",
    "        tr_ind = train_df[train_df[\"time_dose\"] == val].index\n",
    "        te_ind = test_df[test_df[\"time_dose\"] == val].index\n",
    "        tr_trans = scaler.transform(train_df[BIOS].iloc[tr_ind].values.reshape(-1, len(BIOS))).T\n",
    "        te_trans = scaler.transform(test_df[BIOS].iloc[te_ind].values.reshape(-1, len(BIOS))).T\n",
    "\n",
    "        for i, col in enumerate(BIOS):\n",
    "            train_df[col].iloc[tr_ind] = tr_trans[i]\n",
    "            test_df[col].iloc[te_ind] = te_trans[i]\n",
    "\n",
    "drop_cols = [\"cp_time\", \"cp_dose\", \"time_dose\"]\n",
    "\n",
    "train_df = pd.concat([pd.get_dummies(train_df[\"time_dose\"], prefix=\"onehot\"), train_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "test_df = pd.concat([pd.get_dummies(test_df[\"time_dose\"], prefix=\"onehot\"), test_df.drop(drop_cols, axis=1) ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove cp_type = ctl_vehicle\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print()\n",
    "\n",
    "mask = train_df[\"cp_type\"] != \"ctl_vehicle\"\n",
    "train_df = train_df[mask].drop(\"cp_type\", axis=1).reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cp_type\"] != \"ctl_vehicle\"].drop(\"cp_type\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 878)\n",
      "(3624, 878)\n",
      "(21948, 206)\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(\"sig_id\", axis=1)\n",
    "test_X = test_df.drop(\"sig_id\", axis=1)\n",
    "y_nonv = y[mask].drop(\"sig_id\", axis=1).values\n",
    "y = y.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "print(X.shape)\n",
    "print(test_X.shape)\n",
    "print(y_nonv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== fold 1 ========================\n",
      "======================== fold 2 ========================\n",
      "======================== fold 3 ========================\n",
      "======================== fold 4 ========================\n",
      "======================== fold 5 ========================\n",
      "======================== fold 6 ========================\n",
      "======================== fold 7 ========================\n",
      "seed 0 , cv score : 0.01702480093185814\n",
      "======================== fold 1 ========================\n",
      "======================== fold 2 ========================\n",
      "======================== fold 3 ========================\n",
      "======================== fold 4 ========================\n",
      "======================== fold 5 ========================\n",
      "======================== fold 6 ========================\n",
      "======================== fold 7 ========================\n",
      "seed 1 , cv score : 0.01705826480087469\n",
      "======================== fold 1 ========================\n",
      "======================== fold 2 ========================\n",
      "======================== fold 3 ========================\n",
      "======================== fold 4 ========================\n",
      "======================== fold 5 ========================\n",
      "======================== fold 6 ========================\n",
      "======================== fold 7 ========================\n",
      "seed 2 , cv score : 0.0171470868962643\n",
      "======================== fold 1 ========================\n",
      "======================== fold 2 ========================\n",
      "======================== fold 3 ========================\n",
      "======================== fold 4 ========================\n",
      "======================== fold 5 ========================\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros((X.shape[0], y_nonv.shape[1]))\n",
    "preds = np.zeros((test_X.shape[0], y_nonv.shape[1]))\n",
    "\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    K = 7\n",
    "    kf = MultilabelStratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "    train_pred = np.zeros(train_preds.shape)\n",
    "    \n",
    "    for itr, (train_index, valid_index) in enumerate(kf.split(X, y_nonv)):\n",
    "        print(\"======================== fold {} ========================\".format(itr+1))\n",
    "        train_X = X.iloc[train_index].values\n",
    "        train_y = y_nonv[train_index]\n",
    "        valid_X = X.iloc[valid_index].values\n",
    "        valid_y = y_nonv[valid_index]\n",
    "        \n",
    "        #model = DNN_3lmodel(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        model = DNN_model(input_size=train_X.shape[1], output_size=train_y.shape[1])\n",
    "        cb = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                                          mode = 'min',\n",
    "                                                          patience = 10,\n",
    "                                                          restore_best_weights = False,)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                         mode = 'min',\n",
    "                                                         factor = 0.3,\n",
    "                                                         patience = 3,)\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'_{itr}_{seed}.h5',\n",
    "                                                        monitor='val_loss',\n",
    "                                                        save_best_only = True,\n",
    "                                                        save_weights_only = True)\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            train_X, \n",
    "            train_y,\n",
    "            batch_size=128,\n",
    "            epochs=20,\n",
    "            verbose=0,\n",
    "            callbacks=[cb],\n",
    "            #callbacks = [early_stopping, reduce_lr,  checkpoint],\n",
    "            validation_data=(valid_X, valid_y),\n",
    "        )\n",
    "        #model.load_weights(f'_{itr}_{seed}.h5')\n",
    "        \n",
    "        train_pred[valid_index] += model.predict(valid_X, batch_size=128)\n",
    "        preds += model.predict(test_X.values, batch_size=128) / (K*len(seeds))\n",
    "\n",
    "\n",
    "    print(\"seed {} , cv score : {}\".format(seed, metric(y_nonv, train_pred)))\n",
    "    train_preds += train_pred/len(seeds)\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y_nonv, train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = np.zeros((TR_SIZE, y.shape[1]))\n",
    "preds2 = np.zeros((TE_SIZE, y.shape[1]))\n",
    "\n",
    "train_preds2[train_nonvehicle_index] = train_preds\n",
    "preds2[test_nonvehicle_index] = preds\n",
    "\n",
    "print(\"cv score : {}\".format(metric(y, train_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [:,:tag_size]\n",
    "sub_df = pd.read_csv(\"../Data/Raw/sample_submission.csv\")\n",
    "#sub_df = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n",
    "cols = [col for col in sub_df.columns if col != \"sig_id\"]\n",
    "sub_df[cols] = preds2\n",
    "#sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_preds.copy()\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))\n",
    "t_ = train_df[train_df[\"cp_type\"] == 0]\n",
    "t[t_.index] = np.zeros((t_.shape[0], t.shape[1]))\n",
    "t = np.where(t > 1, 1, t)\n",
    "print(metric(y[:,:tag_size], t[:,:tag_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "not_li = []\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(y.shape[1]):\n",
    "        if y[i][j] == 1:\n",
    "            #print(\"====={}, {}====\".format(i,j))\n",
    "            rank = np.where(train_preds[i].argsort()[::-1] == j)[0][0]+1\n",
    "            #print(\"rank {}\".format(rank))\n",
    "            if rank <= 20:\n",
    "                li.append(j)\n",
    "            else:\n",
    "                not_li.append(j)\n",
    "            #print(train_preds[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(set(li))\n",
    "not_li = list(set(not_li))\n",
    "for i in li:\n",
    "    if i not in not_li:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\").columns\n",
    "pd.read_csv(\"../Data/Raw/train_targets_nonscored.csv\")[cols[256-206]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
